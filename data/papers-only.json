[
  {
    "id": "10.1145/1785455.1785456",
    "title": "ExoInterfaces: Novel Exosceleton Haptic Interfaces for Virtual Reality, Augmented Sport and Rehabilitation",
    "authors": [
      "Dzmitry Tsetserukou",
      "Katsunari Sato",
      "Susumu Tachi"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "We developed novel haptic interfaces, FlexTorque and FlexTensor that enable realistic physical interaction with real and Virtual Environments. The idea behind FlexTorque is to reproduce human muscle structure, which allows us to perform dexterous manipulation and safe interaction with environment in daily life. FlexTorque suggests new possibilities for highly realistic, very natural physical interaction in virtual environments. There are no restrictions on the arm movement, and it is not necessary to hold a physical object during interaction with objects in virtual reality. Because the system can generate strong forces, even though it is light-weight, easily wearable, and intuitive, users experience a new level of realism as they interact with virtual environments.",
    "keywords": [
      "haptic display",
      "augmented games",
      "exoskeleton",
      "rehabilitation",
      "game controller",
      "haptic interface",
      "augmented sport",
      "force feedback",
      "virtual reality"
    ],
    "doi": "10.1145/1785455.1785456",
    "url": "https://doi.org/10.1145/1785455.1785456",
    "citations": 63,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "1",
    "numpages": "6",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785457",
    "title": "PossessedHand: A Hand Gesture Manipulation System Using Electrical Stimuli",
    "authors": [
      "Emi Tamaki",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Acquiring knowledge about the timing and speed of hand gestures is important to learn physical skills, such as playing musical instruments, performing arts, and making handicrafts. However, it is difficult to use devices that dynamically and mechanically control a user's hand for learning because such devices are very large, and hence, are unsuitable for daily use. In addition, since groove-type devices interfere with actions such as playing musical instruments, performing arts, and making handicrafts, users tend to avoid wearing these devices. To solve these problems, we propose PossessedHand, a device with a forearm belt, for controlling a user's hand by applying electrical stimulus to the muscles around the forearm of the user. The dimensions of PossessedHand are 10 x 7.0 x 8.0 cm, and the device is portable and suited for daily use. The electrical stimuli are generated by an electronic pulse generator and transmitted from 14 electrode pads. Our experiments confirmed that PossessedHand can control the motion of 16 joints in the hand. We propose an application of this device to help a beginner learn how to play musical instruments such as the piano and koto.",
    "keywords": [
      "wearable",
      "output device",
      "interaction device",
      "hand gesture",
      "electrical stimuli"
    ],
    "doi": "10.1145/1785455.1785457",
    "url": "https://doi.org/10.1145/1785455.1785457",
    "citations": 38,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "2",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1785455.1785458",
    "title": "A GMM Based 2-Stage Architecture for Multi-Subject Emotion Recognition Using Physiological Responses",
    "authors": [
      "Gu Yuan",
      "Tan Su Lim",
      "Wong Kai Juan",
      "Ho Moon-Ho Ringo",
      "Qu Li"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "There is a trend these days to add emotional characteristics as new features into human-computer interaction to equip machines with more intelligence when communicating with humans. Besides traditional audio-visual techniques, physiological signals provide a promising alternative for automatic emotion recognition. Ever since Dr. Picard and colleagues brought forward the initial concept of physiological signals based emotion recognition, various studies have been reported following the same system structure. In this paper, we implemented a novel 2-stage architecture of the emotion recognition system in order to improve the performance when dealing with multi-subject context. This type of system is more realistic practical implementation. Instead of directly classifying data from all the mixed subjects, one step was added ahead to transform a traditional subject-independent case into several subject-dependent cases by classifying new coming sample into each existing subject model using Gaussian Mixture Model (GMM). For simultaneous classification on four affective states, the correct classification ration (CCR) shows significant improvement from 80.7% to over 90% which supports the feasibility of the system.",
    "keywords": [],
    "doi": "10.1145/1785455.1785458",
    "url": "https://doi.org/10.1145/1785455.1785458",
    "citations": 18,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "3",
    "numpages": "6",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785459",
    "title": "Gaze-Directed Ubiquitous Interaction Using a Brain-Computer Interface",
    "authors": [
      "Dieter Schmalstieg",
      "Alexander Bornik",
      "Gernot Müller-Putz",
      "Gert Pfurtscheller"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "n this paper, we present a first proof-of-concept for using a mobile Brain-Computer Interface (BCI) coupled to a wearable computer as an ambient input device for a ubiquitous computing service. BCI devices, such as electroencephalogram (EEG) based BCI, can be used as a novel form of human-computer interaction device. A user can log into a nearby computer terminal by looking at its screen. This feature is enabled by detecting a user's gaze through the analysis of the brain's response to visually evoked patterns. We present the experimental setup and discuss opportunities and limitations of the technique.",
    "keywords": [
      "authentication",
      "electroencephalogram",
      "object selection",
      "brain computer interface",
      "gaze tracking",
      "biometrics"
    ],
    "doi": "10.1145/1785455.1785459",
    "url": "https://doi.org/10.1145/1785455.1785459",
    "citations": 6,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "4",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785460",
    "title": "Relevance of EEG Input Signals in the Augmented Human Reader",
    "authors": [
      "Inês Oliveira",
      "Ovidiu Grigore",
      "Nuno Guimarães",
      "Luís Duarte"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "This paper studies the discrimination of electroencephalographic (EEG) signals based in their capacity to identify silent attentive visual reading activities versus non reading states.The use of physiological signals is growing in the design of interactive systems due to their relevance in the improvement of the coupling between user states and application behavior.Reading is pervasive in visual user interfaces. In previous work, we integrated EEG signals in prototypical applications, designed to analyze reading tasks. This work searches for signals that are most relevant for reading detection procedures. More specifically, this study determines which features, input signals, and frequency bands are more significant for discrimination between reading and non-reading classes. This optimization is critical for an efficient and real time implementation of EEG processing software components, a basic requirement for the future applications.We use probabilistic similarity metrics, independent of the classification algorithm. All analyses are performed after determining the power spectrum density of delta, theta, alpha, beta and gamma rhythms. The results about the relevance of the input signals are validated with functional neurosciences knowledge.The experiences have been performed in a conventional HCI lab, with non clinical EEG equipment and setup. This is an explicit and voluntary condition. We anticipate that future mobile and wireless EEG capture devices will allow this work to be generalized to common applications.",
    "keywords": [
      "feature relevance measurement",
      "similarity metrics",
      "HCI",
      "reading detection",
      "EEG processing and classification"
    ],
    "doi": "10.1145/1785455.1785460",
    "url": "https://doi.org/10.1145/1785455.1785460",
    "citations": 6,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "5",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785461",
    "title": "Brain Computer Interfaces for Inclusion",
    "authors": [
      "P. J. McCullagh",
      "M. P. Ware",
      "G. Lightbody"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this paper, we describe an intelligent graphical user interface (IGUI) and a User Application Interface (UAI) tailored to Brain Computer Interface (BCI) interaction, designed for people with severe communication needs. The IGUI has three components; a two way interface for communication with BCI2000 concerning user events and event handling; an interface to user applications concerning the passing of user commands and associated device identifiers, and the receiving of notification of device status; and an interface to an extensible mark-up language (xml) file containing menu content definitions. The interface has achieved control of domotic applications. The architecture however permits control of more complex 'smart' environments and could be extended further for entertainment by interacting with media devices. Using components of the electroencephalogram (EEG) to mediate expression is also technically possible, but is much more speculative, and without proven efficacy. The IGUI-BCI approach described could potentially find wider use in the augmentation of the general population, to provide alternative computer interaction, an additional control channel and experimental leisure activities.",
    "keywords": [
      "entertainment",
      "user interface",
      "domotic control",
      "brain computer interfaces"
    ],
    "doi": "10.1145/1785455.1785461",
    "url": "https://doi.org/10.1145/1785455.1785461",
    "citations": 11,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "6",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785462",
    "title": "Emotion Detection Using Noisy EEG Data",
    "authors": [
      "Mina Mikhail",
      "Khaled El-Ayat",
      "Rana El Kaliouby",
      "James Coan",
      "John J. B. Allen"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Emotion is an important aspect in the interaction between humans. It is fundamental to human experience and rational decision-making. There is a great interest for detecting emotions automatically. A number of techniques have been employed for this purpose using channels such as voice and facial expressions. However, these channels are not very accurate because they can be affected by users' intentions. Other techniques use physiological signals along with electroencephalography (EEG) for emotion detection. However, these approaches are not very practical for real time applications because they either ask the participants to reduce any motion and facial muscle movement or reject EEG data contaminated with artifacts. In this paper, we propose an approach that analyzes highly contaminated EEG data produced from a new emotion elicitation technique. We also use a feature selection mechanism to extract features that are relevant to the emotion detection task based on neuroscience findings. We reached an average accuracy of 51% for joy emotion, 53% for anger, 58% for fear and 61% for sadness.",
    "keywords": [
      "feature extraction",
      "support vector machines",
      "affective computing",
      "brain signals"
    ],
    "doi": "10.1145/1785455.1785462",
    "url": "https://doi.org/10.1145/1785455.1785462",
    "citations": 40,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "7",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785463",
    "title": "World's First Wearable Humanoid Robot That Augments Our Emotions",
    "authors": [
      "Dzmitry Tsetserukou",
      "Alena Neviarouskaya"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In the paper we are proposing a conceptually novel approach to reinforcing (intensifying) own feelings and reproducing (simulating) the emotions felt by the partner during online communication through wearable humanoid robot. The core component, Affect Analysis Model, automatically recognizes nine emotions from text. The detected emotion is stimulated by innovative haptic devices integrated into the robot. The implemented system can considerably enhance the emotionally immersive experience of real-time messaging. Users can not only exchange messages but also emotionally and physically feel the presence of the communication partner (e.g., family member, friend, or beloved person).",
    "keywords": [
      "3D world",
      "haptic communication",
      "haptic display",
      "online communication",
      "wearable humanoid robot",
      "tactile display",
      "instant messaging",
      "affective user interfaces"
    ],
    "doi": "10.1145/1785455.1785463",
    "url": "https://doi.org/10.1145/1785455.1785463",
    "citations": 31,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "8",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785464",
    "title": "KIBITZER: A Wearable System for Eye-Gaze-Based Mobile Urban Exploration",
    "authors": [
      "Matthias Baldauf",
      "Peter Fröhlich",
      "Siegfried Hutter"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Due to the vast amount of available georeferenced information novel techniques to more intuitively and efficiently interact with such content are increasingly required. In this paper, we introduce KIBITZER, a lightweight wearable system that enables the browsing of urban surroundings for annotated digital information. KIBITZER exploits its user's eye-gaze as natural indicator of attention to identify objects-of-interest and offers speech- and non-speech auditory feedback. Thus, it provides the user with a 6th sense for digital georeferenced information. We present a description of our system's architecture and the interaction technique and outline experiences from first functional trials.",
    "keywords": [
      "mobile spatial interaction",
      "wearable computing",
      "eye-gaze"
    ],
    "doi": "10.1145/1785455.1785464",
    "url": "https://doi.org/10.1145/1785455.1785464",
    "citations": 42,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "9",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1785455.1785465",
    "title": "Airwriting Recognition Using Wearable Motion Sensors",
    "authors": [
      "Christoph Amma",
      "Dirk Gehrig",
      "Tanja Schultz"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this work we present a wearable input device which enables the user to input text into a computer. The text is written into the air via character gestures, like using an imaginary blackboard. To allow hands-free operation, we designed and implemented a data glove, equipped with three gyroscopes and three accelerometers to measure hand motion. Data is sent wirelessly to the computer via Bluetooth. We use HMMs for character recognition and concatenated character models for word recognition. As features we apply normalized raw sensor signals. Experiments on single character and word recognition are performed to evaluate the end-to-end system. On a character database with 10 writers, we achieve an average writer-dependent character recognition rate of 94.8% and a writer-independent character recognition rate of 81.9%. Based on a small vocabulary of 652 words, we achieve a single-writer word recognition rate of 97.5%, a performance we deem is advisable for many applications. The final system is integrated into an online word recognition demonstration system to showcase its applicability.",
    "keywords": [],
    "doi": "10.1145/1785455.1785465",
    "url": "https://doi.org/10.1145/1785455.1785465",
    "citations": 76,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785466",
    "title": "Augmenting the Driver's View with Realtime Safety-Related Information",
    "authors": [
      "Peter Fröhlich",
      "Raimund Schatz",
      "Peter Leitner",
      "Matthias Baldauf",
      "Stephan Mantler"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In the last couple of years, in-vehicle information systems have advanced in terms of design and technical sophistication. This trend manifests itself in the current evolution of navigation devices towards advanced 3D visualizations as well as real-time telematics services. We present important constituents for the design space of realistic visualizations in the car and introduce realization potentials in advanced vehicle-to-infrastructure application scenarios. To evaluate this design space, we conducted a driving simulator study, in which the in-car HMI was systematically manipulated with regard to its representation of the outside world. The results show that in the context of safety-related applications, realistic views provide higher perceived safety than with traditional visualization styles, despite their higher visual complexity. We also found that the more complex the safety recommendation the HMI has to communicate, the more drivers perceive a realistic visualization as a valuable support. In a comparative inquiry after the experiment, we found that egocentric and bird's eye perspectives are preferred to top-down perspectives for safety-related in-car safety information systems.",
    "keywords": [
      "realistic visualization",
      "telematics",
      "user studies"
    ],
    "doi": "10.1145/1785455.1785466",
    "url": "https://doi.org/10.1145/1785455.1785466",
    "citations": 8,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "11",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1785455.1785467",
    "title": "An Experimental Augmented Reality Platform for Assisted Maritime Navigation",
    "authors": [
      "Olivier Hugues",
      "Jean-Marc Cieutat",
      "Pascal Guitton"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "This paper deals with integrating a vision system with an efficient thermal camera and a classical one in maritime navigation software based on a virtual environment (VE). We then present an exploratory field of augmented reality (AR) in situations of mobility and the different applications linked to work at sea provided by adding this functionality. This work was carried out thanks to a CIFRE agreement within the company MaxSea Int.",
    "keywords": [
      "mixed environment",
      "combining exteroceptive data",
      "human factor",
      "image processing",
      "augmented reality"
    ],
    "doi": "10.1145/1785455.1785467",
    "url": "https://doi.org/10.1145/1785455.1785467",
    "citations": 20,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "12",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785468",
    "title": "Skier-Ski System Model and Development of a Computer Simulation Aiming to Improve Skier's Performance and Ski",
    "authors": [
      "François Roux",
      "Gilles Dietrich",
      "Aude-Clémence Doix"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Background. Based on personal experience of ski teaching, ski training and ski competing, we have noticed that some gaps exist between classical models describing body-techniques and actual motor acts made by performing athletes. The evolution of new parabolic shaped skis with new mechanical and geometric characteristics increase these differences even more. Generally, scientific research focuses on situations where skiers are separated from their skis. Also, many specialized magazines, handbooks and papers print articles with similar epistemology. In this paper, we describe the development of a three-dimensional analysis to model the skier-skis' system. We subsequently used the model to propose an evaluation template to coaches that includes eight techniques and three observable consequences in order to make objective evaluations of their athletes' body-techniques. Once the system is modeled, we can develop a computer simulation in the form of a jumping jack, respecting degrees of freedom of the model. We can manipulate movement of each body segment or skis' gears' characteristics to detect performance variations. The purpose of this project is to elaborate assumptions to improve performance and propose experimental protocols to coaches to enable them to evaluate performance. This computer simulation also involves board and wheeled sports.Methods. Eleven elite alpine skiers participated. Video cameras were used to observe motor acts in alpine skiers in two tasks: slalom and giant slalom turns. Kinematic data were input into the 3D Vision software. Two on-board balances were used to measure the six components of ski-boots→skis torques. All data sources were then synchronized.Findings. We found correlations between force and torque measurements, the progression of center of pressure and the eight body-techniques. Based on these results, we created a technological model of the skier-ski system. Then, we have made a reading template and a model to coach young alpine skiers in clubs and world cup alpine skiers and, we have obtained results demonstrating the usefulness of our research.Interpretation. These results suggest that it is now possible to create a three-dimensional simulator of an alpine skier. This tool is able to compare competitors' body-techniques to detect the most performing body-techniques. Additionally, it is potentially helpful to consider and evaluate new techniques and ski characteristics.",
    "keywords": [
      "computer simulation",
      "techniques reading template",
      "elite skiing",
      "skier-ski system"
    ],
    "doi": "10.1145/1785455.1785468",
    "url": "https://doi.org/10.1145/1785455.1785468",
    "citations": 2,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "13",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785469",
    "title": "T.A.C: Augmented Reality System for Collaborative Tele-Assistance in the Field of Maintenance through Internet",
    "authors": [
      "Sébastien Bottecchia",
      "Jean-Marc Cieutat",
      "Jean-Pierre Jessel"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this paper we shall present the T.A.C. (Télé-Assistance-Collaborative) system whose aim is to combine remote collaboration and industrial maintenance. T.A.C. enables the copresence of parties within the framework of a supervised maintenance task to be remotely \"simulated\" thanks to augmented reality (AR) and audio-video communication. To support such cooperation, we propose a simple way of interacting through our O.A.P. paradigm and AR goggles specially developed for the occasion. The handling of 3D items to reproduce gestures and an additional knowledge management tool (e-portfolio, feedback, etc) also enables this solution to satisfy the new needs of industry.",
    "keywords": [
      "computer vision",
      "augmented reality",
      "collaboration",
      "TeleAssistance",
      "cognitive psychology"
    ],
    "doi": "10.1145/1785455.1785469",
    "url": "https://doi.org/10.1145/1785455.1785469",
    "citations": 59,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "14",
    "numpages": "7",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/1785455.1785470",
    "title": "Designing and Evaluating Advanced Interactive Experiences to Increase Visitor's Stimulation in a Museum",
    "authors": [
      "Bénédicte Schmitt",
      "Cedric Bach",
      "Emmanuel Dubois",
      "Francis Duranthon"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this paper, we describe the design and a pilot study of two Mixed Interactive Systems (MIS), interactive systems combining digital and physical artifacts. These MIS aim at stimulating visitors of a Museum of Natural History about a complex phenomenon. This phenomenon is the pond eutrophication that is a breakdown of a dynamical equilibrium caused by human activities: this breakdown results in a pond unfit for life. This paper discusses the differences between these two MIS prototypes, the design process that lead to their implementation and the dimensions used to evaluate these prototypes: user experience (UX), usability of the MIS and the users' understanding of the eutrophication phenomenon.",
    "keywords": [
      "eutrophication",
      "advanced interactive experience",
      "mixed interactive systems",
      "co-design",
      "museology"
    ],
    "doi": "10.1145/1785455.1785470",
    "url": "https://doi.org/10.1145/1785455.1785470",
    "citations": 8,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "15",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785471",
    "title": "Partial Matching of Garment Panel Shapes with Dynamic Sketching Design",
    "authors": [
      "Shuang Liang",
      "Rong-Hua Li",
      "George Baciu",
      "Eddie C. L. Chan",
      "Dejun Zheng"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Fashion industry and textile manufacturing in past decade, have been starting to reapply enhanced intelligent CAD process technologies. In this paper, we propose a partial panel matching system to facilitate the typical garment design process. This process provides recommendations to the designer during the panel design process and performs partial matching of the garment panel shapes. There are three main parts in our partial matching system. First, we make use of a Bézier-based sketch regularization to pre-process the panel sketch data. Second, we propose a set of bi-segment panel shape descriptors to describe and enrich the local features of the shape for partial matching. Finally, based on our previous work, we add an interactive sketching input environment to design garments. Experiment results show the effectiveness and efficiency of the proposed system.",
    "keywords": [],
    "doi": "10.1145/1785455.1785471",
    "url": "https://doi.org/10.1145/1785455.1785471",
    "citations": 8,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "16",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785472",
    "title": "Fur Interface with Bristling Effect Induced by Vibration",
    "authors": [
      "Masahiro Furukawa",
      "Yuji Uema",
      "Maki Sugimoto",
      "Masahiko Inami"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Wearable computing technology is one of the methods that can augment the information processing ability of humans. However, in this area, a soft surface is often necessary to maximize the comfort and practicality of such wearable devices. Thus in this paper, we propose a soft surface material, with an organic bristling effect achieved through mechanical vibration, as a new user interface. We have used fur in order to exhibit the visually rich transformation induced by the bristling effect while also achieving the full tactile experience and benefits of soft materials. Our method needs only a layer of fur and simple vibration motors. The hairs of fur instantly bristle with only horizontal mechanical vibration. The vibration is provided by a simple vibration motor embedded below the fur material. This technology has significant potential as garment textiles or to be utilized as a general soft user interface.",
    "keywords": [
      "physical computer interfaces",
      "computational fashion",
      "soft user interface",
      "visual and haptic design",
      "computational clothing",
      "pet robot"
    ],
    "doi": "10.1145/1785455.1785472",
    "url": "https://doi.org/10.1145/1785455.1785472",
    "citations": 27,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "17",
    "numpages": "6",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1785455.1785473",
    "title": "Evaluating Cross-Sensory Perception of Superimposing Virtual Color onto Real Drink: Toward Realization of Pseudo-Gustatory Displays",
    "authors": [
      "Takuji Narumi",
      "Munehiko Sato",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this research, we aim to realize a gustatory display that enhances our experience of enjoying food. However, generating a sense of taste is very difficult because the human gustatory system is quite complicated and is not yet fully understood. This is so because gustatory sensation is based on chemical signals whereas visual and auditory sensations are based on physical signals. In addition, the brain perceives flavor by combining the senses of gustation, smell, sight, warmth, memory, etc. The aim of our research is to apply the complexity of the gustatory system in order to realize a pseudo-gustatory display that presents flavors by means of visual feedback. This paper reports on the prototype system of such a display that enables us to experience various tastes without changing their chemical composition through the superimposition of virtual color. The fundamental thrust of our experiment is to evaluate the influence of cross-sensory effects by superimposing virtual color onto actual drinks and recording the responses of subjects who drink them. On the basis of experimental results, we concluded that visual feedback sufficiently affects our perception of flavor to justify the construction of pseudo-gustatory displays.",
    "keywords": [
      "cross-sensory perception",
      "pseudo-gustation",
      "gustatory display"
    ],
    "doi": "10.1145/1785455.1785473",
    "url": "https://doi.org/10.1145/1785455.1785473",
    "citations": 46,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "18",
    "numpages": "6",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1785455.1785474",
    "title": "The Reading Glove: Designing Interactions for Object-Based Tangible Storytelling",
    "authors": [
      "Theresa Jean Tanenbaum",
      "Karen Tanenbaum",
      "Alissa Antle"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this paper we describe a prototype Tangible User Interface (TUI) for interactive storytelling that explores the semantic properties of tangible interactions using the fictional notion of psychometry as inspiration. We propose an extension of Heidegger's notions of \"ready-to-hand\" and \"present-at-hand\", which allows them to be applied to the narrative and semantic aspects of an interaction. The Reading Glove allows interactors to extract narrative \"memories\" from a collection of ten objects using natural grasping and holding behaviors via a wearable interface. These memories are presented in the form of recorded audio narration. We discuss the design process and present some early results from an informal pilot study intended to refine these design techniques for future tangible interactive narratives.",
    "keywords": [
      "object stories",
      "interactive narrative",
      "wearable computing",
      "tangible user interfaces"
    ],
    "doi": "10.1145/1785455.1785474",
    "url": "https://doi.org/10.1145/1785455.1785474",
    "citations": 49,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "19",
    "numpages": "9",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/1785455.1785475",
    "title": "Control of Augmented Reality Information Volume by Glabellar Fader",
    "authors": [
      "Hiromi Nakamura",
      "Homei Miyashita"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In this paper, we propose a device for controlling the volume of augmented reality information by the glabellar movement. Our purpose is to avoid increasing the sum of the amount of information during the perception of \"Real Space +Augmented Reality\" by an intuitive and seamless control. For this, we focused on the movement of the glabella (between the eyebrows) when the user stare at objects as a trigger of information presentation. The system detects the movement of the eyebrows by the amount of the light reflected by a photo-reflector, and controlling information volume or the transparency of objects in augmented reality space.",
    "keywords": [
      "information volume",
      "photo reflector",
      "glabellar"
    ],
    "doi": "10.1145/1785455.1785475",
    "url": "https://doi.org/10.1145/1785455.1785475",
    "citations": 14,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "20",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785476",
    "title": "Towards Mobile/Wearable Device Electrosmog Reduction through Careful Network Selection",
    "authors": [
      "Jean-Marc Seigneur",
      "Xavier Titi",
      "Tewfiq El Maliki"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "There is some concern regarding the effect of smart phones and other wearable devices using wireless communication and worn by the users very closely to their body. In this paper, we propose a new network switching selection model and its algorithms that minimize the non-ionizing radiation of these devices during use. We validate the model and its algorithms with a proof-of-concept implementation on the Android platform.",
    "keywords": [
      "wireless hand-over",
      "electrosmog"
    ],
    "doi": "10.1145/1785455.1785476",
    "url": "https://doi.org/10.1145/1785455.1785476",
    "citations": 12,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "21",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785477",
    "title": "Bouncing Star Project: Design and Development of Augmented Sports Application Using a Ball Including Electronic and Wireless Modules",
    "authors": [
      "Osamu Izuta",
      "Toshiki Sato",
      "Sachiko Kodama",
      "Hideki Koike"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "In our project, we created a new ball, \"Bouncing Star\" (Hane-Boshi in Japanese), comprised of electronic devices. We also created augmented sports system using Bouncing Star and a computer program to support an interface between the digital and the physical world. This program is able to recognize the ball's state of motion (static, rolled, thrown, bound, etc.) by analyzing data received through a wireless module. The program also tracks the ball's position through image recognition techniques. On this system, we developed augmented sports applications which integrate real time dynamic computer graphics and responsive sounds which are synchronized with the ball's characteristics of motion. Our project's goal is to establish a new dynamic form of entertainment which can be realized through the combination of the ball and digital technologies.",
    "keywords": [
      "wireless module",
      "interactive surface",
      "image recognition",
      "sensing technology",
      "computer-supported cooperative play",
      "ball interface",
      "augmented sports"
    ],
    "doi": "10.1145/1785455.1785477",
    "url": "https://doi.org/10.1145/1785455.1785477",
    "citations": 33,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "22",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785478",
    "title": "On-Line Document Registering and Retrieving System for AR Annotation Overlay",
    "authors": [
      "Hideaki Uchiyama",
      "Julien Pilet",
      "Hideo Saito"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "We propose a system that registers and retrieves text documents to annotate them on-line. The user registers a text document captured from a nearly top view and adds virtual annotations. When the user thereafter captures the document again, the system retrieves and displays the appropriate annotations, in real-time and at the correct location. Registering and deleting documents is done by user interaction. Our approach relies on LLAH, a hashing based method for document image retrieval. At the on-line registering stage, our system extracts keypoints from the input image and stores their descriptors computed from their neighbors. After registration, our system can quickly find the stored document corresponding to an input view by matching keypoints. From the matches, our system estimates the geometrical relationship between the camera and the document for accurately overlaying the annotations. In the experimental results, we show that our system can achieve on-line and real-time performances.",
    "keywords": [
      "Poes estimation",
      "augmented reality",
      "document retrieval",
      "feature matching",
      "LLAH"
    ],
    "doi": "10.1145/1785455.1785478",
    "url": "https://doi.org/10.1145/1785455.1785478",
    "citations": 4,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "23",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1785455.1785479",
    "title": "Augmenting Human Memory Using Personal Lifelogs",
    "authors": [
      "Yi Chen",
      "Gareth J. F. Jones"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Memory is a key human facility to support life activities, including social interactions, life management and problem solving. Unfortunately, our memory is not perfect. Normal individuals will have occasional memory problems which can be frustrating, while those with memory impairments can often experience a greatly reduced quality of life. Augmenting memory has the potential to make normal individuals more effective, and those with significant memory problems to have a higher general quality of life. Current technologies are now making it possible to automatically capture and store daily life experiences over an extended period, potentially even over a lifetime. This type of data collection, often referred to as a personal life log (PLL), can include data such as continuously captured pictures or videos from a first person perspective, scanned copies of archival material such as books, electronic documents read or created, and emails and SMS messages sent and received, along with context data of time of capture and access and location via GPS sensors. PLLs offer the potential for memory augmentation. Existing work on PLLs has focused on the technologies of data capture and retrieval, but little work has been done to explore how these captured data and retrieval techniques can be applied to actual use by normal people in supporting their memory. In this paper, we explore the needs for augmenting human memory from normal people based on the psychology literature on mechanisms about memory problems, and discuss the possible functions that PLLs can provide to support these memory augmentation needs. Based on this, we also suggest guidelines for data for capture, retrieval needs and computer-based interface design. Finally we introduce our work-in-process prototype PLL search system in the iCLIPS project to give an example of augmenting human memory with PLLs and computer based interfaces.",
    "keywords": [
      "personal information archives",
      "augmented human memory",
      "context-aware retrieval",
      "lifelogs"
    ],
    "doi": "10.1145/1785455.1785479",
    "url": "https://doi.org/10.1145/1785455.1785479",
    "citations": 64,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "24",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1785455.1785480",
    "title": "Aided Eyes: Eye Activity Sensing for Daily Life",
    "authors": [
      "Yoshio Ishiguro",
      "Adiyan Mujibiya",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2010,
    "conference": "AH",
    "conferenceYear": "AH '10",
    "abstract": "Our eyes collect a considerable amount of information when we use them to look at objects. In particular, eye movement allows us to gaze at an object and shows our level of interest in the object. In this research, we propose a method that involves real-time measurement of eye movement for human memory enhancement; the method employs gaze-indexed images captured using a video camera that is attached to the user's glasses. We present a prototype system with an infrared-based corneal limbus tracking method. Although the existing eye tracker systems track eye movement with high accuracy, they are not suitable for daily use because the mobility of these systems is incompatible with a high sampling rate. Our prototype has small phototransistors, infrared LEDs, and a video camera, which make it possible to attach the entire system to the glasses. Additionally, the accuracy of this method is compensated by combining image processing methods and contextual information, such as eye direction, for information extraction. We develop an information extraction system with real-time object recognition in the user's visual attention area by using the prototype of an eye tracker and a head-mounted camera. We apply this system to (1) fast object recognition by using a SURF descriptor that is limited to the gaze area and (2) descriptor matching of a past-images database. Face recognition by using haar-like object features and text logging by using OCR technology is also implemented. The combination of a low-resolution camera and a high-resolution, wide-angle camera is studied for high daily usability. The possibility of gaze-guided computer vision is discussed in this paper, as is the topic of communication by the photo transistor in the eye tracker and the development of a sensor system that has a high transparency.",
    "keywords": [
      "information extracting for lifelog",
      "lifelog computing",
      "eye tracking",
      "gaze information"
    ],
    "doi": "10.1145/1785455.1785480",
    "url": "https://doi.org/10.1145/1785455.1785480",
    "citations": 77,
    "booktitle": "Proceedings of the 1st Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "25",
    "numpages": "7",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/1959826.1959827",
    "title": "Interacting with smart walls: a multi-dimensional analysis of input technologies for augmented environments",
    "authors": [
      "Felix Heidrich",
      "Martina Ziefle",
      "Carsten Röcker",
      "Jan Borchers"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "This paper reports on a multi-dimensional evaluation of three typical interaction devices for wall-sized displays in augmented environments. Touch, trackpad and gesture input were evaluated regarding a variety of usability dimensions in order to understand the quality profile of each input device. Among the three interaction devices, the touch input showed the highest scores in performance and acceptance as well as hedonic value.",
    "keywords": [
      "trackpad",
      "touch input",
      "smart displays",
      "physical strain",
      "performance",
      "large displays",
      "interaction",
      "input devices",
      "gesture input",
      "acceptance"
    ],
    "doi": "10.1145/1959826.1959827",
    "url": "https://doi.org/10.1145/1959826.1959827",
    "citations": 26,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "1",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1959826.1959828",
    "title": "Interactive bookshelf surface for in situ book searching and storing support",
    "authors": [
      "Kazuhiro Matsushita",
      "Daisuke Iwai",
      "Kosuke Sato"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We propose an interactive bookshelf surface to augment a human ability for in situ book searching and storing. In book searching support, when a user touches the edge of the bookshelf, the cover image of a stored book located above the touched position is projected directly onto the book spine. As a result, the user can search for a desired book by sliding his (or her) finger across the shelf edge. In book storing support, when a user brings a book close to the bookshelf, the place where the book should be stored is visually highlighted by a projection light. This paper also presents sensing technologies to achieve the above mentioned interactive techniques. In addition, by considering the properties of the human visual system, we propose a simple visual effect to reduce the legibility degradation of the projected image contents by the complex textures and geometric irregularities of the spines. We confirmed the feasibility of the system and the effectiveness of the proposed interaction techniques through user studies.",
    "keywords": [
      "smart bookshelf",
      "projection-based mixed reality",
      "book searching and storing support"
    ],
    "doi": "10.1145/1959826.1959828",
    "url": "https://doi.org/10.1145/1959826.1959828",
    "citations": 26,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "2",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959829",
    "title": "Homunculus: the vehicle as augmented clothes",
    "authors": [
      "Yoichi Ochiai",
      "Keisuke Toyoshima"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper we propose to add a new system with valuable functionalities to vehicles. We call it \"Homunculus\". It is based on a new concept of interactions between humans and vehicles. It promotes and augments nonverbal communicability of humans in the vehicles.It is difficult to communicate with the drivers in the vehicles by eye contact, hand gestures or touching behavior. Our \"Homunculus\" is a system to solve these problems. The instruments of \"Homunculus\" are composed of three system modules. The First is Robotic Eyes System which is a set of robotic eyes that follows drivers eye movements &amp; head rotations. The Second is Projection System which shows drivers hand gestures on the road. The Third is Haptic Communication System which consists of IR Distance Sensors Array on the vehicle and Vibration motors attached to the driver. It gives drivers the haptic sense to approaching objects to the vehicle. These three Systems are set on vehicle's hood or side.We propose the situation that humans and vehicles can be unified as one unit by Homunculus. This system works as a middleman for communications between men and vehicles, people in other cars, or even people just walking the street. We suggest the new relationship of men and their vehicles could be like men and their clothes.",
    "keywords": [
      "vehicle",
      "projection",
      "physical feedback",
      "eye-tracking"
    ],
    "doi": "10.1145/1959826.1959829",
    "url": "https://doi.org/10.1145/1959826.1959829",
    "citations": 16,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "3",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959830",
    "title": "Full body interaction for serious games in motor rehabilitation",
    "authors": [
      "Christian Schönauer",
      "Thomas Pintaric",
      "Hannes Kaufmann"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Serious games and especially their use in healthcare applications are an active and rapidly growing area of research. A key aspect of games in rehabilitation is 3D input. In this paper we present our implementation of a full body motion capture (MoCap) system, which, together with a biosignal acquisition device, has been integrated in a game engine. Furthermore, a workflow has been established that enables the use of acquired skeletal data for serious games in a medical environment. Finally, a serious game has been implemented, targeting rehabilitation of patients with chronic pain of the lower back and neck, a group that has previously been neglected by serious games. The focus of this work is on the full body MoCap system and its integration with biosignal devices and the game engine. A short overview of the application and prelimiary results are provided.",
    "keywords": [
      "serious games",
      "motor rehabilitation",
      "motion capture"
    ],
    "doi": "10.1145/1959826.1959830",
    "url": "https://doi.org/10.1145/1959826.1959830",
    "citations": 75,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "4",
    "numpages": "8",
    "influentialCitations": 4
  },
  {
    "id": "10.1145/1959826.1959831",
    "title": "The PhantomStation: towards funneling remote tactile feedback on interactive surfaces",
    "authors": [
      "Hendrik Richter",
      "Alina Hang",
      "Benedikt Blaha"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We present the PhantomStation, a novel interface that communicates tactile feedback to remote parts of the user's body. Thus, touch input on interactive surfaces can be augmented with synchronous tactile sensations. With the objective to reduce the number of tactile actuators on the user's body, we use the psychophysical Phantom Sensation (PhS) [1]. This illusion occurs when two or more tactile stimuli are presented simultaneously to the skin. The location of the pseudo-tactile sensation can be changed by modulating intensity or interstimulus time interval. We compare three different actuator technologies to recreate the PhS. Furthermore, we discuss how remote tactile of this kind can improve interaction accuracy. We present our prototype and propose scenarios in conjunction with interactive surfaces.",
    "keywords": [
      "tactile feedback",
      "interactive surfaces",
      "funneling illusion",
      "PhantomStation",
      "Phantom Sensation"
    ],
    "doi": "10.1145/1959826.1959831",
    "url": "https://doi.org/10.1145/1959826.1959831",
    "citations": 13,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "5",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959832",
    "title": "Acquisition of 3D gaze information from eyeball movements using inside-out camera",
    "authors": [
      "Shoichi Shimizu",
      "Hironobu Fujiyoshi"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We propose a method for obtaining 3D gaze information using inside-out camera. Such information on 3D gaze points can be useful not only to clarify higher cognitive processes in humans but also to reproduce the 3D shape of an object from eyeball movement simply by gazing at the object as an extension of the visual function. Using half-mirrors, an inside-out camera can capture a person's eyeball head-on and can capture the person's visual field from a position equivalent to that of the eyeball. Here, the relationship between the gaze vector obtained from images of the eyeball and the gaze point in images capturing the visual field is expressed by a conversion equation. The 3D position of the gaze point can then be estimated by using stereo constraints in two scene cameras. In an evaluation experiment, the gaze point could be estimated with an average error of about 15 pixels, and we also showed the 3D scan path obtained by the proposed method from eyeball movement by gazing at the object.",
    "keywords": [
      "stereo vision",
      "inside-out camera",
      "3D gaze point"
    ],
    "doi": "10.1145/1959826.1959832",
    "url": "https://doi.org/10.1145/1959826.1959832",
    "citations": 16,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "6",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959833",
    "title": "Flying sports assistant: external visual imagery representation for sports training",
    "authors": [
      "Keita Higuchi",
      "Tetsuro Shimada",
      "Jun Rekimoto"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Mental imagery is a quasi-perceptual experience emerging from past experiences. In sports psychology, mental imagery is used to improve athletes' cognition and motivation. Eminent athletes often create their mental imagery as if they themselves are the external observers; such ability plays an important role in sport training and performance. Mental image visualization refers to the representation of external vision containing one's own self from the perspective of others. However, without technological support, it is difficult to obtain accurate external visual imagery during sports. In this paper, we have proposed a system that has an aerial vehicle (a quadcopter) to capture athletes' external visual imagery. The proposed system integrates various sensor data to autonomously track the target athlete and compute camera angle and position. The athlete can see the captured image in realtime through a head mounted display, or more recently through a hand-held device. We have applied this system to support soccer and other sports and discussed how the proposed system can be used during training.",
    "keywords": [
      "sports assistant",
      "mental imagery",
      "aerial vehicle"
    ],
    "doi": "10.1145/1959826.1959833",
    "url": "https://doi.org/10.1145/1959826.1959833",
    "citations": 68,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "7",
    "numpages": "4",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/1959826.1959834",
    "title": "Peripheral vision annotation: noninterference information presentation method for mobile augmented reality",
    "authors": [
      "Yoshio Ishiguro",
      "Jun Rekimoto"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Augmented-reality (AR) systems present information about a user's surrounding environment by overlaying it on the user's real-world view. However, such overlaid information tends to obscure a user's field of view and thus impedes a user's real-world activities. This problem is especially critical when a user is wearing a head-mounted display. In this paper, we propose an information presentation mechanism for mobile AR systems by focusing on the user's gaze information and peripheral vision field. The gaze information is used to control the positions and the level-of-detail of the information overlaid on the user's field of view. We also propose a method for switching displayed information based on the difference in human visual perception between the peripheral and central visual fields. We develop a mobile AR system to test our proposed method consisting of a gaze-tracking system and a retinal imaging display. The eye-tracking system estimates whether the user's visual focus is on the information display area or not, and changes the information type from simple to detailed information accordingly.",
    "keywords": [
      "wearable AR",
      "retinal imaging display",
      "eye gaze"
    ],
    "doi": "10.1145/1959826.1959834",
    "url": "https://doi.org/10.1145/1959826.1959834",
    "citations": 65,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "8",
    "numpages": "5",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/1959826.1959835",
    "title": "Ego-motion analysis using average image data intensity",
    "authors": [
      "Kojiro Kato",
      "Kris M. Kitani",
      "Takuya Nojima"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we present a new method to perform ego-motion analysis using intensity averaging of image data. The method can estimate general motions from two sequential images on pixel plane by calculating cross correlations. With distance information between camera and objects, this method also enables estimates of camera motion. This method is sufficiently robust even for out of focus image and the calculational overhead is quite low because it uses a simple averaging method. In the future, this method could be used to measure fast motions such as human head tracking, or robot movement. We present a detailed description of the proposed method, and experimental results demonstrating its basic capability. With these results, we verify that our proposed system can detect camera motion even with blurred images. Furthermore, we confirm that it can operate at up to 714 FPS in calculating one dimensional translation motion.",
    "keywords": [
      "image processing",
      "ego-motion estimation",
      "correlation",
      "averaging image"
    ],
    "doi": "10.1145/1959826.1959835",
    "url": "https://doi.org/10.1145/1959826.1959835",
    "citations": 0,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "9",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959836",
    "title": "Weight illusion by tangential deformation of forearm skin",
    "authors": [
      "Yuki Kuniyasu",
      "Shogo Fukushima",
      "Masahiro Furukawa",
      "Hiroyuki Kajimoto"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "When we perform exercise or undergo rehabilitation, it is helpful to be supported by another person. To get this support, we normally take hold of a person's arm, and pull it. In this paper, we investigate the use of a special device to produce a \"pulling arm\" sensation on the forearm. Using a weight comparison task, we performed an experiment to confirm the sensation of illusory external force with our device. We concluded that our current device presented about 10g to 20g weight perception.",
    "keywords": [
      "pulling hand",
      "forearm skin",
      "force sensation"
    ],
    "doi": "10.1145/1959826.1959836",
    "url": "https://doi.org/10.1145/1959826.1959836",
    "citations": 8,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "10",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959837",
    "title": "EdgeSonic: image feature sonification for the visually impaired",
    "authors": [
      "Tsubasa Yoshida",
      "Kris M. Kitani",
      "Hideki Koike",
      "Serge Belongie",
      "Kevin Schlei"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We propose a framework to aid a visually impaired user to recognize objects in an image by sonifying image edge features and distance-to-edge maps. Visually impaired people usually touch objects to recognize their shape. However, it is difficult to recognize objects printed on flat surfaces or objects that can only be viewed from a distance, solely with our haptic senses. Our ultimate goal is to aid a visually impaired user to recognize basic object shapes, by transposing them to aural information. Our proposed method provides two types of image sonification: (1) local edge gradient sonification and (2) sonification of the distance to the closest image edge. Our method was implemented on a touch-panel mobile device, which allows the user to aurally explore image context by sliding his finger across the image on the touch screen. Preliminary experiments show that the combination of local edge gradient sonification and distance-to-edge sonification are effective for understanding basic line drawings. Furthermore, our tests show a significant improvement in image understanding with the introduction of proper user training.",
    "keywords": [
      "visually impaired",
      "sensory substitution",
      "image sonification",
      "edge detection"
    ],
    "doi": "10.1145/1959826.1959837",
    "url": "https://doi.org/10.1145/1959826.1959837",
    "citations": 70,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "11",
    "numpages": "4",
    "influentialCitations": 9
  },
  {
    "id": "10.1145/1959826.1959838",
    "title": "An augmented reality learning space for PC DIY",
    "authors": [
      "Heien-Kun Chiang",
      "Yin-Yu Chou",
      "Long-Chyr Chang",
      "Chun-Yen Huang",
      "Feng-Lan Kuo",
      "Hown-Wen Chen"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Because of the advances of computer hardware and software, Computer Aided Instruction (CAI) makes learning effective and interesting through the use of interactive multimedia technology. Recently, Augmented Reality (AR) technology has begun to surge as a new CAI tool because of its ability to create tangible and highly interactive user interface. In addition, recent studies have shown that the learning content as well as the participation of learners in learning activities can greatly affect learners' learning performance. However, studies of the integration of PC DIY (Personal Computer Do It Yourself) learning with AR technology are still few in current literature. Therefore, this study proposes an AR learning space for PC DIY whose system architecture and implementation are detailed. To evaluate the usability of the proposed system, a questionnaire is given to twenty-six graduate students after their hands-on experience with the prototype. Results of the questionnaire show the proposed AR learning space for PC DIY offers students a motivating, pleasant, and satisfying learning experience. Limitation, conclusion and future studies are given.",
    "keywords": [
      "computer aided instruction",
      "augmented reality",
      "PC DIY"
    ],
    "doi": "10.1145/1959826.1959838",
    "url": "https://doi.org/10.1145/1959826.1959838",
    "citations": 8,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "12",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1959826.1959839",
    "title": "Audiolizing body movement: its concept and application to motor skill learning",
    "authors": [
      "Naoyuki Houri",
      "Hiroyuki Arita",
      "Yutaka Sakaguchi"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We propose a concept of \"audiolization of body movement,\" which transforms the posture/movement of the human body or human-controlled-tools into acoustic signals and feeds them back to the users in a real-time manner. It aims at helping people being aware of their body/tool states, and resultantly assisting their motor skill learning. The present paper describes features of the concepts and introduces some demonstrative applications.",
    "keywords": [
      "sensori-motor integration",
      "motor skill learning",
      "body movement",
      "body awareness",
      "audiolization"
    ],
    "doi": "10.1145/1959826.1959839",
    "url": "https://doi.org/10.1145/1959826.1959839",
    "citations": 5,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "13",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959840",
    "title": "ClippingLight: a method for easy snapshots with projection viewfinder and tilt-based zoom control",
    "authors": [
      "Yasuhiro Kajiwara",
      "Keisuke Tajimi",
      "Keiji Uemura",
      "Nobuchika Sakata",
      "Shogo Nishida"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we present a novel method to take photos with a hand-held camera. Cameras are being used for new purposes in our daily lives these days, such as to augment human memory or scan visual markers (e.g. QR-codes) and opportunities to take snapshots are increasing. However, taking snapshots with today's hand-held camera is troublesome, because its viewfinder forces the user to see the real space through itself, and it requires complicated operation to control zoom levels and press a shutter-release button at the same time. Therefore, we propose ClippingLight that is a combination method of Projection Viewfinder and tilt-based zoom control. It enables to take snapshots with low effort. We implement this method using a prototype of real-world projection camera. We conducted user study to confirm the effect of CippingLight in situations to take photos one after another. As a result, we found that ClippingLight is more comfortable and requires lower effort than today's typical camera when a user takes a photo quickly.",
    "keywords": [
      "tilt-based control",
      "spatial input",
      "single handed",
      "mobile devices",
      "micro-projector",
      "camera operation"
    ],
    "doi": "10.1145/1959826.1959840",
    "url": "https://doi.org/10.1145/1959826.1959840",
    "citations": 3,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959841",
    "title": "Designing the sports prosthetic leg",
    "authors": [
      "Shunji Yamanaka",
      "Yuki Tsuji",
      "Mariko Higaki",
      "Hideka Suzuki"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "From a prosthesis hidden under clothing to a one comes on spotlight. Our common recognition is changing through sports. For amputee's more beautiful form in running, we've developed prostheses specially focused on usability, exterior, and safety. Here we'd like to introduce how we've designed the prosthesis for lower limb, knee joints and air stabilizer for the carbon fiber foot.",
    "keywords": [
      "sports prosthetic leg",
      "designing prosthesis",
      "amputee athlete"
    ],
    "doi": "10.1145/1959826.1959841",
    "url": "https://doi.org/10.1145/1959826.1959841",
    "citations": 3,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "15",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959842",
    "title": "Head orientation sensing by a wearable device for assisted locomotion",
    "authors": [
      "Keisuke Takahashi",
      "Hideki Kadone",
      "Kenji Suzuki"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we propose a novel wearable sensor device for the measurement of the head orientation and relative position against the body trunk in real-time. It is known that in natural walking, human locomotion is preceded by changes in head orientation [1, 2, 3] and the walking direction can therefore be predicted by observing the head orientation. We have been developing a wearable sensing device for the measurement of head orientation, which enables prediction of the future walking direction in real-time for the assistive technologies for locomotion - such as exoskeleton robots and wheelchair. Existing body posture measurement devices tend to be large and non-portable [4], therefore measurement in everyday space is still difficult. On the other hand, the developed system enables wireless and location independent measurement of the orientation of the head and it can be applied for assisted locomotion.In evaluation of the accuracy of the developed device, we observed the head anticipation during natural walking. Additionally, we compared the head anticipation in natural walking and electric wheelchair locomotion using the developed device and discuss a novel wheelchair control based on head orientation.",
    "keywords": [
      "wearable device",
      "locomotion",
      "head orientation",
      "head anticipation"
    ],
    "doi": "10.1145/1959826.1959842",
    "url": "https://doi.org/10.1145/1959826.1959842",
    "citations": 7,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "16",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959843",
    "title": "Earthlings Attack! a ball game using human body communication",
    "authors": [
      "Masato Takahashi",
      "Charith Lasantha Fernando",
      "Yuto Kumon",
      "Shuhey Takeda",
      "Hideaki Nii",
      "Takuji Tokiwa",
      "Maki Sugimoto",
      "Masahiko Inami"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we describe a ball game \"Earthlings Attack!\" that uses the contact between users and an active ball device as an information channel to the game content. When the ball device with built-in transmitter comes in contact with the user who wears the receiver, theis system transmits information from the ball device to the receiver through user's body with the human body communication. With this method, we aim at the interaction improvement of the augmentation of the interaction in such a way that presenting information on user's body according to the contact between each ball device and each user. This system also enables to use in a wide range field in the same network by managing contact information of both collectively.",
    "keywords": [
      "wearable computing",
      "human body communication",
      "game design"
    ],
    "doi": "10.1145/1959826.1959843",
    "url": "https://doi.org/10.1145/1959826.1959843",
    "citations": 7,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "17",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959844",
    "title": "Parasitic Humanoid: the wearable robotics as a behavioral assist interface like oneness between horse and rider",
    "authors": [
      "Taro Maeda",
      "HideyUki Ando",
      "Hiroyuki Iizuka",
      "Tomoko Yonemura",
      "Daisuke Kondo",
      "Masataka Niwa"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "The Parasitic Humanoid (PH) is a wearable robotic human interface for sampling, modeling, and assisting nonverbal human behavior. This anthropomorphic robot senses the behavior of the wearer and has the internal models to learn the process of human sensory motor integration, thereafter it begins to predict the next behavior of the wearer using the learned models. When the reliability of the prediction is sufficient, the PH outputs the difference from the actual behavior as a request for motion to the wearer by motion induction using sensory illusion. Through the symbiotic interaction, the internal model and the process of human sensory motor integration approximate each other asymptotically. This process is available to transmit modalities such as senses of sight, hearing, touch, force and balance with human embodiment. This synergistic multimodal communication between distant people wearing PH can realize experience-sharing, skill transmission, and human behavior supports.",
    "keywords": [
      "motion induction",
      "human hack",
      "embodiment",
      "ability extension"
    ],
    "doi": "10.1145/1959826.1959844",
    "url": "https://doi.org/10.1145/1959826.1959844",
    "citations": 10,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "18",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/1959826.1959845",
    "title": "\"Vection field\" for pedestrian traffic control",
    "authors": [
      "Masahiro Furukawa",
      "Hiromi Yoshikawa",
      "Taku Hachisu",
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Visual signs and audio cues are commonly used for pedestrian control in the field of general traffic research. Because pedestrians need to first acquire and then recognize such cues, time delays invariably occur between cognition and action. To better cope with this issue of delays, wearable devices have been proposed to control pedestrians more intuitively. However, the attaching and removing of the devices can be cumbersome and impractical. In this study, we propose a new visual navigation method for pedestrians using a \"Vection Field\" in which the optical flow is presented on the ground. The optical flow is presented using a lenticular lens, a passive optical element that generates a visual stimulus based on a pedestrian's movement without an electrical power supply. In this paper we present a design for the fundamental visual stimulus and evaluate the principle of our proposed method for directional navigation. Results revealed that the optical-flow of a stripe and random-dot pattern displaced pedestrian pathways significantly, and that implementation with a lenticular lens is feasible.",
    "keywords": [
      "vection",
      "traffic control",
      "lenticular lens"
    ],
    "doi": "10.1145/1959826.1959845",
    "url": "https://doi.org/10.1145/1959826.1959845",
    "citations": 26,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "19",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959846",
    "title": "Skill transmission for hand positioning task through view-sharing system",
    "authors": [
      "Keitaro Kurosaki",
      "Hiroki Kawasaki",
      "Daisuke Kondo",
      "Hiroyuki Iizuka",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we describe the skill transmission through our view-sharing system that can mix or exchange the first person perspectives from the exact the partner's viewpoints. Since a non-skilled person can see the first person perspective of a skilled person, the motion of the non-skilled person is intuitively modified and supported. The task for the skill transmission is to play theremin that requires precise hand motions. As a result we show that the skill transmission effectively happens with our view-sharing system compared with the conventional method, side-by-side teaching. The way of effective augmenting human ability will be discussed.",
    "keywords": [
      "view-sharing",
      "video see-trough HMD",
      "tele-existence",
      "skill transmission",
      "cooperative work"
    ],
    "doi": "10.1145/1959826.1959846",
    "url": "https://doi.org/10.1145/1959826.1959846",
    "citations": 8,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "20",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959847",
    "title": "Smart skincare system: remote skincare advice system using life logs",
    "authors": [
      "Maki Nakagawa",
      "Koji Tsukada",
      "Itiro Siio"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Many women find it difficult to maintain beautiful skin as skincare approaches require a great deal of effort, time, and special knowledge. Women often ask experts in cosmetic stores for skincare advice. However, this approach has limitations in terms of time, place, and privacy. To solve these problems, we propose a remote skincare advice system using life logs. This system helps users automatically log information related to their skin condition and share these data with skincare experts in order to obtain appropriate advice. First, we performed a feasibility study to select proper life log data for our system, and then we built prototype systems. Finally, we verified the effectiveness of our system through two studies.",
    "keywords": [
      "skincare",
      "life log",
      "advice"
    ],
    "doi": "10.1145/1959826.1959847",
    "url": "https://doi.org/10.1145/1959826.1959847",
    "citations": 3,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "21",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959848",
    "title": "Skill evaluation method based on variability of antagonism power of EMG",
    "authors": [
      "Yuta Takahashi",
      "Masashi Toda",
      "Shigeru Sakurazawa",
      "Junichi Akita",
      "Kazuaki Kondo",
      "Yuichi Nakamura"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We can more effectively take the physical skills of individual people into consideration from various points of view when we focus on evaluating their skills while exercising. We can focus on their maximum levels of speed and power, their smoothness through a series of exercises, their instantaneous force, repeatability, and their adjustability to agitation or obstacles such as circumjacent people or nature. A lot of exercise skills can relatively and easily be quantitatively evaluated by carefully analyzing the results and performance.However, it is difficult to evaluate the \"repeatability\" aspect, which is only one of exercise skill, when judging its degree from only viewing the given exercise. An example of a physical exercise process that can contribute stable results would need to be equivalent to a \"skill\" such as hitting a home run each time. We believe that the acquisition of a given skill is very useful in fields such as physical training. Therefore, we examined the repeatability aspect from this point of view.We also used an antagonism power index calculated using EMG to achieve such purposes. The index represents any adjustments made in the output power from the muscles. I thought that the adjust function of the output power of the muscles would be very useful when evaluating the exercise skills of a given individual. The antagonism power was calculated using the quasi-muscular tension and a skeletal muscle model consisting of one joint and two muscles. We also made a comparison between the unskilled state and a skilled state. As a result, the differences in exercise skill appeared to be antagonism power. Therefore, we thought that antagonism power was effective enough for creating a new exercise skill evaluation index that we define in this paper.",
    "keywords": [
      "skill",
      "proficiency",
      "exercise",
      "antagonism power",
      "EMG"
    ],
    "doi": "10.1145/1959826.1959848",
    "url": "https://doi.org/10.1145/1959826.1959848",
    "citations": 1,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "22",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959849",
    "title": "Coordinated behavior between visually coupled dyads",
    "authors": [
      "Hiroyuki Iizuka",
      "Daisuke Kondo",
      "Hiroki Kawasaki",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We describe how visually coupled people start their synchronized behavior with two visual coupling conditions: view-swapping and view-blending. In the view-swapping condition, two people's views are changed from the first-person perspective so that both see their partner's views. The view-blending condition allows people to see the blended view of both views. We report the results of different coordinated strategies to start synchronization that is observed in different conditions. In terms of the time required to start synchronization, view-swapping outperforms view-blending.",
    "keywords": [
      "synchronization",
      "sharing first-person perspectives",
      "coordination"
    ],
    "doi": "10.1145/1959826.1959849",
    "url": "https://doi.org/10.1145/1959826.1959849",
    "citations": 5,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "23",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1959826.1959850",
    "title": "The emotional economy for the augmented human",
    "authors": [
      "Jean-Marc Seigneur"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Happiness research findings are increasingly being taken into account in standard economics. However, most findings are based on a posteriori surveys trying to infer how happy people have been. In this paper, we argue that the advances in wearable computing, especially brain-computer interfaces, can lead to realtime measurements of happiness. We then propose a new kind of economy model where people pay depending on the emotions they have experienced. We have combined current commercial-on-the-shelf software and hardware components to create a proof-of-concept of the model.",
    "keywords": [
      "emotions",
      "economics",
      "brain-computer interface"
    ],
    "doi": "10.1145/1959826.1959850",
    "url": "https://doi.org/10.1145/1959826.1959850",
    "citations": 16,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "24",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1959826.1959851",
    "title": "Wearable MC system a system for supporting MC performances using wearable computing technologies",
    "authors": [
      "Tomonari Okada",
      "Tetsuya Yamamoto",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "A master of ceremonies (MC) plays an important role to ensure all events progress smoothly because unexpected interruption make them unsuccessful. MCs must have various abilities such as being able to memorize the content of given scenarios and manage problems that occur unexpectedly. Moreover, since unskilled MCs cannot intuit the atmosphere in the audiences during an event, they cannot control this smoothly. Therefore, we propose a wearable system that solves these problems for MCs achieved through wearable computing technologies. Our system has functions to support MCs in carrying out their duties smoothly, such as a robust voice-tracking function for them to read scripts, a user interface that does not interrupt other tasks, and a function that enables MCs intuit grasp the atmosphere of the audience. We implemented a prototype of the wearable MC system and actually used it at several events. The results we obtained from actually using it confirmed that it worked well and helped MCs to carry out their official duties smoothly.",
    "keywords": [],
    "doi": "10.1145/1959826.1959851",
    "url": "https://doi.org/10.1145/1959826.1959851",
    "citations": 10,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "25",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/1959826.1959852",
    "title": "View sharing system for motion transmission",
    "authors": [
      "Daisuke Kondo",
      "Keitaro Kurosaki",
      "Hiroyuki Iizuka",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "We are developing 'view sharing' system for supporting a remote corporative work. The view sharing is constructed from the video-see-through head mounted displays (VST-HMD) and motion trackers. This system allows two users in remote places to share their first-person views each other. The users can share what the other user is seeing, and furthermore the users can correspond their spatial perception, motion and head movement. By sharing those sensations, the non-verbal skills can be transmitted from skilled person to the non-skilled person. Using this system expert in remote place can instruct the non-skilled person to improve task performance.",
    "keywords": [
      "view sharing",
      "skill transmission",
      "remote cooperative work",
      "parasitic humanoid"
    ],
    "doi": "10.1145/1959826.1959852",
    "url": "https://doi.org/10.1145/1959826.1959852",
    "citations": 10,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "26",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959853",
    "title": "HASC Challenge: gathering large scale human activity corpus for the real-world activity understandings",
    "authors": [
      "Nobuo Kawaguchi",
      "Nobuhiro Ogawa",
      "Yohei Iwasaki",
      "Katsuhiko Kaji",
      "Tsutomu Terada",
      "Kazuya Murao",
      "Sozo Inoue",
      "Yoshihiro Kawahara",
      "Yasuyuki Sumi",
      "Nobuhiko Nishio"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "Understandings of human activity through wearable sensors will enable the next-generation human-oriented computing. However, most of researches on the activity recognition so far are based on small number of test subjects, and not well adapted for real world applications. To overcome the situation, we have started a project named \"HASC Challenge\" to collect a large scale human activity corpus. By the end of 2010, by the collaboration of 20 teams, more than 6700 accelerometer data with 540 subjects have been collected through our project. We also developed a tool named \"HASC Tool\" for management, evaluation and collection of the large number of activity sensor data.",
    "keywords": [
      "wearable sensor",
      "wearable computing",
      "large scale corpus",
      "activity understandings",
      "activity recognition",
      "accelerometer"
    ],
    "doi": "10.1145/1959826.1959853",
    "url": "https://doi.org/10.1145/1959826.1959853",
    "citations": 120,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "27",
    "numpages": "5",
    "influentialCitations": 7
  },
  {
    "id": "10.1145/1959826.1959854",
    "title": "Effective galvanic vestibular stimulation in synchronizing with ocular movement",
    "authors": [
      "Aru Sugisaki",
      "Yuki Hashimoto",
      "Tomoko Yonemura",
      "Hiroyuki Iizuka",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "It is known that galvanic vestibular stimulation can cause ocular movement. Our final goal is to use GVS to support ocular movements. However, the effects of GVS to ocular movements are basically investigated while gazing at a certain point despite the fact that we have two different strategies to follow a moving target such as saccade and smooth pursuit. The effect might be different because those two use different mechanism. Therefore, this paper investigates the GVS effects during saccade. As a result, we show that the effect of GVS depends on the timing when GVS is given after the target marker moves.",
    "keywords": [
      "saccade",
      "optimization",
      "ocular movement",
      "galvanic vestibular stimulation"
    ],
    "doi": "10.1145/1959826.1959854",
    "url": "https://doi.org/10.1145/1959826.1959854",
    "citations": 1,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "28",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959855",
    "title": "Catchy account: a system for acquiring a realistic sense of expenditures",
    "authors": [
      "Mieko Nakamura",
      "Homei Miyashita"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we propose a new household accounting system for realistically sensing expenditures. In 2D mode, expenditures are visualized through the placement of rectangles whose areas are proportional to the amount spent; thus, each item can be understood within the context of the total expenditure. In AR mode, spheres whose volumes are proportional to the amount spent appear to be floating in the camera image. The spheres fill the entire room and the size of expenditure can be realistically sensed. We designed this system in an attempt to \"augment\" the experience, so that the user can acquire a more realistic sense of expenditures.",
    "keywords": [
      "visualization",
      "realistic sense",
      "accounting"
    ],
    "doi": "10.1145/1959826.1959855",
    "url": "https://doi.org/10.1145/1959826.1959855",
    "citations": 1,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "29",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/1959826.1959856",
    "title": "Designing augmented environment with hybrid prototyping using virtual simulation and physical device",
    "authors": [
      "Koji Sekiguchi",
      "Yasuto Nakanishi",
      "Soh Kitahara",
      "Takuro Ohmori",
      "Daisuke Akatsuka"
    ],
    "year": 2011,
    "conference": "AH",
    "conferenceYear": "AH '11",
    "abstract": "In this paper, we describe hybrid prototyping that combines virtual simulation with physical device, and argue the possibility of hybrid prototyping through a simulation of an augmented environment.",
    "keywords": [],
    "doi": "10.1145/1959826.1959856",
    "url": "https://doi.org/10.1145/1959826.1959856",
    "citations": 0,
    "booktitle": "Proceedings of the 2nd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Tokyo, Japan",
    "articleno": "30",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160126",
    "title": "An on-site programming environment for wearable computing",
    "authors": [
      "Shotaro Akiyama",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In wearable computing environments, it is difficult for users to prepare applications that are used beforehand since there are various situations and places. Therefore, they want to define new services by themselves. In this study, we present a development framework and several tools for developing services in wearable computing environments. The framework consists of an event-driven rule processing engine and service implementation tools, which enable users to program services easily and quickly. The proposed system shows elements of event-driven rules as chips, and we can program services by selecting chips on graphical user interfaces. In addition, the proposed system has two functions considering programming features on wearable computing: genetic-algorithm-based programming and social-network-based programming.",
    "keywords": [
      "context awareness",
      "programming",
      "wearable computing"
    ],
    "doi": "10.1145/2160125.2160126",
    "url": "https://doi.org/10.1145/2160125.2160126",
    "citations": 1,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "1",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160127",
    "title": "Therapy: location-aware assessment and tasks",
    "authors": [
      "Luís Carriço",
      "Marco de Sá",
      "Luís Duarte",
      "Tiago Antunes"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In this paper, we present a system that allows therapists to assess and engage patients' in activities triggered by specific stressing contexts. The system is composed by: 1) a web application that the therapist uses to specify the activities and its triggering conditions; and 2) a mobile app that measures physiologic characteristics and challenges the patient to perform the activities according to those conditions. This toolset is part of an extended cognitive behaviour therapy framework. The preliminary evaluation results are encouraging and indicate that the system can be of use and usable for direct application in therapy procedures.",
    "keywords": [
      "location based services",
      "physiological recording",
      "therapy"
    ],
    "doi": "10.1145/2160125.2160127",
    "url": "https://doi.org/10.1145/2160125.2160127",
    "citations": 5,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "2",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160128",
    "title": "A study to understand lead-lag performance of subject vs rehabilitation system",
    "authors": [
      "Radhika Chemuturi",
      "Farshid Amirabdollahian",
      "Kerstin Dautenhahn"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Robotic assistance in stroke rehabilitation is rapidly advancing based on the recent developments in robotics, haptic interfaces and virtual reality. GENTLE/S is a rehabilitation system that utilized haptic and virtual reality technologies to deliver challenging and meaningful therapies to upper limb impaired stroke subjects. The current research is working towards designing GENTLE/A system with a better adaptive human-robot interface, which allows for automatic tuning of the assistance and resistance based on provided input. This paper presents the results from a preliminary study conducted with three healthy subjects as part of this research. The aim of the investigation is to explore whether it is possible to identify if a robot or a person is leading the interaction by comparing the results from the actual performance of the subject with the minimum jerk model used to drive the robot. The final goal is to use these observations to probe various ways in which the contribution of robot can be established and the adaptability of the robot during the therapy can be enhanced.",
    "keywords": [
      "adaptive system",
      "lead-lag interactive behaviour",
      "robotic therapy",
      "stroke rehabilitation"
    ],
    "doi": "10.1145/2160125.2160128",
    "url": "https://doi.org/10.1145/2160125.2160128",
    "citations": 8,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "3",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160129",
    "title": "Can you feel it? sharing heart beats with Augmento",
    "authors": [
      "Luís Duarte",
      "Tiago Antunes",
      "Luís Carriço"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "This paper presents Augmento, a system which aims at providing individuals with an asynchronous approach of reinforcing the bonds with their relatives by sharing emotions when they are in the vicinity of places which hold special memories of their lives. Augmento capitalizes on existing technologies to accomplish its goal, ranging from the usage of location based services, to the retrieval of the individual's physiological signals to convey typically occluded information between individuals, more particularly in long-distance relationships. The paper presents the general vision for the system, its workflow, architecture, scenarios and early prototypes. We performed an early assessment of the system and, in particular, we were interested in obtaining valuable insight whether vibrotactile feedback would be suited to convey and mimic an individual's heartbeat rate value to other users. The results for this testing period are presented and discussed in the paper.",
    "keywords": [
      "Augmento",
      "affective computing",
      "location based services"
    ],
    "doi": "10.1145/2160125.2160129",
    "url": "https://doi.org/10.1145/2160125.2160129",
    "citations": 2,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "4",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160130",
    "title": "User performance tweaking in videogames: a physiological perspective of player reactions",
    "authors": [
      "Luís Duarte",
      "Luís Carriço"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "The videogame industry has suffered significant modifications in the last years, broadening its horizons towards a more casual market. This market expansion not only brings new opportunities from an interaction point-of-view, but also new challenges with the inclusion of users who are not accustomed to these games. This paper presents part of an ongoing study which aims at providing a better understanding of player behavior both from an interactive and a physiological standpoint. The experiment addressed here assesses different gameplay mechanics influence not only a subset of the players' physiological signals, but also their performance and interactive behavior.",
    "keywords": [
      "physiological assessment",
      "user performance",
      "videogames"
    ],
    "doi": "10.1145/2160125.2160130",
    "url": "https://doi.org/10.1145/2160125.2160130",
    "citations": 0,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "5",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160131",
    "title": "A method to evaluate metal filing skill level with wearable hybrid sensor",
    "authors": [
      "Yu Enokibori",
      "Kenji Mase"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "This paper presents a method to evaluate a person's skill level for metal filing. Metal filing by expert engineers is an important manufacturing skill that supports basic areas of industry, although most sequences are already automated with industrial robots.However, there is no effective training method for the skill; \"coaching\" has been most weighted. Most coaching has depended on the coaches' personal viewpoints. In addition, skill levels have been assessed subjectively by the coaches. Because of these problems, learners have to spend several hundred hours to acquire the basic manufacturing skill.Therefore, to develop an effective skill training scheme and an objective skill level assessment, we analyzed metal filing and implemented a method to evaluate metal-filing skill. We used wearable hybrid sensors that support an accelerometer and gyroscope, and collected data from 4 expert coaches and 10 learners. The data are analyzed from the viewpoint of the mechanical structure of their bodies during metal filing. Our analysis yielded three effective measures for skill assessment: \"Class 2 Lever-like Movement Measure\", \"Upper Body Rigidity Measure\", and \"Pre-Acceleration Measure\".The weighted total measure succeeded in distinguishing the coach group and the learner group as individual skill level groups at a 95% confidence level. The highest-level learner, the lowest-level learner, and the group of other learners were also able to be distinguished as individual skill level groups at a 95% confidence level; this is the same result as an expert coach's subjective score.",
    "keywords": [],
    "doi": "10.1145/2160125.2160131",
    "url": "https://doi.org/10.1145/2160125.2160131",
    "citations": 4,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "6",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160132",
    "title": "Relation between location of information displayed by augmented reality and user's memorization",
    "authors": [
      "Yuichiro Fujimoto",
      "Goshiro Yamamoto",
      "Hirokazu Kato",
      "Jun Miyazaki"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "This study aims to investigate the effectiveness of Augmented Reality (AR) on user's memory skills when it is used as an information display method. By definition, AR is a technology which displays virtual images on the real world. These computer generated images naturally contain location information on the real world. It is also known that humans can easily memorize and remember information if this information is retained along with some locations on the real world. Thus, we hypothesize that displaying annotations by using AR may have better effects on the user's memory skill, if they are associated with the location of the target object on the real world rather than when connected with an unrelated location. A user study was conducted with 30 participants in order to verify our hypothesis. As a result, a significant difference was found between the situation when information was associated with the location of the target object on the real world and when it was connected with an unrelated location. In this paper, we present the test results and explain the verification based on the results.",
    "keywords": [
      "augmented reality",
      "memorization",
      "user study"
    ],
    "doi": "10.1145/2160125.2160132",
    "url": "https://doi.org/10.1145/2160125.2160132",
    "citations": 28,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "7",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160133",
    "title": "Facilitating a surprised feeling by artificial control of piloerection on the forearm",
    "authors": [
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "There have been many proposals that have added haptic stimulation to entertainment content such as music, games, and movies. These technologies enrich the quality of the experiences by improving the reality thereof. In contrast, we present a novel approach to enrich the quality of these experiences by facilitating the emotional feeling evoked by the content. In this paper, we focus on piloerection, which is a kind of involuntary emotional reaction. Our hypothesis is that not only is it an emotional \"reaction\", but it can also work as an emotional \"input\" that enhances the emotion itself. We have constructed a device that controls piloerection on the forearm through electrostatic force. Based on a psychophysical experiment, we confirm that the piloerection system enhances the feeling of surprise.",
    "keywords": [
      "enhancement of emotion",
      "enhancement of feeling",
      "piloerection",
      "tactile interaction"
    ],
    "doi": "10.1145/2160125.2160133",
    "url": "https://doi.org/10.1145/2160125.2160133",
    "citations": 25,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "8",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160134",
    "title": "KUSUGURI: a shared tactile interface for bidirectional tickling",
    "authors": [
      "Masahiro Furukawa",
      "Hiroyuki Kajimoto",
      "Susumu Tachi"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Tickling, a nonverbal form of communication, can provide entertainment. Therefore, tickling is a desirable addition as content as a remote communication method. However, tickling is difficult to realize because it requires both body contact as well as bidirectionality. In this paper, we propose a method of \"Shared Tactile Interface\" which allows sharing of a body part with another user at a distance. The interface has three features: direct contact, transfer of the tickling sensation, and bidirectionality. The first allows users to view another person's finger as if it is directly contacting the user's own palm and moving on the user's palm. The second feature delivers a vibration to the user's palm which generates an illusion and perception of a tickling sensation. The third feature enables bidirectional tickling because one user can also tickle the other user's palm in the same manner. We built prototypes based on this design method, and evaluated the proposed method through two technical exhibitions. The users were able to tickle each other, which confirmed that the design method \"Shared Tactile Interface\" works as expected. However, we found issues especially regarding the reliability of the tickling sensation.",
    "keywords": [
      "haptic communication",
      "tactile communication",
      "tactile illusion",
      "tactile interface",
      "tickling sensation"
    ],
    "doi": "10.1145/2160125.2160134",
    "url": "https://doi.org/10.1145/2160125.2160134",
    "citations": 34,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "9",
    "numpages": "8",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/2160125.2160135",
    "title": "Stereo camera based wearable reading device",
    "authors": [
      "Roman Guilbourd",
      "Noam Yogev",
      "Raúl Rojas"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "The ability to access textual information is crucial for visually impaired people in terms of achieving greater independence in their everyday life. Thus, there is a need for a mobile easy-to-use reading device, capable of dealing with the complexity of the outdoor environment. In this paper a wearable camera-based solution is presented, aiming at improving the performance of existing systems through the use of stereo vision. Specific aspects of the stereo matching problem in document images are discussed and an approach for its integration into the document processing procedure is introduced. We conclude with the presentation of experimental results from a prototype system, which demonstrate the practical benefits of the presented approach.",
    "keywords": [],
    "doi": "10.1145/2160125.2160135",
    "url": "https://doi.org/10.1145/2160125.2160135",
    "citations": 2,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "10",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160136",
    "title": "Realtime sonification of the center of gravity for skiing",
    "authors": [
      "Shoichi Hasegawa",
      "Seiichiro Ishijima",
      "Fumihiro Kato",
      "Hironori Mitake",
      "Makoto Sato"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Control of body position is important in skiing. During turn, novice skiers often lean back and lose their control. Leaning back is natural reaction for people. They arc afraid of the slope or speed. We develop a device to provide realtime sonification feedback of the center of gravity of the skier. The device guides the position of skier. A preliminary experiment shows possibility of improvements that the user become to be able to control their position immediately and even to overcome the afraid of slope and speed.",
    "keywords": [
      "augmented sport",
      "ski",
      "sonification"
    ],
    "doi": "10.1145/2160125.2160136",
    "url": "https://doi.org/10.1145/2160125.2160136",
    "citations": 61,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "11",
    "numpages": "4",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/2160125.2160137",
    "title": "A pointing method using accelerometers for graphical user interfaces",
    "authors": [
      "Tatsuya Horie",
      "Tsutomu Terada",
      "Takuya Katayama",
      "Masahiko Tsukamoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Graphical User Interfaces (GUIs) are widely used and pointing devices are required to operate most of them. We have proposed Xangle, a pointing method using two accelerometers for wearable computing environments. The cursor is positioned at the intersection of two straight lines, which are synchronized with the angles of the accelerometers at fingers. However, Xangle is difficult to be used in daily-life, when the user frequently changes which part of the body they point with. Therefore, we propose a method of changing the body parts used for pointing according to the situation. Additionally, we proposed a method to accelerate the pointer and a method to layout menu items for Xangle since these methods are suitable for using GUI in wearable computing environments. We confirmed that the proposed method was effective from the results of evaluations.",
    "keywords": [
      "GUI",
      "pointing method",
      "wearable computing"
    ],
    "doi": "10.1145/2160125.2160137",
    "url": "https://doi.org/10.1145/2160125.2160137",
    "citations": 10,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "12",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160138",
    "title": "Effects of auditory feedback for augmenting the act of writing",
    "authors": [
      "Junghyun Kim",
      "Tomoko Hashida",
      "Tomoko Ohtani",
      "Takeshi Naemura"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In this paper, focusing on the writing sound when using an ordinary paper and pen, we explain how auditory feedback augments the act of writing. Specifically, we evaluated the effectiveness of the auditory feedback by comparing writing tasks, which involved tracing Chinese characters, Without Feedback (No), with Monaural Feedback (MF), and Stereo Feedback (SF). The results of this study showed that auditory feedback (MF and SF) of writing produced more written characters than Without Feedback (No) and had fewer negative impressions during the writing task.",
    "keywords": [
      "auditory feedback",
      "augmenting the act of writing",
      "supporting system for handwriting",
      "writing sounds"
    ],
    "doi": "10.1145/2160125.2160138",
    "url": "https://doi.org/10.1145/2160125.2160138",
    "citations": 4,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "13",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160139",
    "title": "Stop motion goggle: augmented visual perception by subtraction method using high speed liquid crystal",
    "authors": [
      "Naoya Koizumi",
      "Maki Sugimoto",
      "Naohisa Nagaya",
      "Masahiko Inami",
      "Masahiro Furukawa"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Stop Motion Goggle (SMG) expands visual perception by allowing users to perceive visual information selectively through a high speed shutter. In this system, the user can easily observe not only periodic rotational motion such as rotating fans or wheels, but also random motion like bouncing balls. In this research, we developed SMG and evaluated the effect of SMG on visual perception of high speed moving objects. Furthermore this paper describes users' behaviors under the expanded visual experience.",
    "keywords": [
      "deblurring",
      "motion perception",
      "visuosensory augmentation"
    ],
    "doi": "10.1145/2160125.2160139",
    "url": "https://doi.org/10.1145/2160125.2160139",
    "citations": 7,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "14",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160140",
    "title": "Quantifying Japanese onomatopoeias: toward augmenting creative activities with onomatopoeias",
    "authors": [
      "Takanori Komatsu"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Onomatopoeias are used when one cannot describe certain phenomena or events literally in the Japanese language, and it is said that one's ambiguous and intuitive feelings are embedded in these onomatopoeias. Therefore, an interface system that can use onomatopoeia as input information could comprehend such users' feelings, and moreover, this system would contribute to augmenting creative activities such as with computer graphics, music, choreography, and so on. The purpose of this study is to propose an objective quantification method for onomatopoeias in the form of an expression vector to be applied to an interface system in order to augment various creative activities.",
    "keywords": [
      "augmenting users' creative activities",
      "onomatopoeias",
      "quantification",
      "sound symbolism"
    ],
    "doi": "10.1145/2160125.2160140",
    "url": "https://doi.org/10.1145/2160125.2160140",
    "citations": 15,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "15",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160141",
    "title": "Transmission of forearm motion by tangential deformation of the skin",
    "authors": [
      "Yuki Kuniyasu",
      "Michi Sato",
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "When teaching device handling skills such as those required in calligraphy, sports or surgery, it is important that appropriate arm motion is transmitted from the trainer to the trainee. In this study, we present a novel, wearable haptic device that produces arm motion using force sensation. The device produces skin deformation and a pseudo-force sensation that is similarly to the force produced when the arm is \"pulled\". The device generates skin deformation in four directions, and in this paper we have evaluated the device using a directions perception experiment.",
    "keywords": [
      "force sensation",
      "forearm skin",
      "pulling arm"
    ],
    "doi": "10.1145/2160125.2160141",
    "url": "https://doi.org/10.1145/2160125.2160141",
    "citations": 36,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "16",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160142",
    "title": "Ma petite chérie: what are you looking at? a small telepresence system to support remote collaborative work for intimate communication",
    "authors": [
      "Kana Misawa",
      "Yoshio Ishiguro",
      "Jun Rekimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "We present a telepresence system with a reduced scale face-shaped display for supporting intimate telecommunication. In our previous work, we have developed a real-size face shaped display that tracks and reproduces the remote user's head motion and face image. It can convey user's nonverbal information such as facial expression and gaze awareness. In this paper, we examine the value and effect of scale reduction of such face-shaped displays. We expect small size face displays retain the benefit of real-size talking-head type telecommunication systems, and also provide more intimate impression. It is easier to transport or put on a desk, and it can be worn on the shoulder of the local participants so that people bring it like a small buddy. However, it is not clear how such reduced-size face screen might change the quality of nonverbal communication. We thus conducted an experiment using a 1/14 scale face display, and found critical nonverbal information, such as gaze-direction, is still correctly transmitted even when face size is reduced.",
    "keywords": [
      "CSCW",
      "eye gaze",
      "telepresence",
      "wearable"
    ],
    "doi": "10.1145/2160125.2160142",
    "url": "https://doi.org/10.1145/2160125.2160142",
    "citations": 16,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2160125.2160143",
    "title": "A new typology of augmented reality applications",
    "authors": [
      "Jean-Marie Normand",
      "Myriam Servières",
      "Guillaume Moreau"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In recent years Augmented Reality (AR) has become more and more popular, especially since the availability of mobile devices, such as smartphones or tablets, brought AR into our everyday life. Although the AR community has not yet agreed on a formal definition of AR, some work focused on proposing classifications of existing AR methods or applications. Such applications cover a wide variety of technologies, devices and goals, consequently existing taxonomies rely on multiple classification criteria that try to take into account AR applications diversity. In this paper we review existing taxonomies of augmented reality applications and we propose our own, which is based on (1) the number of degrees of freedom required by the tracking of the application, as well as on (2) the visualization mode used, (3) the temporal base of the displayed content and (4) the rendering modalities used in the application. Our taxonomy covers location-based services as well as more traditional vision-based AR applications. Although AR is mainly based on the visual sense, other rendering modalities are also covered by the same degree-of-freedom criterion in our classification.",
    "keywords": [
      "augmented reality",
      "degrees of freedom",
      "multi-modality",
      "taxonomy"
    ],
    "doi": "10.1145/2160125.2160143",
    "url": "https://doi.org/10.1145/2160125.2160143",
    "citations": 71,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "18",
    "numpages": "8",
    "influentialCitations": 7
  },
  {
    "id": "10.1145/2160125.2160144",
    "title": "Investigating the use of brain-computer interaction to facilitate creativity",
    "authors": [
      "D. A. Todd",
      "P. J. McCullagh",
      "M. D. Mulvenna",
      "G. Lightbody"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Brain Computer Interaction (BCI) has mainly been utilized for communication and control, but it may also find application as a channel for creative expression, as part of an entertainment package. In this paper we provide an initial investigation on how creativity can be supported and assessed. An art-based approach was adopted to investigate the effects of achieving simple drawing and painting. Subjects were asked to complete three tasks using an Steady State Visual Evoked Potential BCI; a drawing task called 'etch-a-sketch' (TASK 1) which relied entirely upon BCI control, and two painting tasks, the first (TASK 2) with a set goal and the second (TASK 3) with more potential for user expression. The tasks varied in the proportion of control to creativity required. Participants provided feedback on their perception of the control and creative aspects and their overall experience. The painting application (TASK 3) for which users perceived that they had more creativity was well accepted; 50% of the users preferred this mode of interaction. The experimental approach described allows for an initial assessment of the acceptance of BCI-mediated artistic expression.",
    "keywords": [
      "assessment",
      "brain computer interaction",
      "brain painting",
      "creativity"
    ],
    "doi": "10.1145/2160125.2160144",
    "url": "https://doi.org/10.1145/2160125.2160144",
    "citations": 24,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "19",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160145",
    "title": "Human-centric panoramic imaging stitching",
    "authors": [
      "Tomohiro Ozawa",
      "Kris M. Kitani",
      "Hideki Koike"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "We introduce a novel image mosaicing algorithm to generate 360° landscape images while also taking into account the presence of people at the boundaries between stitched images. Current image mosaicing techniques tend to fail when there is extreme parallax caused by nearby objects or moving objects at the boundary between images. This parallax causes ghosting or unnatural discontinuities in the image. To address this problem, we present an image mosaicing algorithm that is robust to parallax and misalignment, and is also able to preserve the important human-centric content, specifically faces. In particular, we find an optimal path between the boundary of two images that preserves color continuity and peoples' faces in the scene. Preliminary results show promising results of preserving close-up faces with parallax while also being able to generate a perceptually plausible 360° panoramic image.",
    "keywords": [
      "computer vision",
      "content-based image stitching",
      "face detection",
      "image stitching",
      "panorama image"
    ],
    "doi": "10.1145/2160125.2160145",
    "url": "https://doi.org/10.1145/2160125.2160145",
    "citations": 16,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "20",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160146",
    "title": "Augmenting on-road perception: enabling smart and social driving with sensor fusion and cooperative localization",
    "authors": [
      "Chieh-Chih Wang",
      "Jennifer Healey",
      "Meiyuan Zhao"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In many ways the car is the most common human augmentation: it increases our speed, renders us more powerful and enables us to reach distances that are otherwise impossible. In this paper, we show how advanced localization systems enable yet another dimension of human augmentation: allowing the driver to visually perceive data streams from other cars. These data streams may contain social messages from other drivers such as \"Follow Me\" or warnings from the sensor systems of the other cars themselves such as \"Distracted Driver!\" We describe both the technical work in progress that makes this system possible as well as the future vision of how this technology will enable smart and social driving through M2M communication with other vehicles that are encountered ad hoc on the road.",
    "keywords": [
      "augmented perception",
      "driver distraction",
      "in vehicle infotainment",
      "robotics",
      "simultaneous localization and mapping"
    ],
    "doi": "10.1145/2160125.2160146",
    "url": "https://doi.org/10.1145/2160125.2160146",
    "citations": 6,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "21",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160147",
    "title": "EyeSound: single-modal mobile navigation using directionally annotated music",
    "authors": [
      "Shingo Yamano",
      "Takamitsu Hamajo",
      "Shunsuke Takahashi",
      "Keita Higuchi"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In this paper, we propose a mobile navigation system that uses only auditory information, i.e., music, to guide the user. The sophistication of mobile devices has introduced the use of contextual information in mobile navigation, such as the location and the direction of motion of a pedestrian. Typically in such systems, a map on the screen of the mobile device is required to show the current position and the destination. However, this restricts the movements of the pedestrian, because users must hold the device to observe the screen. We have, therefore, implemented a mobile navigation system that guides the pedestrian in a non-restricting manner by adding direction information to music. By measuring the resolution of the direction that the user can perceive, the phase of the musical sound is changed to guide the pedestrian. Using this system, we have verified the effectiveness of the proposed mobile navigation system.",
    "keywords": [
      "eyes and hands free interaction",
      "mobile navigation",
      "single-modal"
    ],
    "doi": "10.1145/2160125.2160147",
    "url": "https://doi.org/10.1145/2160125.2160147",
    "citations": 10,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "22",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160148",
    "title": "Looming silhouette: an approaching visual stimulus device for pedestrians to avoid collisions",
    "authors": [
      "Maki Yokoyama",
      "Yu Okano",
      "Michi Sato",
      "Shogo Fukushima",
      "Masahiro Furukawa",
      "Hiroyuki Kajimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "We are exposed daily to the risk of collision at numerous blind intersections. To avoid the risk of collision, we propose a system that elicits an \"approaching sensation\" by presenting a visual stimulus. Possible factors for the approaching sensation are the \"expansion\" and \"motion\" of a silhouette. We compared the effects of these two factors on the approaching sensation and found that to elicit an approaching sensation, the expansion factor is important, and the motion factor has a certain effect in alarming pedestrians. On the base of this result, we produced a system that presents an expanding and moving silhouette of an approaching pedestrian to the pedestrians user.",
    "keywords": [
      "\"approaching sensation\"",
      "collision avoidance",
      "peripheral vision field"
    ],
    "doi": "10.1145/2160125.2160148",
    "url": "https://doi.org/10.1145/2160125.2160148",
    "citations": 1,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "23",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160149",
    "title": "Augmentation of obstacle sensation by enhancing low frequency component for horror game background sound",
    "authors": [
      "Shuyang Zhao",
      "Taku Hachisu",
      "Asuka Ishii",
      "Yuki Kuniyasu",
      "Hiroyuki Kajimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Computer games provide users with a mental stimulation that the real world cannot. Especially, horror games are a popular category. Current horror games can provide the user with a visible ghost and the stereo background sound to thrill the user. Inspired by obstacle sense - the ability of blind people localizing themselves only with hearing, a novel method to augment the sense of existence in the game background sound is proposed in this paper. We found that an effective sense can be created by decreasing high frequency component and increasing low frequency component simultaneously.",
    "keywords": [
      "augmented reality",
      "game background sound",
      "obstacle sensation"
    ],
    "doi": "10.1145/2160125.2160149",
    "url": "https://doi.org/10.1145/2160125.2160149",
    "citations": 3,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "24",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160150",
    "title": "Crowd augmented wireless access",
    "authors": [
      "Carlos Ballester Lafuente",
      "Jean-Marc Seigneur"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "Environments such as ski slopes are highly dynamic, as users are constantly moving at high speeds and in different directions, and also many users are not locals, thus having to roam in order to be able to connect through mobile data. These two previous reasons make connectivity through regular means to be difficult to attain. This demo paper presents the simulation and validation of a crowd augmented wireless access used in order to tackle this problem.",
    "keywords": [
      "crowd augmented wi-fi",
      "mobility",
      "smart ski",
      "wireless access"
    ],
    "doi": "10.1145/2160125.2160150",
    "url": "https://doi.org/10.1145/2160125.2160150",
    "citations": 4,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "25",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160151",
    "title": "Usability of video-overlaying SSVEP based BCIs",
    "authors": [
      "Christoph Kapeller",
      "Christoph Hintermüller",
      "Christoph Guger"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "This work investigates the usability of an steady-state visual evoked potentials (SSVEP) based brain-computer interface (BCI) with on-screen stimulation. The BCI controls were displayed with an underlying feedback video. Each control had a unique flashing frequency. For classification a combination of minimum energy (ME) and linear discriminant analysis (LDA) was used. Two experiments showed that the use of overlaying controls is possible, but also decreasing the performance.",
    "keywords": [
      "BCI",
      "EEG",
      "SSVEP",
      "brain computer interface",
      "on-screen stimulation",
      "overlay",
      "steady-state visual evoked potential",
      "video"
    ],
    "doi": "10.1145/2160125.2160151",
    "url": "https://doi.org/10.1145/2160125.2160151",
    "citations": 6,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "26",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160152",
    "title": "Augmented control of an avatar using an SSVEP based BCI",
    "authors": [
      "Christoph Kapeller",
      "Christoph Hintermüller",
      "Christoph Guger"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "The demonstration shows the usage of an EEG-based brain-computer interface (BCI) for the real-time control of an avatar in World of Warcraft. Visitors can test the installation during the conference after about 5 minutes of training time. World of Warcraft is a common Massively Multiplayer Online Role-Playing Game (MMORPG) in which the player controls an avatar in a virtual environment.The user has to wear newly developed dry EEG electrodes which are connected to a biosignal amplifier. Then the data is transmitted to a computer to perform the real-time analysis of the EEG data. The BCI system is using steady-state visual evoked potentials (SSVEPs) as control signal. Therefore the system shows different icons flickering with different frequencies. If the user focuses now on one of the icons the flickering frequency is visible in the EEG data and can be extracted with frequency analysis algorithms.In order to control an avatar in World of Warcraft it is necessary to have 4 control icons that are analyzed in real-time. Three icons are necessary to turn left or right or to move forward. Additionally a 4th icon is required to perform certain actions like grasping objects, attacking other objects....like shown in Figure 1. The visual stimulation took place via a 60Hz LCD-display with flickering frequencies of 15, 12, 10 and 8.57Hz in combination with an underlying video.To visualize the flickering controls a BCI-Overlay library based on OpenGL was implemented, which can be used by any graphics application. It provides the possibility to generate BCI controls within a virtual reality environment or as overlays in combination with video sequencesFigure 2 shows the components of the complete system. The user is connected with 8 EEG electrodes to the BCI system that is running under Windows and MATLAB. The BCI system uses the minimum energy algorithm and a linear discriminant analysis to determine if the user is looking at one of the icons or if the user is not attending.Via a UDP communication channel the BCI system is controlling the BCI-Overlay module that generates the 4 flickering icons around the WoW User Interface. If the BCI system detects a certain command it is transmitted to the game controller which generates the corresponding WoW command. This is straight forward for the left, right and move forward commands, but more complicated for the action command. Action commands are context dependant and the controller has to select certain possible actions. Finally the command is transmitted to WoW and the avatar performs the action.This allows the user to play WoW with the BCI system only by thought.",
    "keywords": [
      "BCI",
      "EEG",
      "SSVEP",
      "WoW",
      "World of Warcraft",
      "augmented control",
      "brain-computer interface",
      "on-screen stimulation",
      "steady-state visual evoked potentials"
    ],
    "doi": "10.1145/2160125.2160152",
    "url": "https://doi.org/10.1145/2160125.2160152",
    "citations": 24,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "27",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160153",
    "title": "Augmentation of kinesthetic sensation by adding \"rotary switch feeling\" feedback",
    "authors": [
      "Yosuke Kurihara",
      "Yuki Kuniyasu",
      "Taku Hachisu",
      "Michi Sato",
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In sports, dancing and playing music, it is important to achieve correct body movement as it greatly affects performance. However, matching one's movement with ideal movement is fundamentally difficult, because we do not have a detailed perception of our own body movement. In this study, we propose to present \"rotary switch feeling\" feedback as a new haptic cue. A periodical ticking sensation, like that of a rotary switch, can be presented at each joint so that the user vividly perceives his/her movement. This paper presents a simple mechanical prototype that is attached to the elbow.",
    "keywords": [
      "kinesthetic sensation",
      "motion instruction",
      "rotary switch feeling"
    ],
    "doi": "10.1145/2160125.2160153",
    "url": "https://doi.org/10.1145/2160125.2160153",
    "citations": 3,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "28",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160154",
    "title": "Gesture keyboard with a machine learning requiring only one camera",
    "authors": [
      "Taichi Murase",
      "Atsunori Moteki",
      "Genta Suzuki",
      "Takahiro Nakai",
      "Nobuyuki Hara",
      "Takahiro Matsuda"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "In this paper, the authors propose a novel gesture-based virtual keyboard (Gesture Keyboard) that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique. Gesture Keyboard tracks the user's fingers and recognizes finger motions to judge keys input in the horizontal direction. Real-Adaboost (Adaptive Boosting), a machine learning technique, uses HOG (Histograms of Oriented Gradients) features in an image of the user's hands to estimate keys in the depth direction. Each virtual key follows a corresponding finger, so it is possible to input characters at the user's preferred hand position even if the user displaces his hands while inputting data. Additionally, because Gesture Keyboard requires only one camera, keyboard-less devices can implement this system easily. We show the effectiveness of utilizing a machine learning technique for estimating depth.",
    "keywords": [
      "gesture recognition",
      "hand gesture",
      "keyboard",
      "machine learning"
    ],
    "doi": "10.1145/2160125.2160154",
    "url": "https://doi.org/10.1145/2160125.2160154",
    "citations": 11,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "29",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2160125.2160155",
    "title": "Kaleidoscopes for binocular rivalry",
    "authors": [
      "Yoichi Ochiai"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "When you look into the two kaleidoscopes at the same time, a wonderful and strange scene is in sight. We developed the stereo electronic kaleidoscope with the high definition display. It shows the images as beautiful as classic kaleidoscopes can show.We tested and selected the images which cause the binocular rivalry effect. This work makes the augmented kaleidoscopes which gives us wonderful feeling on the structure and functions of our brain.",
    "keywords": [
      "binocular rivalry",
      "brain hacks",
      "high resolution display",
      "kaleidoscopes",
      "optical illusion",
      "stereo-vision"
    ],
    "doi": "10.1145/2160125.2160155",
    "url": "https://doi.org/10.1145/2160125.2160155",
    "citations": 1,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "30",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160156",
    "title": "Invisible feet under the vehicle",
    "authors": [
      "Yoichi Ochiai",
      "Keisuke Toyoshima"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "When we drive a car, we have many blind spots. The information from the outside is almost limited to vision and sound. We have a vision that the driver and the car unified and moves as one[1] to face the problem of the gap of the information between the outside of the car and inside. We call the unity of driver and the car Homunculus which makes communication with the outside of the vehicle.With this concept, we developed a new haptic system. Our system assigns the sense of driver's foot to the bottom of a car. It connects nine vibration motors on a grid to the nine IR distance sensors on a grid. If users use this system, they can feel something passed through the bottom of a car, a bump and so on with feeling like a sole of hid foot was touched. It is like a invisible foot(Figure1) is sticked out bottom of the cars.We applied our prototype to several cases in driving and found several interesting points on this. We would discuss on these points on this paper.",
    "keywords": [
      "augmented drive",
      "tactile feedback",
      "vehicle"
    ],
    "doi": "10.1145/2160125.2160156",
    "url": "https://doi.org/10.1145/2160125.2160156",
    "citations": 3,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "31",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2160125.2160157",
    "title": "Presentation of directional information by sound field control",
    "authors": [
      "Yutaka Takase",
      "Shoichi Hasegawa"
    ],
    "year": 2012,
    "conference": "AH",
    "conferenceYear": "AH '12",
    "abstract": "We propose a novel method for presentation of directional information. The system presents an perception of presence of obstacles by controlling environment sound field. Visual map and voice prompts are practical method for directional information presentation and used in car navigation system. However they occupy sense of sight and hearing.By contrast, our method can present directional information naturally without occupying sensory channels. Therefore, users can get benefits of directional information with enjoying surrounding environment.",
    "keywords": [
      "environmental sound",
      "virtual reality",
      "walking navigation"
    ],
    "doi": "10.1145/2160125.2160157",
    "url": "https://doi.org/10.1145/2160125.2160157",
    "citations": 4,
    "booktitle": "Proceedings of the 3rd Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Megève, France",
    "articleno": "32",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459267",
    "title": "NeuroPlace: making sense of a place",
    "authors": [
      "Lulwah Al-Barrak",
      "Eiman Kanjo"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "The ability to detect mental states, whether relaxation or stressed, would be useful in categorizing places according to their impact on our brains and many other domains. Newly available, affordable and dry-electrode devices make electroencephalography headsets (EEG) feasible to use outside the lab, for example in open spaces and shopping malls. The purpose of this pervasive experimental manipulation is to analyze brain signals in order to label outdoor places according to how users perceive them with a focus on ---relaxing and ---stressful mental states. That is, when the user is experiencing tranquil brain waves or not when visiting a particular place. This paper demonstrates the potential of exploiting the temporal structure of EEG signals in making sense of outdoor places. The EEG signals induced by the place stimuli are analyzed and exploited to distinguish what we refer to as a place signature.",
    "keywords": [
      "EEG",
      "mobile sensing",
      "neurology",
      "pervasive computing",
      "social and behavioral sciences"
    ],
    "doi": "10.1145/2459236.2459267",
    "url": "https://doi.org/10.1145/2459236.2459267",
    "citations": 22,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "186–189",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459268",
    "title": "A monitoring device as assistive lifestyle technology: combining functional needs with pleasure",
    "authors": [
      "Florian Güldenpfennig",
      "Geraldine Fitzpatrick"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Assistive Technologies can be of enormous help for people with disabilities. Still, such supportive devices are often considered to be poor in aesthetics, leaving the person feeling stigmatised by the technology and resulting in a reduced usage and compliance. In this paper we report on a case study of a young person suffering from cerebral palsy and describe a wearable device, RemoteLogCam, that was designed to help him self-manage his hand spasms and at the same time provide his first opportunity to take his own photos. We call this an example of assistive lifestyle technologies (ALT), designed not only to assist people with special needs in a functional sense, but that also enhance the experience of such a device in a pleasing way. In this case, over the course of 6 months use to date, RemoteLogCam augmented our participant's own self-management of spasms and his creative and practical documentation needs.",
    "keywords": [
      "assistive lifestyle technology",
      "case study",
      "cerebral palsy",
      "device stigma",
      "digital photography",
      "empowerment",
      "photography",
      "sensor glove",
      "special needs",
      "wearable computing"
    ],
    "doi": "10.1145/2459236.2459268",
    "url": "https://doi.org/10.1145/2459236.2459268",
    "citations": 11,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "190–193",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2459236.2459269",
    "title": "Using RFID tags as reference for phone location and orientation in daily life",
    "authors": [
      "F. Wahl",
      "O. Amft"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "This paper investigates a novel approach to obtain location and orientation annotation for smartphones in real-life recordings. We attached RFID tags to places where phones are located in daily life, such as pockets and backpacks. The RFID reader integrated in modern smartphones was used to continuously scan for registered tags. In a first evaluation across several full-day recordings and using nine locations, our approach achieved an accuracy of 80 % when compared to a manual diary. Only 5.3 % of all tags were missed. We conclude that RFID-based location and orientation tagging is a viable option to obtain ground truth reference for real-life activity recognition algorithm developments.",
    "keywords": [
      "NFC",
      "ground truth",
      "unsupervised annotation"
    ],
    "doi": "10.1145/2459236.2459269",
    "url": "https://doi.org/10.1145/2459236.2459269",
    "citations": 5,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "194–197",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459270",
    "title": "Recovering 3-D gaze scan path and scene structure from inside-out camera",
    "authors": [
      "Yuto Goto",
      "Hironobu Fujiyoshi"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "First-Person Vision (FPV) is a wearable sensor that takes images from a user's visual field and interprets them, with available information about the user's head motion and gaze, through eye tracking [1]. Measuring the 3-D gaze trajectory of a user moving dynamically in 3-D space is interesting for understanding a user's intention and behavior. In this paper, we present a system for recovering 3-D scan path and scene structure in 3-D space on the basis of ego-motion computed from an inside-out camera. Experimental results show that the 3-D scan paths of a user moving in complex dynamic environments were recovered.",
    "keywords": [
      "3-D gaze scan path",
      "ego-motion",
      "inside-out camera"
    ],
    "doi": "10.1145/2459236.2459270",
    "url": "https://doi.org/10.1145/2459236.2459270",
    "citations": 2,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "198–201",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459271",
    "title": "3D building reconstruction and thermal mapping in fire brigade operations",
    "authors": [
      "Christian Schönauer",
      "Emanuel Vonach",
      "Georg Gerstweiler",
      "Hannes Kaufmann"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Fire fighting remains a dangerous profession despite many recent technological and organizational measures. Sensors and technical systems can augment the performance of fire fighters to increase safety and efficiency during operation. An important aspect in that context is the awareness of location, structure and thermal properties of the environment.This paper focuses on the design and development of a mobile system, which can reconstruct a 3d model of a building's interior structure in real-time and fuses the visualization with the image of a thermal camera. In addition the position and viewing direction of the fire fighter within the model is determined and a thermal map can be generated from the gathered data, which could help an operational commander to guide his men during a mission.First tests with our system in different situations showed good results, being able to reconstruct different larger scenes and create thermal maps thereof.",
    "keywords": [
      "augmented reality",
      "fire fighter safety",
      "real-time dense reconstruction",
      "thermal camera"
    ],
    "doi": "10.1145/2459236.2459271",
    "url": "https://doi.org/10.1145/2459236.2459271",
    "citations": 26,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "202–205",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459272",
    "title": "Communication pedometer: a discussion of gamified communication focused on frequency of smiles",
    "authors": [
      "Yukari Hori",
      "Yutaka Tokuda",
      "Takahiro Miura",
      "Atsushi Hiyama",
      "Michitaka Hirose"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Communication skills are essential in our everyday lives. Yet, it can be difficult for people with communication disorders to improve these skills without professional help. Quantifying communication and providing feedback advice in an automated manner would significantly improve that process. Therefore, we aim to propose a method to monitor communication that employs life-logging technology to evaluate parameters related to communication skills. In our study, we measured frequency of smiles as a metric for smooth communication. In addition, smiling can improve happiness even if a smile is mimicked. Ultimately, we provided feedback results to users in a gamified form and investigated the effects of feedback on communication.",
    "keywords": [
      "communication skill",
      "feedback",
      "game",
      "life-log",
      "motivation",
      "smile",
      "visualization"
    ],
    "doi": "10.1145/2459236.2459272",
    "url": "https://doi.org/10.1145/2459236.2459272",
    "citations": 23,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "206–212",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2459236.2459273",
    "title": "A smile/laughter recognition mechanism for smile-based life logging",
    "authors": [
      "Kurara Fukumoto",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Most situations that cause people to smile are important and treasured events that happen in front of the other people. In life-logging systems that record everything with wearable cameras and microphones, it is difficult to extract the important events from a large amount of recordings. In this research, we design and implement a smile-based life-logging system that focuses on smile/laughter for indexing the interesting/enjoyable events on a recorded video. Our system, features an original smile/laughter recognition device using photo interrupters that is comfortable enough for daily use and proposed an algorithm that detects smile/laughter separately by threshold-based clustering. The main challenge is that, since the reasons people smile and laugh are quite diverse, the system has to detect a smile/laughter as different events. Evaluation results showed that our mechanism achieved a 73%/94% accuracy in detecting smile/laughter, while actual use of the system showed that it can accurately detect interesting scenes from a recorded life log.",
    "keywords": [
      "context awareness",
      "programming",
      "wearable computing"
    ],
    "doi": "10.1145/2459236.2459273",
    "url": "https://doi.org/10.1145/2459236.2459273",
    "citations": 37,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "213–220",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2459236.2459274",
    "title": "A system for visualizing human behavior based on car metaphors",
    "authors": [
      "Hiroaki Sasaki",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "There are many accidents such as bumping between walkers in crowded places. One of reasons for them is that it is difficult for each person to predict the behaviors of other people. On the other hand, cars implicitly communicate with other cars by presenting their contexts with equipments such as brake lights and turn signals. In this paper, we propose a system for visualizing the user context by using information presentation methods based on those found in cars, such as wearing LEDs as brake lights, which can be seen by surrounding people. The evaluation results when using our prototype system confirmed that our method visually and intuitively presented the user context. In addition, we evaluated the visibility effects of changing the mounting position of the wearable devices.",
    "keywords": [
      "car metaphor",
      "visualizing",
      "wearable computing"
    ],
    "doi": "10.1145/2459236.2459274",
    "url": "https://doi.org/10.1145/2459236.2459274",
    "citations": 3,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "221–228",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459275",
    "title": "Geometrically consistent mobile AR for 3D interaction",
    "authors": [
      "Hikari Uchida",
      "Takashi Komuro"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "In this study, we propose a method to present an image that maintains geometric consistency between the actual scene outside the mobile display and the camera image. Thereby, we expect that interaction with the virtual object through the mobile display becomes more intuitive, and the operability is improved. By cameras mounted on the front and back of the mobile display, the user's face position and the distance to the subject are obtained. Using the information, it is possible to present an image that maintains geometric consistency between the inside and outside of the display depending on the user's viewpoint.",
    "keywords": [
      "augmented reality",
      "geometric consistency",
      "geometric correction",
      "mobile device",
      "projective transformation"
    ],
    "doi": "10.1145/2459236.2459275",
    "url": "https://doi.org/10.1145/2459236.2459275",
    "citations": 7,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "229–230",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2459236.2459276",
    "title": "Muscle-propelled force feedback: bringing force feedback to mobile devices using electrical stimulation",
    "authors": [
      "Pedro Lopes",
      "Lars Butzmann",
      "Patrick Baudisch"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "We propose mobile force feedback devices based on actuating the user's muscles using electrical stimulation. Since this allows us to eliminate exoskeletons, motors and reduce battery size, our approach results in devices that are substantially smaller and lighter than traditional motor-based devices, and thus suitable for usage on-the-go. We present a simple prototype that we mount to the back of a mobile phone. It actuates users' forearm muscles via four electrodes, causing the muscles to contract involuntarily, so that users tilt the device sideways. As users resist this motion using their other arm, they perceive force feedback. We demonstrate the interaction at the example of three interactive videogames in which our approach to mobile force feedback provides a richer gaming experience.",
    "keywords": [
      "actuation",
      "force feedback",
      "mobile"
    ],
    "doi": "10.1145/2459236.2459276",
    "url": "https://doi.org/10.1145/2459236.2459276",
    "citations": 14,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "231–232",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2459236.2459277",
    "title": "Augmented reality using a 3D motion capturing suit",
    "authors": [
      "Ionut Damian",
      "Mohammad Obaid",
      "Felix Kistler",
      "Elisabeth André"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "In the paper, we propose an approach that immerses the human user in an Augmented Reality (AR) environment with the use of an inertial motion capturing suit and a Head Mounted Displays system. The proposed approach allows for full body interaction with the AR environment in real-time and it does not require the use of any markers or cameras.",
    "keywords": [],
    "doi": "10.1145/2459236.2459277",
    "url": "https://doi.org/10.1145/2459236.2459277",
    "citations": 9,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "233–234",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2459236.2459278",
    "title": "Virtual prototyping of a spatial audio interface for obstacle avoidance using image processing",
    "authors": [
      "Yoko Nakanishi",
      "Yasuto Nakanishi"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "In this paper, we describe a process used to prototype a spatial audio user interface using virtual simulation. The interface is designed to assist mobile users who are in motion avoid physical dangers while their eyes are engaged. Our prototype system uses a simple form of a non-speech, spatial audio composed with optical flow through a head mounted camera. This paper describes our prototyping process involving various candidate image processing and audio mappings via 3D virtual simulation.",
    "keywords": [
      "augmented reality",
      "mobile",
      "prototyping",
      "soniphication"
    ],
    "doi": "10.1145/2459236.2459278",
    "url": "https://doi.org/10.1145/2459236.2459278",
    "citations": 0,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "235–236",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459279",
    "title": "Optimal selection of electrodes for muscle electrical stimulation using twitching motion measurement",
    "authors": [
      "Manami Katoh",
      "Narihiro Nishimura",
      "Maki Yokoyama",
      "Taku Hachisu",
      "Michi Sato",
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Muscle electrical stimulation envisions a wide range of human augmentation application. However, the applications commonly have issue of optimal electrodes placement. In this paper, we propose a method to select the optimal electrodes placement for finger flexion using twitching motion measurement. We delivered electrical stimulation producing twitching motion and measured the acceleration. By summing and averaging the acceleration waveforms and taking the difference between the maximum and minimum value, we measured the contribution of the electrical stimulation and used it to select the optimal electrodes pair for the movement. Preliminary experiment with four electrodes showed feasibility of our method.",
    "keywords": [
      "FES",
      "electrical stimulation",
      "electrode",
      "selectivity",
      "twitching motion"
    ],
    "doi": "10.1145/2459236.2459279",
    "url": "https://doi.org/10.1145/2459236.2459279",
    "citations": 11,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "237–238",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459280",
    "title": "Caruso: augmenting users with a tenor's voice",
    "authors": [
      "Jochen Feitsch",
      "Marco Strobel",
      "Christian Geiger"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "In this poster paper we describe an ongoing project that aims at providing users with the experience to sing like a tenor from the 20th century. We combine 3D body tracking, face recognition and morphing, sound synthesis and 3D character rendering into an interactive media application. Although this project is still in a preliminary stage, first prototypical results are encouraging to continue this work towards a media installation for a science fair.",
    "keywords": [
      "musical interface",
      "sound synthesis",
      "virtual character"
    ],
    "doi": "10.1145/2459236.2459280",
    "url": "https://doi.org/10.1145/2459236.2459280",
    "citations": 1,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "239–240",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459281",
    "title": "Towards participatory design for contextual visualization in education using augmented reality x-ray",
    "authors": [
      "Marc Ericson C. Santos",
      "Goshiro Yamamoto",
      "Mitsuaki Terawaki",
      "Jun Miyazaki",
      "Takafumi Taketomi",
      "Hirokazu Kato"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "We propose Augmented Reality (AR) x-ray as an educational tool for contextual visualization--presenting virtual information in the rich context of a real environment. Teachers and students evaluated a state-of-the-art implementation of AR x-ray. Results show that realism, visibility, and perception of depth in AR x-ray are not significantly different from viewing 3D models with no occlusion cues. Moreover, teachers perceive AR x-ray useful.",
    "keywords": [
      "augmented reality",
      "contextual learning",
      "participatory design"
    ],
    "doi": "10.1145/2459236.2459281",
    "url": "https://doi.org/10.1145/2459236.2459281",
    "citations": 0,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "241",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "1",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459282",
    "title": "Evaluation of a tactile device for augmentation of audiovisual experiences with a pseudo heartbeat",
    "authors": [
      "Narihiro Nishimura",
      "Taku Hachisu",
      "Michi Sato",
      "Shogo Fukushima",
      "Hiroyuki Kajimoto"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "The impression that the viewer has of characters is an important factor affecting the viewer's opinion of audiovisual media, such as movies, television and video games. In particular, when we feel affection toward characters, we sometimes go so far as to identify ourselves as one of them, leading to extreme immersion in the content of the media. Therefore, there is the possibility that content technology can control affective feelings towards characters and create an immersive environment. We propose a device that can be used to facilitate the affection of the user by controlling their positive feelings toward characters in the media content. Previous studies have shown that emotional or physiological states can be altered by the visual and auditory presentation of false heartbeats [1, 2, 3]. However, if these techniques are applied to audiovisual media such as movies, television, or video games, the audio and visual heartbeat cues may interfere with and pollute the audiovisual content.",
    "keywords": [],
    "doi": "10.1145/2459236.2459282",
    "url": "https://doi.org/10.1145/2459236.2459282",
    "citations": 0,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "242",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "1",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459283",
    "title": "A real-time gait improvement tool using a smartphone",
    "authors": [
      "Hirotaka Kashihara",
      "Hiroki Shimizu",
      "Hiroyoshi Houchi",
      "Masato Yoshimi",
      "Tsutomu Yoshinaga",
      "Hidetsugu Irie"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "Recent handy devices are provided with various sensors and have realized a lot of functions as downsizing and speeding up of computers. Currently smartphones occupy significant positions as the multifunctional handy devices. One of the most observable feature is that the users carry the smartphone whenever leaving home. Analyzing the motion measured by such device can be useful to improve lifestyle habits. Gaits should be focused as the representative behavior of daily living, which is shown by the fact that there are a lot of exercises intended to improve gaits.",
    "keywords": [],
    "doi": "10.1145/2459236.2459283",
    "url": "https://doi.org/10.1145/2459236.2459283",
    "citations": 8,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "243",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "1",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2459236.2459284",
    "title": "Applying augmented reality to industrial settings",
    "authors": [
      "Elina Vartiainen",
      "Peder Boberg",
      "Oskar Qvarnström",
      "Jonas Brönmark"
    ],
    "year": 2013,
    "conference": "AH",
    "conferenceYear": "AH '13",
    "abstract": "State-of-the-art mobile devices containing various sensors and interaction technologies have enabled the development of novel solutions for people working in industrial settings. In particular, introducing augmented reality into the mobile device domain could help maintenance engineers while they perform work tasks in a factory. This poster presents and discusses two concepts that explore how maintenance engineers could use augmented reality to view additional information related to equipment found in a factory setting.",
    "keywords": [
      "augmented reality",
      "industry",
      "mobile devices"
    ],
    "doi": "10.1145/2459236.2459284",
    "url": "https://doi.org/10.1145/2459236.2459284",
    "citations": 0,
    "booktitle": "Proceedings of the 4th Augmented Human International Conference",
    "pages": "244",
    "publisher": "Association for Computing Machinery",
    "location": "Stuttgart, Germany",
    "articleno": "",
    "numpages": "1",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582052",
    "title": "Video generation method based on user's tendency of viewpoint selection for multi-view video contents",
    "authors": [
      "Yuki Muramatsu",
      "Takatsugu Hirayama",
      "Kenji Mase"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "A multi-view video makes it possible for users to watch video contents, for example, live concerts or sports events, more freely from various viewpoints. However, the users need to select a camera that captures a scene from their own preferred viewpoint at each event. In this paper, we propose a video generation method based on the user's View Tendency, which is a tendency of viewpoint selection according to the user-dependent interest for multi-view video content. The proposed method learns the View Tendency by Support Vector Machine (SVM) using several measures such as the geometric features of an object. Then, this method estimates the consistency of each viewpoint with the learned View Tendency and integrates the estimation results to obtain a temporal sequence of the viewpoints. The proposed method enables the users to reduce the burden of viewpoint selection and to watch the viewpoint sequence that reflects the interest as viewing assistance for the multi-view video content.",
    "keywords": [
      "machine learning",
      "multi-view video",
      "soccer",
      "video generation",
      "viewing assistance"
    ],
    "doi": "10.1145/2582051.2582052",
    "url": "https://doi.org/10.1145/2582051.2582052",
    "citations": 8,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "1",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582053",
    "title": "Tearsense: a sensor system for illuminating and recording teardrops",
    "authors": [
      "Marina Mitani",
      "Yasuaki Kakehi"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "People shed tears when they become emotional watching movies or reading novels. In this research, we propose a sensor system for illuminating and recording teardrops and describe its applications pertaining to entertainment content such as movies and novels. A tape sensor is attached below the eyes, which is controlled by a microcontroller and a PC. Two parallel lines are drawn on the tape with conductive ink, and when a teardrop runs over these two lines, a microcontroller senses the change in electrical voltage. In this paper, we propose a system that provides a new way to experience and communicate reactions to entertainment content by converting tears into light or sound in real-time, or by sharing with others archives of the content that has moved us to tears.",
    "keywords": [
      "sharing emotions and preferences",
      "teardrops",
      "watching movies"
    ],
    "doi": "10.1145/2582051.2582053",
    "url": "https://doi.org/10.1145/2582051.2582053",
    "citations": 2,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "2",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582054",
    "title": "Present information through afterimage with eyes closed",
    "authors": [
      "Kozue Nojiri",
      "Suzanne Low",
      "Koki Toda",
      "Yuta Sugiura",
      "Youichi Kamiyama",
      "Masahiko Inami"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We propose a display method using the afterimage effect to illustrate images, so that people can perceive the images with their eyes closed. Afterimage effect is an everyday phenomenon that we often experienced and it is commonly utilized in many practical situations such as in movie creation. However, many of us are not aware of it. We strongly believe that this afterimage effect is an interesting phenomenon to display information to the users. We conducted an experiment to compare the duration of the afterimage effect to the duration of participant exposure to the image projection. We also prototyped a wearable type display to give more flexibility and mobility to our proposal. With this, one can utilize this method for various applications such as to confirm password at a bank etc.",
    "keywords": [
      "afterimage effect",
      "blink",
      "display"
    ],
    "doi": "10.1145/2582051.2582054",
    "url": "https://doi.org/10.1145/2582051.2582054",
    "citations": 1,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "3",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582055",
    "title": "Unloading muscle activation enhances force perception",
    "authors": [
      "Yuichi Kurita",
      "Jumpei Sato",
      "Takayuki Tanaka",
      "Minoru Shinohara",
      "Toshio Tsuji"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this study, we examined (1) weight discrimination capability of human subjects with different body postures, and (2) sensorimotor capability of human subjects when using a muscle assistive equipment. According to previous studies, humans can sense the intensity of external stimulus more accurately when voluntary muscle activation is less. We developed a three-dimensional musculoskeletal model based on an upper extremity model, and calculated the muscle activity required to keep a posture. We also conducted human experiments and revealed that the weight discrimination capability improves as voluntary muscle activation is less. Based on the experimental results, we developed a muscle assistive equipment that unloads the weight of one's upper limb and evaluated the improvement in the sensorimotor capability when using the equipment. The results show that assisting the muscle load is effective to improve the sensorimotor performance.",
    "keywords": [
      "force perception capability",
      "muscle assistive equipment",
      "sensorimotor enhancement"
    ],
    "doi": "10.1145/2582051.2582055",
    "url": "https://doi.org/10.1145/2582051.2582055",
    "citations": 3,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "4",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582056",
    "title": "Multi-touch steering wheel for in-car tertiary applications using infrared sensors",
    "authors": [
      "Shunsuke Koyama",
      "Yuta Sugiura",
      "Masa Ogata",
      "Anusha Withana",
      "Yuji Uema",
      "Makoto Honda",
      "Sayaka Yoshizu",
      "Chihiro Sannomiya",
      "Kazunari Nawa",
      "Masahiko Inami"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper proposes a multi-touch steering wheel for in-car tertiary applications. Existing interfaces for in-car applications such as buttons and touch displays have several operating problems. For example, drivers have to consciously move their hands to the interfaces as the interfaces are fixed on specific positions. Therefore, we developed a steering wheel where touch positions can correspond to different operating positions. This system can recognize hand gestures at any position on the steering wheel by utilizing 120 infrared (IR) sensors embedded in it. The sensors are lined up in an array surrounding the whole wheel. An Support Vector Machine (SVM) algorithm is used to learn and recognize the different gestures through the data obtained from the sensors. The gestures recognized are flick, click, tap, stroke and twist. Additionally, we implemented a navigation application and an audio application that utilizes the torus shape of the steering wheel. We conducted an experiment to observe the possibility of our proposed system to recognize flick gestures at three positions. Results show that an average of 92% of flick could be recognized.",
    "keywords": [
      "automobile",
      "gesture recognition",
      "infrared sensor",
      "interaction design",
      "multi-touch",
      "torus interface"
    ],
    "doi": "10.1145/2582051.2582056",
    "url": "https://doi.org/10.1145/2582051.2582056",
    "citations": 21,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "5",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582057",
    "title": "Evaluating effect of types of instructions for gesture recognition with an accelerometer",
    "authors": [
      "Kazuya Murao",
      "Tsutomu Terada"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Mobile phones or remotes for video games using gesture recognition technologies enable easy and intuitive operations such as scrolling browser and drawing objects. Gesture input has an advantage of rich expressive power over the conventional interfaces, but it is difficult to share the gesture trajectory with other people through writing or verbally. In this paper, we evaluate how user gestures change according to the types of the instruction. We obtained acceleration data for 10 kinds of gestures instructed through three types of texts, figures, and videos, totalling 44 patterns from 13 test subjects, for a total of 2,630 data samples.",
    "keywords": [
      "accelerometer",
      "gesture recognition",
      "instructing gestures"
    ],
    "doi": "10.1145/2582051.2582057",
    "url": "https://doi.org/10.1145/2582051.2582057",
    "citations": 1,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "6",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582058",
    "title": "Eyefeel &amp; EyeChime: a face to face communication environment by augmenting eye gaze information",
    "authors": [
      "Asako Hosobori",
      "Yasuaki Kakehi"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In face-to-face communication, humans convey nonverbal information to supplement verbal language. Eye gaze in particular is a critical element. While a variety of studies on communication support focusing on eye gaze have been performed in the past, most of these studies have aimed to support communications between people in remote locations. In contrast, this study aims to extend and transform gaze to lower the hurdles of establishing communication, or to induce a new form of communication through gaze in face-to-face communication. As a specific proposal, in this study we developed two types of systems: Eyefeel, which converts and delivers the gaze of another as tactile information, and EyeChime, which produces a spatial presentation by converting events such as gazing at another or eyes contact to sound. The preliminary study suggested that these interfaces induced communication through active use of eye gaze, gave users the opportunity to increase the amount of time gazes were sent to the conversation partner, and that the hurdles to make eye contact were lowered. In this study, we discuss the design and implementation of the system as well as the details of its use.",
    "keywords": [
      "augmented reality",
      "eye gaze",
      "face-to-face communication",
      "interface"
    ],
    "doi": "10.1145/2582051.2582058",
    "url": "https://doi.org/10.1145/2582051.2582058",
    "citations": 3,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "7",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582059",
    "title": "On achieving dependability for wearable computing by device bypassing",
    "authors": [
      "Tsutomu Terada",
      "Seiji Takeda",
      "Masahiko Tsukamoto",
      "Yutaka Yanagisawa",
      "Yasue Kishino",
      "Takayuki Sugyama"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "When using wearable computers used for medical operation and aviation safety, disrupted information presentation due to hardware/software problems can have serious consequences including medical accidents. We propose a mechanism that maintains information presentation in such situatoins by I/O device bypassing. In out method, I/O devices directly communicate with other devices if a system failure happens. The proposed method selects appropriate data converters by considering recognizability in order to present information that is easily understandable to users. We confirmed that the proposed method works effectively by implementing several applications with our prototype system.",
    "keywords": [
      "dependability",
      "wearable computing",
      "wearable user interfaces"
    ],
    "doi": "10.1145/2582051.2582059",
    "url": "https://doi.org/10.1145/2582051.2582059",
    "citations": 0,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "8",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582060",
    "title": "Haptic foot interface for language communication",
    "authors": [
      "Erik Hill",
      "Hiroyuki Hatano",
      "Masahiro Fujii",
      "Yu Watanabe"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper examines the feasibility of language transmission through a haptic foot interface. The devices tested the placement, timing, and complexity of an array of vibrating electromagnets with the optimal device using an array of ten electromagnets placed under the arch of one foot. Moderate proficiency was reached after only an hour of training at which point the subjects were able to read a short e-mail through the device.",
    "keywords": [
      "HCI",
      "feet",
      "haptic interface",
      "magnets",
      "vibration"
    ],
    "doi": "10.1145/2582051.2582060",
    "url": "https://doi.org/10.1145/2582051.2582060",
    "citations": 6,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "9",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582061",
    "title": "A half-implant device on fingernails",
    "authors": [
      "Emi Tamaki",
      "Ken Iwasaki"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Hand gesture feedback systems using tactile or visual information can only be used in given situations because of the limitations of the device features such as the need for a battery. In this paper, a half-implant device is suggested. The half-implant device consists of a radio frequency (RF) receiving antenna, small electronic parts, and UV gel. The UV gel is used to glue the parts onto the filed user's nail and cover the parts meant to be waterproof. The device receives power from the RF antenna; therefore, it does not require a battery to function. The device notifies whether the finger is in a gesture space by lighting an LED or activating a vibration motor. The primary benefit of this device is that the user can feel hand gesture feedback, anytime and anywhere. The device can be placed on the users' fingernail for approximately three weeks. To verify the devices' influence on the users' gesture task, we conducted a preliminary user study. The experiment revealed that the tactile notification reduced the task time by 2.62 seconds compared to that of the test with no feedback. We also investigated user's acceptability of this kind of technology. It revealed that this technology is acceptable only when it can be removed in the user's will and used in daily-life.",
    "keywords": [
      "RF (radio frequency)",
      "design",
      "fingertip",
      "half implant",
      "haptic",
      "nail",
      "tactile",
      "tangible",
      "wearable"
    ],
    "doi": "10.1145/2582051.2582061",
    "url": "https://doi.org/10.1145/2582051.2582061",
    "citations": 5,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "10",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582062",
    "title": "Pressure detection on mobile phone by camera and flash",
    "authors": [
      "Suzanne Low",
      "Yuta Sugiura",
      "Dixon Lo",
      "Masahiko Inami"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper proposes a method to detect pressure asserted on a mobile phone by utilizing the back camera and flash on the phone. There is a gap between the palm and camera when the phone is placed on the palm. This allows the light from the flashlight to be reflected to the camera. However, when pressure is applied on the phone, the gap will reduce, reducing the brightness captured by the camera. This phenomenon is applied to detect two gestures: pressure applied on the screen and pressure applied when user squeezes the phone. We also conducted an experiment to detect the change in brightness level depending on the amount of force asserted on the phone when it is placed in two positions: parallel to the palm and perpendicular to the palm. The results show that when the force increases, the brightness level decreases. Using the phones ability to detect fluctuations in brightness, various pressure interaction applications such as for gaming purposes may be developed.",
    "keywords": [
      "camera",
      "flash light",
      "mobile phone",
      "pressure sensing"
    ],
    "doi": "10.1145/2582051.2582062",
    "url": "https://doi.org/10.1145/2582051.2582062",
    "citations": 9,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "11",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582063",
    "title": "On the tip of my tongue: a non-invasive pressure-based tongue interface",
    "authors": [
      "Jingyuan Cheng",
      "Ayano Okoso",
      "Kai Kunze",
      "Niels Henze",
      "Albrecht Schmidt",
      "Paul Lukowicz",
      "Koichi Kise"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Mobile and wearable devices became pervasive in daily life. The dominant input techniques for mobile and wearable technology are touch and speech. Both approaches are not appropriate in all settings. Therefore, we propose a novel interface that is controlled through the tongue. It is based on an array of textile pressure sensors attached to the user's cheek. It can be easily integrated into helmets or face masks in a non-invasive way. In an initial study, we investigate gestures for tongue-based interface. Six participants repeatedly performed five simple tongue gestures. We show that gestures can be recognized with 98% accuracy. Based on feedback from participants, we discuss potential use cases and provide an outlook on further improvement of the system.",
    "keywords": [
      "hands-free gestures",
      "mobile HCI",
      "pressure sensor",
      "tongue interface",
      "user interface",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582063",
    "url": "https://doi.org/10.1145/2582051.2582063",
    "citations": 28,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "12",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582064",
    "title": "HoverBall: augmented sports with a flying ball",
    "authors": [
      "Kei Nitta",
      "Keita Higuchi",
      "Jun Rekimoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Balls are the most popular equipment for sports. To play with balls, certain physical methods, or \"vocabularies,\" such as throwing, hitting, spinning, or kicking have been developed by reflecting the fact that balls obey physical dynamics. This feature forms the foundation of ball-based sports; however, we consider that it limits the possibility of such sports. For instance, the speed of balls could be considerably fast for small children, senior people, or people with physical disabilities. In this paper, we propose a flying ball based on quadcopter technology. This ball has the ability hover and to change its location and behavior based on the context of the sport or game. With this technology, the physical dynamics of a ball can be re-programmed by sports designers, and new ball-playing vocabularies, such as hovering, anti-gravity, proximity, or remote manipulation, can be introduced to extend the method in which people interact with balls. In this paper, we introduce this concept as a method of augmenting sports, and present our initial flying ball system that consists of a grid shell that comprises a micro quadcopter, and demonstrates new sports interactions with the ball.",
    "keywords": [
      "augmented sports",
      "ball",
      "interaction device",
      "unmanned aerial vehicle"
    ],
    "doi": "10.1145/2582051.2582064",
    "url": "https://doi.org/10.1145/2582051.2582064",
    "citations": 53,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "13",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582065",
    "title": "Emotional priming of mobile text messages with ring-shaped wearable device using color lighting and tactile expressions",
    "authors": [
      "Gilang Andi Pradana",
      "Adrian David Cheok",
      "Masahiko Inami",
      "Jordan Tewell",
      "Yongsoon Choi"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this paper, as a hybrid approach to place a greater emphasis on existing cues in Computer Mediated Communication (CMC), the authors explore the emotional augmentation benefit of vibro-tactile stimulation, color lighting, and simultaneous transmission of both signals to accompany text messages. Ring U, A ring-shaped wearable system aimed at promoting emotional communications between people using vibro-tactile and color lighting expressions, is proposed as the implementation method. The result of the experiment has shown that non-verbal stimuli can prime the emotion of a text message, and it can be driven into the direction of the emotional characteristic of the stimuli. Positive stimuli can prime the emotion to a more positive valence, and negative stimuli can invoke a more negative valence. Another finding from the experiment is that compared to the effect on valence, touch stimuli have more effect on the activity level.",
    "keywords": [
      "color",
      "emotion",
      "verbal and non-verbal communication",
      "vibro-tactile"
    ],
    "doi": "10.1145/2582051.2582065",
    "url": "https://doi.org/10.1145/2582051.2582065",
    "citations": 43,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/2582051.2582066",
    "title": "In the blink of an eye: combining head motion and eye blink frequency for activity recognition with Google Glass",
    "authors": [
      "Shoya Ishimaru",
      "Kai Kunze",
      "Koichi Kise",
      "Jens Weppner",
      "Andreas Dengel",
      "Paul Lukowicz",
      "Andreas Bulling"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We demonstrate how information about eye blink frequency and head motion patterns derived from Google Glass sensors can be used to distinguish different types of high level activities. While it is well known that eye blink frequency is correlated with user activity, our aim is to show that (1) eye blink frequency data from an unobtrusive, commercial platform which is not a dedicated eye tracker is good enough to be useful and (2) that adding head motion patterns information significantly improves the recognition rates. The method is evaluated on a data set from an experiment containing five activity classes (reading, talking, watching TV, mathematical problem solving, and sawing) of eight participants showing 67% recognition accuracy for eye blinking only and 82% when extended with head motion patterns.",
    "keywords": [
      "Google Glass",
      "IMU",
      "activity recognition",
      "blink frequency",
      "head mounted sensor",
      "infrared proximity sensor"
    ],
    "doi": "10.1145/2582051.2582066",
    "url": "https://doi.org/10.1145/2582051.2582066",
    "citations": 150,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "15",
    "numpages": "4",
    "influentialCitations": 9
  },
  {
    "id": "10.1145/2582051.2582067",
    "title": "PhotoelasticBall: a touch detectable ball using photoelasticity",
    "authors": [
      "Kei Nitta",
      "Toshiki Sato",
      "Hideki Koike",
      "Takuya Nojima"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Balls are a key equipment for sports and entertainments such as juggling, etc. Then, much research has been conducted for developing balls of the next generation to enhance ball related entertainments. Such balls have plenty of special effects such as sound and light, but have limited input method. Those effects are often controlled through ball's native motion by using accelerometers, etc. However, as increasing the variety of special functions of such balls, the appropriate input method should be required. In this research, we developed a force vector sensor sheet unit that can be implemented on the surface of the ball. In this paper, we report the detail of the sensory system and its experimental results.",
    "keywords": [
      "ball",
      "enhanced sports",
      "entertainment",
      "force sensor",
      "haptic",
      "input method",
      "interaction device",
      "photoelasticity"
    ],
    "doi": "10.1145/2582051.2582067",
    "url": "https://doi.org/10.1145/2582051.2582067",
    "citations": 4,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "16",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582068",
    "title": "CarCast: a framework for situated in-car conversation sharing",
    "authors": [
      "Kohei Matsumura",
      "Yasuyuki Sumi"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this paper, we propose a situated in-car conversation sharing framework. People often have conversations in the car. In those conversations, people talk about their points of interest that they have just passed. These conversations may contain valuable information because the conversations reflect situations such as seasons and passenger's own experiences. However, in-car conversations are transient and cannot be shared to others. We therefore aim to share these valuable in-car conversation with others. This paper describes a framework of our in-car conversation sharing system and discusses challenges to realize it.",
    "keywords": [
      "in-car conversations",
      "knowledge sharing",
      "location-aware"
    ],
    "doi": "10.1145/2582051.2582068",
    "url": "https://doi.org/10.1145/2582051.2582068",
    "citations": 6,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "17",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582069",
    "title": "Representing indoor location of objects on wearable computers with head-mounted displays",
    "authors": [
      "Markus Funk",
      "Robin Boldt",
      "Bastian Pfleging",
      "Max Pfeiffer",
      "Niels Henze",
      "Albrecht Schmidt"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "With head-mounted displays becoming more ubiquitous, the vision of extending human object search capabilities using a wearable system becomes feasible. Wearable cameras can recognize known objects and store their indoor location. But how can the location of objects be represented on a wearable device like Google Glass and how can the user be navigated towards the object? We implemented a prototype on a wearable computer with a head-mounted display and compared a last seen image representation against a map representation of the location. We found a significant interaction effect favoring the last seen image with harder hidden objects. Additionally, all objective and subjective measures generally favor the last seen image. Results suggest that a map representation is more helpful for gross navigation and an image representation is more supportive for fine navigation.",
    "keywords": [
      "augmented reality",
      "indoor location",
      "location representation",
      "real-world search",
      "visualization",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582069",
    "url": "https://doi.org/10.1145/2582051.2582069",
    "citations": 26,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "18",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582070",
    "title": "Two-level fast-forwarding using speech detection for rapidly perusing video",
    "authors": [
      "Kazutaka Kurihara",
      "Yoko Sasaki",
      "Jun Ogata",
      "Masataka Goto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In video content such as feature films, the main themes and messages are often sufficiently conveyed through dialogue and narration. To augment human capability to consume video content, here we propose a system for watching such videos at very high speed while ensuring that speech is still comprehensible. Specifically, we employ a purpose-built automatic speech detector to realize two-level fast-forwarding for a wide variety of video content: very fast during segments without speech, and understandably fast during segments with speech. In our experiments, practical performance was achieved by frame-by-frame audio classification using Gaussian mixture models trained on subtitle information from 120 commercial DVD movies.",
    "keywords": [
      "GMM",
      "fast-forwarding",
      "video",
      "voice activity detection"
    ],
    "doi": "10.1145/2582051.2582070",
    "url": "https://doi.org/10.1145/2582051.2582070",
    "citations": 0,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "19",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582071",
    "title": "Real-time typing action detection in a 3D pointing gesture interface",
    "authors": [
      "Risa Ishijima",
      "Kayo Ogawa",
      "Masakazu Higuchi",
      "Takashi Komuro"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this paper, we propose a method to detect typing actions in the air by applying principal component analysis and linear discriminant analysis on time-series finger scale data in real time. The proposed method was implemented to an experimental system of in-air typing interface, and a preliminary user study using a keyboard typing application was conducted. The results showed about 95% of the detection rate of typing actions and more than 80% of the input recognition rate.",
    "keywords": [
      "gesture recognition",
      "machine learning",
      "time-series analysis",
      "virtual keyboard"
    ],
    "doi": "10.1145/2582051.2582071",
    "url": "https://doi.org/10.1145/2582051.2582071",
    "citations": 3,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "20",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582072",
    "title": "The health bar: a persuasive ambient display to improve the office worker's well being",
    "authors": [
      "Victor Mateevitsi",
      "Khairi Reda",
      "Jason Leigh",
      "Andrew Johnson"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Recent research studies have shown the serious health risks associated with prolonged sitting. Standing up and walking on a regular basis has been proved to improve an office worker's well-being. Small behavioral changes like the aforementioned are the basis of preventive medicine and even though they appear easy to follow, in practice are difficult to apply. Advances in technology miniaturization and smart sensors are paving the way for the development of devices that empower preventive medicine. These devices use the power of persuasion to help people change behavior and maintain well-being. They act as an ambient personal 'coach' that monitors and intervenes at the right time. In this paper we present the HealthBar, an ambient persuasive device that helps users break up their prolonged sitting habits.",
    "keywords": [
      "ambient displays",
      "calm technologies",
      "context-awareness",
      "human augmentics",
      "persuasive technologies",
      "preventive healthcare"
    ],
    "doi": "10.1145/2582051.2582072",
    "url": "https://doi.org/10.1145/2582051.2582072",
    "citations": 41,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "21",
    "numpages": "2",
    "influentialCitations": 8
  },
  {
    "id": "10.1145/2582051.2582073",
    "title": "Single-trial decoding for an event-related potential-based brain-computer interface",
    "authors": [
      "Yaming Xu",
      "Yoshikazu Nakajima"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We propose a trial-based decoding method for an event-related potential (ERP)-based brain-computer interface (BCI). In contrast to conventional methods decoding based on single ERP-epoch, we integrate within-trial ERP-epochs and combine a 5-gram language model to ease BCI spelling. Experiment results on 10 subjects show that the proposed method improves the ERP decoding accuracy by 18.05%, when compared with state-of-the-art method.",
    "keywords": [
      "N-gram language model",
      "brain-computer interface (BCI)",
      "multi-class ERP model"
    ],
    "doi": "10.1145/2582051.2582073",
    "url": "https://doi.org/10.1145/2582051.2582073",
    "citations": 0,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "22",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582074",
    "title": "BubStack: a self-revealing chorded keyboard on touch screens to type for remote wall displays",
    "authors": [
      "Hyeonjoong Cho",
      "Chulwon Kim"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "The common soft keyboard on touchscreens for numerous emerging smart devices requires users to look repeatedly at their fingertip locations. To reduce this visual dependency on size-restricted touchscreens, chorded keyboards have been restudied recently. However, one of their intrinsic problems, learning difficulty, limits their widespread use. Here, we introduce a visual guide that makes a chorded keyboard self-revealing to alleviate the learning difficulty. Next, we propose a simple supplementary instrument and a finger-recognition algorithm for tablet computers to be used as remote controllers for wall displays. In this use case, we claim that our self-revealing chorded keyboard and the proposed configuration provide complete visual independence.",
    "keywords": [
      "chorded keyboard",
      "eyes-free",
      "gesture recognition",
      "image processing",
      "self-revelation",
      "text entry",
      "wall display"
    ],
    "doi": "10.1145/2582051.2582074",
    "url": "https://doi.org/10.1145/2582051.2582074",
    "citations": 2,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "23",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582075",
    "title": "AnyButton: unpowered, modeless and highly available mobile input using unmodified clothing buttons",
    "authors": [
      "Liwei Chan",
      "Chien-Ting Weng",
      "Rong-Hao Liang",
      "Bing-Yu Chen"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper presents wearable opportunistic controls using unmodified clothing buttons. Buttons are commonly sewn on formal clothing and often came with multiple duplicates. In this paper, we turn passive buttons into dial widgets. Each button provides simple input modalities (e.g., tap and spin inputs). Multiple buttons allow for modeless and rich interactions. We present AnyButton, a wearable motion-sensor set, allowing for transferring buttons on clothing into mobile input on the move. Our prototype consists of three motion sensors attached on the index fingernail, the wrist, and the elbow. We interpret which button is under user interaction according to the wrist and elbow orientations, and how the button in the user's finger pinches being operated according to the motions on the fingertips. Each button allows for partial tap, discrete spin and dwell spin inputs. By distributing interface to the buttons, applications such as music players and call centers can use opportunistic clothing buttons as wearable controls.",
    "keywords": [],
    "doi": "10.1145/2582051.2582075",
    "url": "https://doi.org/10.1145/2582051.2582075",
    "citations": 1,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "24",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582076",
    "title": "TongueDx: a tongue diagnosis for health care on smartphones",
    "authors": [
      "Ini Ryu",
      "Itiro Siio"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "On the TongueDx system, users can keep track of their health condition by recording the color of tongue coating and body on smartphones. In fact, our system uses tongue diagnosis techniques originated from Traditional Chinese Medicine (TCM) theories. In the theories, tongue symptom as one of the important diagnosing indicators can tell the health of human body. To avoid color error affected by surrounding light, a tongue color calibration by using teeth color is proposed to adjust white balance of the tongue picture. K-means algorithm is used to separate tongue coating from body. From the line graph of tongue coating and body color displayed on smartphones, people can know their health conditions timely. We have evaluated the TongueDx performance for one month in the preliminary user experience.",
    "keywords": [
      "health management",
      "smartphones",
      "tongue diagnosis",
      "traditional Chinese medicine"
    ],
    "doi": "10.1145/2582051.2582076",
    "url": "https://doi.org/10.1145/2582051.2582076",
    "citations": 8,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "25",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582077",
    "title": "Narratology and narrative generation: expanded literary theory and the integration as a narrative generation system (2)",
    "authors": [
      "Takashi Ogata",
      "Shohei Imabuchi",
      "Taisuke Akimoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper overviews our approaches towards narrative generation using various literary theories in the field of narratology in the first part and proposes the synthetic use of the following three literary theories in the integrated narrative generation system in the second part. Although we have studied several literary theories in the relation with the narrative generation, the three literary theories by Propp, Genette and Jauss are organically incorporated in the current version of the integrated system.",
    "keywords": [
      "Genette",
      "Jauss",
      "Propp",
      "expanded literary theory",
      "integrated narrative generation system",
      "narratology"
    ],
    "doi": "10.1145/2582051.2582077",
    "url": "https://doi.org/10.1145/2582051.2582077",
    "citations": 4,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "26",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582078",
    "title": "Carrier pigeon-like sensing system: animal-computer interface design for opportunistic data exchange interaction for a wildlife monitoring application",
    "authors": [
      "Keijiro Nakagawa",
      "Hiroki Kobayashi",
      "Kaoru Sezaki"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Carrier pigeon-like sensing system is a future--present archetype in human interface that will enable the humans to observe inaccessible and contaminated forests such as around Fukushima nuclear power plant. The system employs wildlife-borne sensing devices, which have Animal- Touch'n Go (ATG) and animal-to-animal Internet sharing capability, and can be used to expand the size of monitoring areas where the electricity supply and information infrastructure is either limited or nonexistent. Thus, monitoring information can be collected from remote areas cost-effectively and safely. The system is based on the concept of human--computer--biosphere interaction. This paper presents an overview of the concept, the methods employed, and the work in progress.",
    "keywords": [
      "animal behaviour",
      "delay-tolerant networking",
      "human-computer-biosphere interaction (HCBI)",
      "monitoring"
    ],
    "doi": "10.1145/2582051.2582078",
    "url": "https://doi.org/10.1145/2582051.2582078",
    "citations": 8,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "27",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582079",
    "title": "An interface for unconscious learning using mismatch negativity neurofeedback",
    "authors": [
      "Ming Chang",
      "Hiroyuki Iizuka",
      "Yasushi Naruse",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "There are a lot of skills that it takes time for us to learn in our life. To be precise, it is not clear what and how to learn. For example, one of the biggest problems in the language learning is that learners cannot recognize novel sounds that do not exist in their native language, and it is difficult to gain a listening ability for these novel sounds [1]. Here, we developed a novel neurofeedback (NF) method, using the mismatch negativity (MMN) responses elicited by similar sounds, that can help people to unconsciously improve their auditory perceptual skills. In our method, the strength of the participants' MMN as a measure of perceptual discriminability is presented as visual feedback to provide a continuous, not binary, cue for learning. We found evidence that significant performance improvement for behavioral auditory discrimination and neurophysiological measure occurs unconsciously. Based on our findings, the method has great potential to provide effortless auditory perceptual training and develop an unconscious learning interface device.",
    "keywords": [
      "interface",
      "neurofeedback",
      "unconscious learning"
    ],
    "doi": "10.1145/2582051.2582079",
    "url": "https://doi.org/10.1145/2582051.2582079",
    "citations": 4,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "28",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582081",
    "title": "BITAIKA: development of self posture adjustment system",
    "authors": [
      "Haruna Ishimatsu",
      "Ryoko Ueoka"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We define \"BITAIKA\" as a self posture adjustment system while sitting for correcting its posture. As a first step of developing BITAIKA system, we developed a system visually inducing a user to correct its posture. BITAIKA monitors one's posture to find a continuous bad posture using kinect and multiple piezoelectric sensors. When a bad posture continues, a window to assist for adjusting a posture pops up on PC monitor. We conducted the prototype experiment to evaluate the effectiveness of the system while PC work and confirmed that BITAIKA will effectively work as a posture adjustment system.",
    "keywords": [
      "kinect and piezoelectric sensors",
      "posture recognition",
      "self posture adjustment interface"
    ],
    "doi": "10.1145/2582051.2582081",
    "url": "https://doi.org/10.1145/2582051.2582081",
    "citations": 22,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "30",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582082",
    "title": "Toward practical implementation of emotion driven digital camera using EEG",
    "authors": [
      "Tomomi Takashina",
      "Miyuki Yanagi",
      "Yoshiyuki Yamariku",
      "Yoshikazu Hirayama",
      "Ryota Horie",
      "Michiko Ohkura"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Photography is closely tied with people's emotion. Therefore, the concept of emotion driven camera is a natural consequence of future photography. As for EEG emotion detection in research laboratories, there have been many works but it is considered difficult to apply such methodologies in generic environments. For measuring ERP (event related potential), some kinds of cues are typically used for knowing the exact time in which a stimulus given, but it is difficult to know such cues in real world. Therefore, we propose a cue detection mechanism based on the architecture of digital single lens reflex camera. It enables to reproduce an environment similar to research laboratories and is still a natural configuration for ordinary digital cameras.",
    "keywords": [
      "affective computing",
      "brain machine interface",
      "camera",
      "kawaii feeling"
    ],
    "doi": "10.1145/2582051.2582082",
    "url": "https://doi.org/10.1145/2582051.2582082",
    "citations": 8,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "31",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582083",
    "title": "Haven't we met before? a realistic memory assistance system to remind you of the person in front of you",
    "authors": [
      "Masakazu Iwamura",
      "Kai Kunze",
      "Yuya Kato",
      "Yuzuko Utsumi",
      "Koichi Kise"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper presents a perceived real-time system for memory augmentation. We propose a realistic approach to realize a memory assistance system, focusing on retrieving the person in front of you. The proposed system is capable of fully automatic indexing and is scalable in the database size. We utilize face recognition to show the user previous encounters with the person they are currently looking at. The system works fast (under 200 ms, perceived real time) with a decent database size (45 videos of 24 people). We also provide evidence in terms of an online questionnaire that our proposed memory augmentation system is useful and would be worn by most of the participants if it can be implemented in an unobtrusive way.",
    "keywords": [
      "face recognition",
      "memory assistance system",
      "memory augmentation",
      "wearable"
    ],
    "doi": "10.1145/2582051.2582083",
    "url": "https://doi.org/10.1145/2582051.2582083",
    "citations": 20,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "32",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582085",
    "title": "Development of tactile biofeedback system for amplifying horror experience",
    "authors": [
      "Kouya Ishigaki",
      "Ryoko Ueoka"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Adding physical effect to a 3D film is called 4D. This attraction system becomes a common entertainment system which generates more realistic sensation. In our laboratory, previous study concludes changing the viewing environment amplified horror experience of the viewers. Developing a further horror amplifying system we focus attention on biofeedback. In this paper, our tactile biofeedback system's prototype is described and preliminary experiment is conducted to evaluate the effect of feedback of heart rate and pseudo feedback of heart rate whether the tactile feedback causes entrainment of subjects' heart rate.",
    "keywords": [
      "emotional display",
      "pseudo biofeedback",
      "tactile biofeedback"
    ],
    "doi": "10.1145/2582051.2582085",
    "url": "https://doi.org/10.1145/2582051.2582085",
    "citations": 1,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "34",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582086",
    "title": "A fault diagnostic system by line status monitoring for ubiquitous computers connected with multiple communication lines",
    "authors": [
      "Shintaro Kawabata",
      "Shoji Sano",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In ubiquitous computing environments, many computers should be controlled in cooperation to support human daily-life. We pick up a dance performance that performers wear a large amount of LEDs to combine body expression and lighting effects. In such situation, there is a problem that failure frequently occurs caused by dance movements. In the environment involving hundreds of computers, it costs much time and effort to check where failures have occurred. To solve this problem, we propose a system for identifying the position of the failure easily by checking the status of communication among computers. In addition, we implemented a fault diagnostic system that can provide a series of flows for detecting the failure, guessing why the failure occurred and taking a measure. Furthermore, we actually used our system into LED dance performances, conducted an evaluative experiment, and confirmed its usefulness.",
    "keywords": [
      "monitor line status",
      "ubiquitous computing"
    ],
    "doi": "10.1145/2582051.2582086",
    "url": "https://doi.org/10.1145/2582051.2582086",
    "citations": 0,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "35",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582087",
    "title": "User-centered design of a lamp customization tool",
    "authors": [
      "Monica Perusquía-Hernández",
      "Hella Kriening",
      "Carina Palumbo",
      "Barbara Wajda"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Unique self-designed products are currently in great demand. The customization process of these products requires a good understanding of the customer's needs as well as tools that allow them to make the right choices. We provide a solution that enables users to design lamps that fit their needs and the interior design of their home. Our proposal is an Augmented Reality (AR) tablet application that allows customization in context. The solution was low-and high-fidelity prototyped in several iterations. Users enjoyed the customization process and expressed satisfaction that the app would enable them to create a lamp that is personalized and unique.",
    "keywords": [
      "augmented reality",
      "customization",
      "rapid prototyping"
    ],
    "doi": "10.1145/2582051.2582087",
    "url": "https://doi.org/10.1145/2582051.2582087",
    "citations": 4,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "36",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582088",
    "title": "Fall prevention using head velocity extracted from visual based VDO sequences",
    "authors": [
      "Nuth Otanasap",
      "Poonpong Boonbrahm"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "More than ten millions elderly people fall each year. Falls are the important cause of injury related to death and set of symptoms. Most of \"fall detection\" systems are focused on critical and post-fall phase which mean that the faller may already be injured. In this study, we propose an early fall detection in critical fall phase using velocity characteristics, collected by Kinect sensor with 30 frames per second. A series of normal and falling activities were performed by 5 volunteers in first experiment and 11 volunteers in second experiment. The fall velocity based point was calculated by the first experiment as 50 postures, 2210 frames recorded.The result from the first experiment, velocity ratio 90.35 pixels per millisecond was calculated by adding μ with σ, was defined as velocity fall detection based point for the second experiment. In the second experiment, the fall activities were detected at 85.07 percent from 134 fall activities. The mean time of fall activities that was detected before dash to the ground is 391.15 milliseconds. The detected mean time may be useful to developing a preventive fall system to protect the faller before injured i.e. wearable airbag system. For high accuracy prediction, automatically adjusted vertical fall based point by train dataset of individual person will be investigated in future work.",
    "keywords": [
      "elderly fall",
      "fall-down detection",
      "kinect",
      "velocity"
    ],
    "doi": "10.1145/2582051.2582088",
    "url": "https://doi.org/10.1145/2582051.2582088",
    "citations": 4,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "37",
    "numpages": "2",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582089",
    "title": "Using smart phone mobility traces for the diagnosis of depressive and manic episodes in bipolar patients",
    "authors": [
      "Agnes Gruenerbl",
      "Venet Osmani",
      "Gernot Bahle",
      "Jose C. Carrasco",
      "Stefan Oehler",
      "Oscar Mayora",
      "Christian Haring",
      "Paul Lukowicz"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this paper we demonstrate how smart phone sensors, specifically inertial sensors and GPS traces, can be used as an objective \"measurement device\" for aiding psychiatric diagnosis. In a trial with 12 bipolar disorder patients conducted over a total (summed over all patients) of over 1000 days (on average 12 weeks per patient) we have achieved state change detection with a precision/recall of 96%/94% and state recognition accuracy of 80%. The paper describes the data collection, which was conducted as a medical trial in a real life every day environment in a rural area, outlines the recognition methods, and discusses the results.",
    "keywords": [
      "bipolar disorder",
      "real-life study",
      "smart phone",
      "state change detection",
      "state recognition"
    ],
    "doi": "10.1145/2582051.2582089",
    "url": "https://doi.org/10.1145/2582051.2582089",
    "citations": 146,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "38",
    "numpages": "8",
    "influentialCitations": 11
  },
  {
    "id": "10.1145/2582051.2582090",
    "title": "Anywhere surface touch: utilizing any surface as an input area",
    "authors": [
      "Takehiro Niikura",
      "Yoshihiro Watanabe",
      "Masatoshi Ishikawa"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "The current trend towards smaller and smaller mobile devices may cause considerable difficulties in using them. In this paper, we propose an interface called Anywhere Surface Touch, which allows any flat or curved surface in a real environment to be used as an input area. The interface uses only a single small camera and a contact microphone to recognize several kinds of interaction between the fingers of the user and the surface. The system recognizes which fingers are interacting and in which direction the fingers are moving. Additionally, the fusion of vision and sound allows the system to distinguish the contact conditions between the fingers and the surface. Evaluation experiments showed that users became accustomed to our system quickly, soon being able to perform input operations on various surfaces.",
    "keywords": [
      "high-speed camera",
      "interaction techniques",
      "mobile devices",
      "touch interface",
      "vision-based UI"
    ],
    "doi": "10.1145/2582051.2582090",
    "url": "https://doi.org/10.1145/2582051.2582090",
    "citations": 21,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "39",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582091",
    "title": "Augmenting expressivity of artificial subtle expressions (ASEs): preliminary design guideline for ASEs",
    "authors": [
      "Takanori Komatsu",
      "Kazuki Kobayashi",
      "Seiji Yamada",
      "Kotaro Funakoshi",
      "Mikio Nakano"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Unfortunately, there is little hope that information-providing systems will ever be perfectly reliable. The results of some studies have indicated that imperfect systems can reduce the users' cognitive load in interacting with them by expressing their level of confidence to users. Artificial subtle expressions (ASEs), which are machine-like artificial sounds to express the confidence information to users added just after the system's suggestions, were keenly focused on because of their simplicity and efficiency. The purpose of the work reported here was to develop a preliminary design guideline for ASEs in order to determine the expandability of ASEs. We believe that augmenting the expressivity of ASEs would lead reducing the users' cognitive load for processing the information provided from the systems, and this would also lead augmenting users' various cognitive capacities. Our experimental results showed that ASEs with decreasing pitch conveyed a low confidence level to users. This result were used to formulate a concrete design guideline for ASEs.",
    "keywords": [
      "artificial subtle expressions (ASEs)",
      "confidence",
      "design guideline",
      "inflection pattern"
    ],
    "doi": "10.1145/2582051.2582091",
    "url": "https://doi.org/10.1145/2582051.2582091",
    "citations": 2,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "40",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582092",
    "title": "Illusion cup: interactive controlling of beverage consumption based on an illusion of volume perception",
    "authors": [
      "Eiji Suzuki",
      "Takuji Narumi",
      "Sho Sakurai",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper proposes a system and an interaction design for implicitly influencing the satisfaction we experience while drinking a beverage and for controlling beverage consumption by creating a volume perception illusion using augmented reality. Recent studies have revealed consumption of food and beverage is influenced by both its actual volume and external factors during eating/drinking. We focus on the fact that the shape of the beverage container influences beverage consumption. Based on this fact, we constructed a system that changes the apparent size of the cup. We investigated how the beverage consumption would change by using the proposed system. The results showed subjects consumed significantly greater amounts when they drank from a visually lengthened cup and consumed significantly smaller amounts when they drank from a visually shortened cup. This technique can be used for daily health-care applications with wearable computers.",
    "keywords": [
      "augmented reality",
      "cross-modality",
      "drink consumption",
      "health",
      "human food interaction"
    ],
    "doi": "10.1145/2582051.2582092",
    "url": "https://doi.org/10.1145/2582051.2582092",
    "citations": 18,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "41",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582093",
    "title": "Towards emotional regulation through neurofeedback",
    "authors": [
      "Marc Cavazza",
      "Fred Charles",
      "Gabor Aranyi",
      "Julie Porteous",
      "Stephen W. Gilroy",
      "Gal Raz",
      "Nimrod Jakob Keynan",
      "Avihay Cohen",
      "Gilan Jackont",
      "Yael Jacob",
      "Eyal Soreq",
      "Ilana Klovatch",
      "Talma Hendler"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This paper discusses the potential of Brain-Computer Interfaces based on neurofeedback methods to support emotional control and pursue the goal of emotional control as a mechanism for human augmentation in specific contexts. We illustrate this discussion through two proof-of-concept, fully-implemented experiments: one controlling disposition towards virtual characters using pre-frontal alpha asymmetry, and the other aimed at controlling arousal through activity of the amygdala. In the first instance, these systems are intended to explore augmentation technologies that would be incorporated into various media-based systems rather than permanently affect user behaviour.",
    "keywords": [
      "affective computing",
      "brain-computer interfaces",
      "neurofeedback",
      "virtual reality"
    ],
    "doi": "10.1145/2582051.2582093",
    "url": "https://doi.org/10.1145/2582051.2582093",
    "citations": 24,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "42",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582094",
    "title": "Around me: a system with an escort robot providing a sports player's self-images",
    "authors": [
      "Junya Tominaga",
      "Kensaku Kawauchi",
      "Jun Rekimoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Providing self-images is an effective approach to identifying sports players' body movements that should be corrected. Traditional means providing self-images, however, such as mirrors and videos, are not effective in terms of mobility and immediacy. In this paper we propose a system, Around Me, providing self-images through a display attached to an escort robot that runs in front of the user. This system captures the user's posture from the front and recognizes his/her position relative to the robot. The user's movements are synchronized with the robot's movements because the robot's movements are determined by the user's location. In this research we developed an experimental prototype specialized for assistance in jogging. In pilot studies we observed that the ability of Around Me to provide real-time images is potentially able to encourage the user to improve his/her jogging form, which is essential for performance and for injury prevention. In addition, compared with the robot running in front of the user with one following behind the user, we clarified the frontal robot's characteristics: the robot can control the jogging speed, and the user needs to adjust the robot's steering and the distance between the robot and him/her as he/she requires. Then we found indications that Around Me can, with various jogging support functions, encourage the user to practice jogging with ideal form.",
    "keywords": [
      "escort robot",
      "jogging",
      "self-images",
      "sports assistance"
    ],
    "doi": "10.1145/2582051.2582094",
    "url": "https://doi.org/10.1145/2582051.2582094",
    "citations": 28,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "43",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582095",
    "title": "Boundary conditions for information visualization with respect to the user's gaze",
    "authors": [
      "Marcus Tönnis",
      "Gudrun Klinker"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Gaze tracking in Augmented Reality is mainly used to trigger buttons and access information. Such selectable objects are usually placed in the world or in screen coordinates of a head- or hand-mounted display. Yet, no work has investigated options to place information with respect to the line of sight.This work presents our first steps towards gaze-mounted information visualization and interaction, determining boundary conditions for such an approach. We propose a general concept for information presentation at an angular offset to the line of sight. A user can look around freely, yet having information attached nearby the line of sight. Whenever the user wants to look at the information and does so, the information is placed directly at the axis of sight for a short time.Based on this concept we investigate how users understand frames of reference, specifically, if users relate directions and alignments in head or world coordinates. We further investigate if information may have a preferred motion behavior. Prototypical implementations of three variants are presented to users in guided interviews. The three variants resemble a rigid offset and two different floating motion behaviors of the information. Floating algorithms implement an inertia based model and either allow the user's gaze to surpass the information or to push information with the gaze. Testing our proto-types yielded findings that users strongly prefer information maintaining world-relation and that less extra motion is preferred.",
    "keywords": [
      "augmented reality",
      "gaze mounting",
      "gaze tracking",
      "information presentation",
      "virtual reality"
    ],
    "doi": "10.1145/2582051.2582095",
    "url": "https://doi.org/10.1145/2582051.2582095",
    "citations": 12,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "44",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582096",
    "title": "What's on your mind? mental task awareness using single electrode brain computer interfaces",
    "authors": [
      "Alireza Sahami Shirazi",
      "Mariam Hassib",
      "Niels Henze",
      "Albrecht Schmidt",
      "Kai Kunze"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Recognizing and summarizing persons' activities have proven to be effective for increasing self-awareness and enable to improve habits. Reading improves one's language skills and periodic relaxing improves one's health. Recognizing these activities and conveying the time spent would enable to ensure that users read and relax for an adequate time. Most previous attempts in activity recognition deduce mental activities by requiring expensive/bulky hardware or by monitoring behavior from the outside. Not all mental activities can, however, be recognized from the outside. If a person is sleeping, relaxing, or intensively thinks about a problem can hardly be differentiated by observing carried-out reactions. In contrast, we use simple wearable off-the-shelf single electrode brain computer interfaces. These devices have the potential to directly recognize user's mental activities. Through a study with 20 participants, we collect data for five representative activities. We describe the dataset collected and derive potential features. Using a Bayesian classifier we show that reading and relaxing can be recognized with 97% and 79% accuracy. We discuss how sensory tasks associated with different brain lobes can be classified using a single dry electrode BCI.",
    "keywords": [
      "BCI",
      "EEG",
      "general knowledge",
      "reading",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582096",
    "url": "https://doi.org/10.1145/2582051.2582096",
    "citations": 22,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "45",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582097",
    "title": "JackIn: integrating first-person view with out-of-body vision generation for human-human augmentation",
    "authors": [
      "Shunichi Kasahara",
      "Jun Rekimoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "JackIn is a new human-human communication framework for connecting two or more people. With first-person view video streaming from a person (called Body) wearing a transparent head-mounted display and a head-mounted camera, the other person (called Ghost) participates in the shared first-person view. With JackIn, people's activities can be shared and assistance or guidance can be given through other peoples expertise. This can be applied to daily activities such as cooking lessons, shopping navigation, education in craft-work or electrical work, and sharing experiences of sporting and live events. For a better viewing experience with frist-person view, we developed the out-of-body view in which first-person images are integrated to construct a scene around a Body, and a Ghost can virtually control the viewpoint to look around the space surrounding the Body. We also developed a tele-pointing gesture interface. We conducted an experiment to evaluate how effective this framework is and found that Ghosts can understand the spatial situation of the Body.",
    "keywords": [
      "augmented human",
      "augmented reality",
      "first person view streaming",
      "out-of-body"
    ],
    "doi": "10.1145/2582051.2582097",
    "url": "https://doi.org/10.1145/2582051.2582097",
    "citations": 123,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "46",
    "numpages": "8",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/2582051.2582098",
    "title": "An information presentation method for head mounted display considering surrounding environments",
    "authors": [
      "Masayuki Nakao",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "When wearing a head mounted display (HMD), the degree of concentration on the HMD varies depending on the surrounding environment. In this work, we developed an information presentation method considering cognitive cost and safety in wearable computing environments. The proposed method changes its information presentation method based on possible gazing time that varies in according with the surrounding environment and user context. We used an eye tracker to measure the relationship between eyestrain and watching an HMD and also clarified the relationship between gaze time and surrounding environment. We then used the results to develop an algorithm to change the information presentation method. Evaluation results revealed cases in which it was difficult and dangerous to gaze at an HMD and therefore necessary to change the information presentation.",
    "keywords": [
      "eyestrain",
      "gaze tracking",
      "head mounted display",
      "information presentation",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582098",
    "url": "https://doi.org/10.1145/2582051.2582098",
    "citations": 15,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "47",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582099",
    "title": "Let me grab this: a comparison of EMS and vibration for haptic feedback in free-hand interaction",
    "authors": [
      "Max Pfeiffer",
      "Stefan Schneegass",
      "Florian Alt",
      "Michael Rohs"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Free-hand interaction with large displays is getting more common, for example in public settings and exertion games. Adding haptic feedback offers the potential for more realistic and immersive experiences. While vibrotactile feedback is well known, electrical muscle stimulation (EMS) has not yet been explored in free-hand interaction with large displays. EMS offers a wide range of different strengths and qualities of haptic feedback. In this paper we first systematically investigate the design space for haptic feedback. Second, we experimentally explore differences between strengths of EMS and vibrotactile feedback. Third, based on the results, we evaluate EMS and vibrotactile feedback with regard to different virtual objects (soft, hard) and interaction with different gestures (touch, grasp, punch) in front of a large display. The results provide a basis for the design of haptic feedback that is appropriate for the given type of interaction and the material.",
    "keywords": [
      "electrical muscle stimulation",
      "free-hand interaction",
      "haptic feedback",
      "large displays",
      "tactile feedback"
    ],
    "doi": "10.1145/2582051.2582099",
    "url": "https://doi.org/10.1145/2582051.2582099",
    "citations": 83,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "48",
    "numpages": "8",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/2582051.2582100",
    "title": "SpiderVision: extending the human field of view for augmented awareness",
    "authors": [
      "Kevin Fan",
      "Jochen Huber",
      "Suranga Nanayakkara",
      "Masahiko Inami"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We present SpiderVision, a wearable device that extends the human field of view to augment a user's awareness of things happening behind one's back. SpiderVision leverages a front and back camera to enable users to focus on the front view while employing intelligent interface techniques to cue the user about activity in the back view. The extended back view is only blended in when the scene captured by the back camera is analyzed to be dynamically changing, e.g. due to object movement. We explore factors that affect the blended extension, such as view abstraction and blending area. We contribute results of a user study that explore 1) whether users can perceive the extended field of view effectively, and 2) whether the extended field of view is considered a distraction. Quantitative analysis of the users' performance and qualitative observations of how users perceive the visual augmentation are described.",
    "keywords": [
      "augmented awareness",
      "field of view extension",
      "human behavior",
      "visual augmentation",
      "visual blending"
    ],
    "doi": "10.1145/2582051.2582100",
    "url": "https://doi.org/10.1145/2582051.2582100",
    "citations": 66,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "49",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2582051.2582101",
    "title": "TAMA: development of trajectory changeable ball for future entertainment",
    "authors": [
      "Tomoya Ohta",
      "Shumpei Yamakawa",
      "Takashi Ichikawa",
      "Takuya Nojima"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In this paper, we propose a ball interface \"TAMA\" (Trajectory chAnging, Motion bAll; \"tama\" means a ball in Japanese) that can change its own trajectory dynamically. Conventionally, it is impossible to go against the laws of physics. However, if the trajectory of the balls can be changed during flight, that virtually means balls can fly against physical laws. This should enhance the pleasure of ball-related games such as baseball, basketball, juggling, etc. In this research, we used the force of compressed gas from within the ball itself to change the ball trajectory. Previously, we developed the ball prototype equipped with a gas-jet unit. However, the primal prototype was too heavy to use in amusement. Additionally, there was no control of the timing of the jet and it was wired for power supply. In this paper, we introduce the latest prototype of TAMA, which trims the weight and adds new functionality. We discuss the feasibility of this system through experimentation in changing the ball's trajectory during downward flight.",
    "keywords": [
      "ball",
      "entertainment",
      "jet pressure",
      "motion sensing",
      "sports"
    ],
    "doi": "10.1145/2582051.2582101",
    "url": "https://doi.org/10.1145/2582051.2582101",
    "citations": 12,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "50",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582102",
    "title": "Implementation and evaluation on a concealed interface using abdominal circumference",
    "authors": [
      "Hirotaka Sumitomo",
      "Takuya Katayama",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "The downsizing of computers enables users to operate computers anywhere. Generally, since a user operates his/her computer by his/her hands, computer operation is unconcealed to surrounding people. On the other hand, there are demands for hidden operation of the computer in situations such as face to face communication and during a meeting. The operation of computer in those scenes often gives a bad impression to surroundings and discommunication. Therefore, in this study, we propose an interface using a user's abdominal circumference as input. The motion of the abdomen is hard to be recognized by surroundings, and a user can move one's abdomen independently of the other body parts. We implemented a prototype of input method using the moving velocity and the absolute size of abdomen as input. Then, we evaluated the granularity, reproducibility, parallelism, resistance to environment, confidentiality and resistance to mis-recognition on the proposed interface.",
    "keywords": [
      "abdominal circumference",
      "input interface",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582102",
    "url": "https://doi.org/10.1145/2582051.2582102",
    "citations": 6,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "51",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582103",
    "title": "Pseudo-transparent tablet based on 3D feature tracking",
    "authors": [
      "Makoto Tomioka",
      "Sei Ikeda",
      "Kosuke Sato"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "This demonstration shows geometrically consistent image rendering that realizes pseudo-transparency in tablet-based augmented reality. The rendering method is based on the homography estimated by feature tracking and face detection using on-board rear and front cameras, respectively. This configuration is the most practical for typical off-the-shelf tablets in the sense that it does not require any special devices or designed environments. Although the local misalignment of images rectified by homography is an unavoidable artifact in a non-planar scene, the rendered images are globally consistent with the real scene. This is the first demonstration in which such pseudo-transparency can be experienced in an unprepared environment.",
    "keywords": [
      "augmented reality",
      "geometric consistency",
      "pseudo-transparency",
      "tablet-based AR",
      "video see-through display"
    ],
    "doi": "10.1145/2582051.2582103",
    "url": "https://doi.org/10.1145/2582051.2582103",
    "citations": 6,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "52",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582104",
    "title": "KinecDrone: enhancing somatic sensation to fly in the sky with Kinect and AR.Drone",
    "authors": [
      "Kohki Ikeuchi",
      "Tomoaki Otsuka",
      "Akihito Yoshii",
      "Mizuki Sakamoto",
      "Tatsuo Nakajima"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "KinecDrone enhances our somatic sensation to fly in the sky with Kinect and AR.Drone. A Video stream captured in AR.Drone is transmited to a user's head mounted display. While a user behaves like flying in the sky in a room, he/she can watch the scene captured by flying AR.Drone. Thus, the user feels that he/she is really flying in the sky. Also, the user can control AR.Drone with his/her natural gestures without losing the reality of the feeling that he/she is flying in the sky as if he/she becomes AR.Drone himself/herself. This significantly increases the immersive experiences as flying in the sky.",
    "keywords": [
      "ar.drone",
      "enhancing somatic sensation",
      "kinect"
    ],
    "doi": "10.1145/2582051.2582104",
    "url": "https://doi.org/10.1145/2582051.2582104",
    "citations": 36,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "53",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582105",
    "title": "Early gesture recognition method with an accelerometer",
    "authors": [
      "Ryo Izuta",
      "Kazuya Murao",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "An accelerometer is installed in most current mobile phones, such as the iPhones, Android-powered devices, and video game controllers for Wii or PS3, which enable easy and intuitive operations such as scrolling browsers and drawing 3D objects by detecting the inclination and motion of devices. Therefore, many gesture-based user interfaces with accelerometers are expected to appear in the future. Gesture recognition systems with accelerometers generally have to construct gesture models with user's gesture data before use, and recognize unknown gestures by comparing them with training data. As recognition process generally starts after the gesture has finished, output of the recognition result and feedback, e.g. scrolling, have a delay, which may cause users to retry gestures and degrade interface usability. We propose a method of early gesture recognition that calculates the distance between input data and training data sequentially, and outputs recognition results only when one output candidate has a stronger likelihood than the others. Additionally, we implemented a gesture-based photo viewer as an example of useful applications of our proposed method.",
    "keywords": [
      "accelerometer",
      "early recognition",
      "gesture recognition"
    ],
    "doi": "10.1145/2582051.2582105",
    "url": "https://doi.org/10.1145/2582051.2582105",
    "citations": 14,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "54",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582106",
    "title": "A system for practicing formations in dance performance using a two-axis movable electric curtain track",
    "authors": [
      "Shuhei Tsuchida",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Improving physical expressions and the sense of rhythm in dance performances has become important in recent years due to the increase in child dancers and dance studios. Even beginners in dance gain more opportunities to perform dances in groups. When dancing in a group, collapsed formation will greatly reduce the quality of dance performance even if the choreography is synchronized with the music. Therefore, learning the dance formation in a group is as important as learning its choreography. It is also important to be aware of keeping the proper formation and moving smoothly into the next formation to perform professional level group dances. However, it is difficult to obtain the sense of a proper formation if some members of the dance cannot participate in the practice. We have proposed a practice-support system for performing the formation smoothly using a self-propelled screen even if there is no dance partner. However, the movement of people was limited more than necessary by the excessive presence of a self-propelled screen moving irregular and the fear of the collision with the screen. Therefore, the reproducibility of the trajectory in the case where the user danced with another dancer was low. In this work, we propose a practice-support system for performing the formation using the two-axis movable curtain rail, whose movement direction does not drift and the material used for projection is soft. These characteristics reduce the fear of the collision and improve the accuracy in movement of the screen.",
    "keywords": [
      "electric curtain track",
      "formation dance training",
      "projector"
    ],
    "doi": "10.1145/2582051.2582106",
    "url": "https://doi.org/10.1145/2582051.2582106",
    "citations": 6,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "55",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582107",
    "title": "iMake: computer-aided eye makeup",
    "authors": [
      "Ayano Nishimura",
      "Itiro Siio"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "Many women enjoy applying makeup. Eye makeup is especially important for face makeup, because eyeshadow color and eye line shape can dramatically change a person's impression given to others. In addition to standard eye makeup, there is \"artistic eye makeup,\" which tends to have a greater variety of designs and is more ostentatious than standard eye makeup. Artistic eye makeup often has a motif of characters or symbols, such as a butterfly or a musical note. Needless to say, it is often difficult for non-artistic people to apply this type of eye makeup. Artistic eye makeup requires a special technique; therefore, we propose and implement a computer-aided eye makeup design system called \"iMake.\" This system generates eye makeup designs from the colors and shapes of a favorite characters selected by a user. Once the user has selected the desired eye makeup pattern, an ink-jet color printer prints it on a transfer sheet that the user can apply to his/her eyelids. The user can design any type of eye makeup with a simple operation, and then apply the transfer sheet makeup without any special techniques.",
    "keywords": [
      "augmented fashion",
      "eye makeup",
      "image processing",
      "transfer sheet"
    ],
    "doi": "10.1145/2582051.2582107",
    "url": "https://doi.org/10.1145/2582051.2582107",
    "citations": 2,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "56",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582108",
    "title": "An interactive system for recognizing user actions on a surface using accelerometers",
    "authors": [
      "Naoya Isoyama",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "There are various approaches to recognizing user actions for interactive arts. By making a system interactive, the audience has more fun because they are participating, and the artists can translate what is in their imagination more richly. Since user actions have great variety and the restrictions on installations are diverse, conventional systems use mechanisms for recognizing user motions that are specialized to their own work, i.e., that are not general. In this paper, we propose a method that adds interactivity to any surface and recognizes the position and intensity of a preformed action by using multiple accelerometers.",
    "keywords": [
      "accelerometers",
      "interactive system",
      "media art",
      "recognition method",
      "surface"
    ],
    "doi": "10.1145/2582051.2582108",
    "url": "https://doi.org/10.1145/2582051.2582108",
    "citations": 2,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "57",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2582051.2582109",
    "title": "A multi-modal interface for performers in stuffed suits",
    "authors": [
      "Yoshiyuki Tei",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "In wearable computing environments, a user can obtain information safely and efficiently without disturbing their daily life. However, since the surrounding conditions change frequently according to the situation, the information presentation method of the wearable system needs to adapt to the change. The main purpose of our research is to construct a system that changes information presentation methods according to the user situation. We focus on supporting performers in stuffed suits with multi-modal information presentation. We investigated the interfaces for these performers who cannot acquire sufficient information of the surrounding environments.",
    "keywords": [
      "multi-modal interface",
      "stuffed suit",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582109",
    "url": "https://doi.org/10.1145/2582051.2582109",
    "citations": 0,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "58",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2582051.2582110",
    "title": "A sound-based lifelog system using ultrasound",
    "authors": [
      "Hiroki Watanabe",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2014,
    "conference": "AH",
    "conferenceYear": "AH '14",
    "abstract": "We propose an activity and context recognition method where the user carries a neck-worn receiver comprising a microphone, and small speakers on his wrists that generate ultrasounds. The system recognizes gestures on the basis of the volume of the received sound and the Doppler effect. The former indicates the distance between the neck and wrists, and the latter indicates the speed of motions. Thus, our approach substitutes the wired or wireless communication typically required in body area motion sensing networks by ultrasounds. Our system also recognizes the place where the user is in and the people who are near the user by ID signals generated from speakers placed in rooms and on people. The strength of the approach is that, for offline recognition, a simple audio recorder can be used for the receiver. In this paper, we introduce our new device.",
    "keywords": [
      "environment recognition",
      "gesture recognition",
      "location recognition",
      "person recognition",
      "ultrasonic",
      "wearable computing"
    ],
    "doi": "10.1145/2582051.2582110",
    "url": "https://doi.org/10.1145/2582051.2582110",
    "citations": 3,
    "booktitle": "Proceedings of the 5th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kobe, Japan",
    "articleno": "59",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735787",
    "title": "Vision enhancement: defocus correction via optical see-through head-mounted displays",
    "authors": [
      "Yuta Itoh",
      "Gudrun Klinker"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Vision is our primary, essential sense to perceive the real world. Human beings have been keen to enhance the limit of the eye function by inventing various vision devices such as corrective glasses, sunglasses, telescopes, and night vision goggles. Recently, Optical See-Through Head-Mounted Displays (OST-HMD) have penetrated in the commercial market. While the traditional devices have improved our vision by altering or replacing it, OST-HMDs can augment and mediate it. We believe that future OST-HMDs will dramatically improve our vision capability, combined with wearable sensing systems including image sensors.For taking a step toward this future, this paper investigates Vision Enhancement (VE) techniques via OST-HMDs. We aim at correcting optical defects of human eyes, especially defocus, by overlaying a compensation image on the user's actual view so that the filter cancels the aberration. Our contributions are threefold. Firstly, we formulate our method by taking the optical relationships between OST-HMD and human eye into consideration. Secondly, we demonstrate the method in proof-of-concept experiments. Lastly and most importantly, we provide a thorough analysis of the results including limitations of the current system, potential research issues necessary for realizing practical VE systems, and possible solutions for the issues for future research.",
    "keywords": [
      "vision enhancement",
      "optical see-through",
      "near-eye displays",
      "image deblurring",
      "head-mounted displays",
      "augmented reality"
    ],
    "doi": "10.1145/2735711.2735787",
    "url": "https://doi.org/10.1145/2735711.2735787",
    "citations": 31,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "1–8",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735831",
    "title": "Exploring users' attitudes towards social interaction assistance on Google Glass",
    "authors": [
      "Qianli Xu",
      "Michal Mukawa",
      "Liyuan Li",
      "Joo Hwee Lim",
      "Cheston Tan",
      "Shue Ching Chia",
      "Tian Gan",
      "Bappaditya Mandal"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Wearable vision brings about new opportunities for augmenting humans in social interactions. However, along with it comes privacy concerns and possible information overload. We explore users' needs and attitudes toward augmented interaction in face-to-face communications. In particular, we want to find out whether users need additional information when interacting with acquaintances, what information they want to access, and how they use it. Based on observations of user behaviors in interactions assisted by Google Glass, we find that users in general appreciated the usefulness of wearable assistance for social interactions. We highlight a few key issues of how wearable devices affect user experience in social interaction.",
    "keywords": [
      "wearable vision",
      "social interaction",
      "social context",
      "privacy",
      "Google Glass"
    ],
    "doi": "10.1145/2735711.2735831",
    "url": "https://doi.org/10.1145/2735711.2735831",
    "citations": 19,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "9–12",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735792",
    "title": "PickRing: seamless interaction through pick-up detection",
    "authors": [
      "Katrin Wolf",
      "Jonas Willaredt"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We are frequently switching between devices, and currently we have to unlock most of them. Ideally such devices should be seamlessly accessible and not require an unlock action. We introduce PickRing, a wearable sensor that allows seamless interaction with devices through predicting the intention to interact with them through the device's pick-up detection. A cross-correlation between the ring and the device's motion is used as basis for identifying the intention of device usage. In an experiment, we found that the pick-up detection using PickRing cost neither additional effort nor time when comparing it with the pure pick-up action, while it has more hedonic qualities and is rated to be more attractive than a standard smartphone technique. Thus, PickRing can reduce the overhead in using device through seamlessly activating mobile and ubiquitous computers.",
    "keywords": [
      "wearable",
      "ubiquitous",
      "seamless",
      "pick-up",
      "activation"
    ],
    "doi": "10.1145/2735711.2735792",
    "url": "https://doi.org/10.1145/2735711.2735792",
    "citations": 15,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "13–20",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735830",
    "title": "SkinWatch: skin gesture interaction for smart watch",
    "authors": [
      "Masa Ogata",
      "Michita Imai"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We propose SkinWatch, a new interaction modality for wearable devices. SkinWatch provides gesture input by sensing deformation of the skin under a wearable wrist device, also known as a smart watch. A gesture command that is matched by learning data and two-dimensional linear input recognizes the gestures. The sensing part is small, thin, and stable, to accept accurate input via a user's skin. We also implement an anti-error mechanism to prevent unexpected input when the user moves or rotates his or her forearm. The whole sensor costs less than $1.50 and the sensor layer does not exceed a height of more than 3 mm in this prototype. We demonstrate sample applications with a practical task; using two-finger skin gesture input.",
    "keywords": [
      "wearable computing",
      "smart watch",
      "skin gesture",
      "photo-reflectivity"
    ],
    "doi": "10.1145/2735711.2735830",
    "url": "https://doi.org/10.1145/2735711.2735830",
    "citations": 72,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "21–24",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735791",
    "title": "Improving work productivity by controlling the time rate displayed by the virtual clock",
    "authors": [
      "Yuki Ban",
      "Sho Sakurai",
      "Takuji Narumi",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "The main contribution of this paper is establishing the method for improving work productivity unconsciously by controlling the time rate that a virtual clock displays. Recently, it became clear that the work efficiency is influenced by various environmental factors. One of a way to increase work productivity is improving the work rate during certain duration. On the contrary, it is becoming clarified that the time pressure has the potential to enhance the task performance and the work productivity. The approximation of the work rate per certain time and this time pressure are evoked by the time sensation. In this study, we focus on a \"clock\" as a tool, which gives the recognition of time rate and length for everyone mutually. We propose a method to improve a person's work productivity unconsciously by giving an illusion of false sense of the passaged time by a virtual clock that displays the time rate that differ from real one visually. We conducted experiments to investigate the influence of the changes in the displayed virtual time rate on time perception and work efficiency. The experimental results showed that by displaying an the accelerated time rate, it is possible to improve work efficiency with constant time perception.",
    "keywords": [
      "work productivity",
      "work efficiency",
      "virtual time rate",
      "virtual clock",
      "time sensation",
      "time perception"
    ],
    "doi": "10.1145/2735711.2735791",
    "url": "https://doi.org/10.1145/2735711.2735791",
    "citations": 17,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "25–32",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735795",
    "title": "Gravitamine spice: a system that changes the perception of eating through virtual weight sensation",
    "authors": [
      "Masaharu Hirose",
      "Karin Iwazaki",
      "Kozue Nojiri",
      "Minato Takeda",
      "Yuta Sugiura",
      "Masahiko Inami"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "The flavor of food is not just limited to the sense of taste, but also it changes according to the perceived information from other perception such as the auditory, visual, tactile senses, or through individual experiences or cultural background, etc. We proposed \"Gravitamine Spice\", a system that focuses on the cross-modal interaction between our perception; mainly the weight of food we perceived when we carry the utensils. This system consists of a fork and a seasoning called the \"OMOMI\". User can change the weight of the food by sprinkling seasoning onto it. Through this sequence of actions, users can enjoy different dining experiences, which may change the taste of their food or the feeling towards the food when they are chewing it.",
    "keywords": [
      "virtual reality",
      "interactive system",
      "entertainment system",
      "cross-modal",
      "augmented reality"
    ],
    "doi": "10.1145/2735711.2735795",
    "url": "https://doi.org/10.1145/2735711.2735795",
    "citations": 29,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "33–40",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735825",
    "title": "DogPulse: augmenting the coordination of dog walking through an ambient awareness system at home",
    "authors": [
      "Christoffer Skovgaard",
      "Josephine Raun Thomsen",
      "Nervo Verdezoto",
      "Daniel Vestergaard"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper presents DogPulse, an ambient awareness system to support the coordination of dog walking among family members at home. DogPulse augments a dog collar and leash set to activate an ambient shape-changing lamp and visualize the last time the dog was taken for a walk. The lamp gradually changes its form and pulsates its lights in order to keep the family members aware of the dog walking activity. We report the iterative prototyping of DogPulse, its implementation and its preliminary evaluation. Based on our initial findings, we present the limitations and lessons learned as well as highlight recommendations for future work.",
    "keywords": [
      "shape-changing interface",
      "peripheral awareness",
      "dog walking",
      "coordination",
      "augmented artifacts",
      "ambient awareness"
    ],
    "doi": "10.1145/2735711.2735825",
    "url": "https://doi.org/10.1145/2735711.2735825",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "41–44",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735829",
    "title": "Snow walking: motion-limiting device that reproduces the experience of walking in deep snow",
    "authors": [
      "Tomohiro Yokota",
      "Motohiro Ohtake",
      "Yukihiro Nishimura",
      "Toshiya Yui",
      "Rico Uchikura",
      "Tomoko Hashida"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We propose \"Snow Walking,\" a boot-shaped device that reproduces the experience of walking in deep snow. The main purpose of this study is reproducing the feel of walking in a unique environment that we do not experience daily, particularly one that has depth, such as of deep snow. When you walk in deep snow, you get three feelings: the feel of pulling your foot up from the deep snow, the feel of putting your foot down into the deep snow, and the feel of your feet crunching across the bottom of deep snow. You cannot walk in deep snow easily, and with the system, you get a unique feeling not only on the sole of your foot but as if your entire foot is buried in the snow. We reproduce these feelings by using a slider, electromagnet, vibration speaker, a hook and loop fastener, and potato starch.",
    "keywords": [
      "tactile presentation",
      "electromagnet",
      "deep snow"
    ],
    "doi": "10.1145/2735711.2735829",
    "url": "https://doi.org/10.1145/2735711.2735829",
    "citations": 15,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "45–48",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735833",
    "title": "The kraftwork and the knittstruments: augmenting knitting with sound",
    "authors": [
      "Enrique Encinas",
      "Konstantia Koulidou",
      "Robb Mitchell"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper presents a novel example of technological augmentation of a craft practice. By translating the skilled, embodied knowledge of knitting practice into the language of sound, our study explores how audio augmentation of routinized motion patterns affects an individual's awareness of her bodily movements and alters conventional practice. Four different instruments (The Knittstruments: The ThereKnitt, The KnittHat, The Knittomic, and The KraftWork) were designed and tested in four different locations. This research entails cycles of data collection and analysis based on the action and grounded theory methods of noting, coding and memoing. Analysis of the data collected suggests substantial alterations in the knitters performance due to audio feedback at both an individual and group level and improvisation in the process of making. We argue that the usage of Knittstruments can have relevant consequences in the fields of interface design, wearable computing or artistic and musical creation in general and hope to provide a new inspiring venue for designers, artists and knitters to explore.",
    "keywords": [
      "sonification",
      "skilled practice",
      "knitting",
      "instrument",
      "improvisation",
      "embodied interaction",
      "craft"
    ],
    "doi": "10.1145/2735711.2735833",
    "url": "https://doi.org/10.1145/2735711.2735833",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "49–52",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735797",
    "title": "Augmenting spatial skills with semi-immersive interactive desktop displays: do immersion cues matter?",
    "authors": [
      "Erin Treacy Solovey",
      "Johanna Okerlund",
      "Cassie Hoef",
      "Jasmine Davis",
      "Orit Shaer"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "3D stereoscopic displays for desktop use show promise for augmenting users' spatial problem solving tasks. These displays have the capacity for different types of immersion cues including binocular parallax, motion parallax, proprioception, and haptics. Such cues can be powerful tools in increasing the realism of the virtual environment by making interactions in the virtual world more similar to interactions in the real non-digital world [21, 32]. However, little work has been done to understand the effects of such immersive cues on users' understanding of the virtual environment. We present a study in which users solve spatial puzzles with a 3D stereoscopic display under different immersive conditions while we measure their brain workload using fNIRS and ask them subjective workload questions. We conclude that 1) stereoscopic display leads to lower task completion time, lower physical effort, and lower frustration; 2) vibrotactile feedback results in increased perceived immersion and in higher cognitive workload; 3) increased immersion (which combines stereo vision with vibrotactile feedback) does not result in reduced cognitive workload.",
    "keywords": [
      "zSpace",
      "vibrotactile feedback",
      "stereoscopic displays",
      "haptic feedback",
      "fNIRS",
      "3-d displays"
    ],
    "doi": "10.1145/2735711.2735797",
    "url": "https://doi.org/10.1145/2735711.2735797",
    "citations": 6,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "53–60",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735790",
    "title": "RippleTouch: initial exploration of a wave resonant based full body haptic interface",
    "authors": [
      "Anusha Withana",
      "Shunsuke Koyama",
      "Daniel Saakes",
      "Kouta Minamizawa",
      "Masahiko Inami",
      "Suranga Nanayakkara"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We propose RippleTouch, a low resolution haptic interface that is capable of providing haptic stimulation to multiple areas of the body via a single point of contact actuator. Concept is based on the low frequency acoustic wave propagation properties of the human body. By stimulating the body with different amplitude modulated frequencies at a single contact point, we were able to dissipate the wave energy in a particular region of the body, creating a haptic stimulation without direct contact. The RippleTouch system was implemented on a regular chair, in which, four base range speakers were mounted underneath the seat and driven by a simple stereo audio interface. The system was evaluated to investigate the effect of frequency characteristics of the amplitude modulation system. Results demonstrate that we can effectively create haptic sensations at different parts of the body with a single contact point (i.e. chair surface). We believe RippleTouch concept would serve as a scalable solution for providing full-body haptic feedback in variety of situations including entertainment, communication, public spaces and vehicular applications.",
    "keywords": [
      "haptic interfaces",
      "full body haptics",
      "acoustic wave propagation"
    ],
    "doi": "10.1145/2735711.2735790",
    "url": "https://doi.org/10.1145/2735711.2735790",
    "citations": 7,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "61–68",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735793",
    "title": "Optimal design for individualised passive assistance",
    "authors": [
      "Robert Peter Matthew",
      "Victor Shia",
      "Masayoshi Tomizuka",
      "Ruzena Bajcsy"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Assistive devices are capable of restoring independence and function to people suffering from musculoskeletal impairments. Traditional assistive exoskeletons can be divided into active or passive devices depending on the method used to provide joint torques. The design of these devices often does not take into account the abilities of the individual leading to complex designs, joint misalignment and muscular atrophy due to over assistance at each joint.We present a novel framework for the design of passive assistive devices whereby the device provides the minimal amount of assistance required to maximise the space that they can reach. In doing so, we effectively remap their capable torque load over their workspace, exercising existing muscle while ensuring that key points in the workspace are reached. In this way we hope to reduce the risk of muscular atrophy while assisting with tasks.We implement two methods for finding the necessary passive device parameters, one looks at static loading conditions while the second simulates the system dynamics using level set methods. This allows us to determine the set of points that an individual can hold their arms stationary, the statically achievable workspace (SAW).We show the efficacy of these methods on a number of case studies which show that individuals with pronounced muscle weakness and asymmetric muscle weakness can have restored SAW restoring a range of motion.",
    "keywords": [
      "optimization-based design",
      "exoskeleton",
      "assistive devices"
    ],
    "doi": "10.1145/2735711.2735793",
    "url": "https://doi.org/10.1145/2735711.2735793",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "69–76",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735837",
    "title": "Design of a novel finger exoskeleton with a sliding six-bar joint mechanism",
    "authors": [
      "Mahasak Surakijboworn",
      "Witaya Wannasuphoprasit"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "The objective of the paper is to propose a novel design of a finger exoskeleton. The merit of the work is that the proposed mechanism is expected to eliminate interference and translational force on a finger. The design consists of 3 identical joint mechanisms which, for each, adopts a six-bar RCM as an equivalent revolute joint incorporating with 2 prismatic joints to form a close-chain structure with a finger joint. Cable and hose transmission is designed to reduce burden from prospective driving modules. As a result, the prototype coherently follows finger movement throughout full range of motion for every size of fingers. This prototype is a part of the research that will be used in hand rehabilitation.",
    "keywords": [
      "rehabilitation",
      "hand",
      "finger",
      "exoskeleton",
      "RCM"
    ],
    "doi": "10.1145/2735711.2735837",
    "url": "https://doi.org/10.1145/2735711.2735837",
    "citations": 4,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "77–80",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735788",
    "title": "A life log system that recognizes the objects in a pocket",
    "authors": [
      "Kota Shimozuru",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "A novel approach has been developed for recognizing objects in pockets and for recording the events related to the objects. Information on putting an object into or taking it out of a pocket is closely related to user contexts. For example, when a house key is taken out from a pocket, the owner of the key is likely just getting home. We implemented a objects-in-pocket recognition device, which has a pair of infrared sensors arranged in a matrix, and life log software to obtain the time stamp of events happening. We evaluated whether or not the system could deal with one of five objects (a smartphone, ticket, hand, key, and lip balm) using template matching. When one registered object (the smartphone, ticket, or key) was put in the pocket, our system recognized the object correctly 91% of the time on average. We also evaluated our system in one action scenario. With our system's time stamps, user could easily remember what he took on that day and when he used the items.",
    "keywords": [
      "wearable computing",
      "pockets",
      "life log",
      "context awareness"
    ],
    "doi": "10.1145/2735711.2735788",
    "url": "https://doi.org/10.1145/2735711.2735788",
    "citations": 4,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "81–88",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735823",
    "title": "VISTouch: dynamic three-dimensional connection between multiple mobile devices",
    "authors": [
      "Masasuke Yasumoto",
      "Takehiro Teraoka"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "It has become remarkably common recently for people to own multiple mobile devices, although it is still difficult to effectively use them in combination. In this study, we constructed a new system called VISTouch that achieves a new operational capability and increases user interest in mobile devices by enabling multiple devices to be used in combination dynamically and spatially. Using VISTouch, for example, to spatially connect a smart-phone to a horizontally positioned tablet that is displaying a map as viewed from above enables these devices to dynamically obtain the correct relative position. The smart-phone displays images viewed from its position, direction, and angle in real time as a window to show the virtual 3D space. We applied VISTouch to two applications that used detailed information of the relative position in real space between multiple devices. These applications showed the potential improvement in using multiple devices in combination.",
    "keywords": [
      "tangible interaction",
      "multi-display",
      "multi-device",
      "interface",
      "design"
    ],
    "doi": "10.1145/2735711.2735823",
    "url": "https://doi.org/10.1145/2735711.2735823",
    "citations": 4,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "89–92",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735824",
    "title": "LumoSpheres: real-time tracking of flying objects and image projection for a volumetric display",
    "authors": [
      "Hideki Koike",
      "Hiroaki Yamaguchi"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper proposes a method for real-time tracking of flying objects and image projection onto them for developing a particle-based volumetric 3D display. The first section describes the concept using high-speed cameras and projectors for a particle-based volumetric 3D display. Our solution suggests a prediction model with kinematic laws and uses Kalman Filters to address latency issues within the projector-camera system. We conducted experiments to show the accuracy of the image projection. We also present an application of our method in entertainment, Digital Juggling.",
    "keywords": [],
    "doi": "10.1145/2735711.2735824",
    "url": "https://doi.org/10.1145/2735711.2735824",
    "citations": 16,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "93–96",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/2735711.2735835",
    "title": "B-C-invisibility power: introducing optical camouflage based on mental activity in augmented reality",
    "authors": [
      "Jonathan Mercier-Ganady",
      "Maud Marchal",
      "Anatole Lécuyer"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper we introduce a novel and interactive approach for controlling optical camouflage called \"B-C-Invisibility power\". We propose to combine augmented reality and Brain-Computer Interface (BCI) technologies to design a system which somehow provides the \"power of becoming invisible\". Our optical camouflage is obtained on a PC monitor combined with an optical tracking system. A cut out image of the user is computed from a live video stream and superimposed to the prerecorded background image using a transparency effect. The transparency level is controlled by the output of a BCI, making the user able to control her invisibility directly with mental activity. The mental task required to increase/decrease the invisibility is related to a concentration/relaxation state. Results from a preliminary study based on a simple video-game inspired by the Harry Potter universe could notably show that, compared to a standard control made with a keyboard, controlling the optical camouflage directly with the BCI could enhance the user experience and the feeling of \"having a super-power\".",
    "keywords": [
      "optical camouflage",
      "invisibility",
      "BCI"
    ],
    "doi": "10.1145/2735711.2735835",
    "url": "https://doi.org/10.1145/2735711.2735835",
    "citations": 8,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "97–100",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735789",
    "title": "Word out! learning the alphabet through full body interactions",
    "authors": [
      "Kelly Yap",
      "Clement Zheng",
      "Angela Tay",
      "Ching-Chiuan Yen",
      "Ellen Yi-Luen Do"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper presents Word Out, an interactive game for learning of the alphabet through full body interaction. Targeted for children 4-7 years old, Word Out employs the Microsoft Kinect to detect the silhouette of players. Players are tasked to twist and form their bodies to match the shapes of the letters displayed on the screen. By adopting full body interactions in games, we aim to promote learning through play, as well as encourage collaboration and kinesthetic learning for children. Over two months, more than 15,000 children have played Word Out installed in two different museums. This paper presents the design and implementation of the Word Out game, preliminary analyses of a survey carried out at the museums to share insights and discusses future work.",
    "keywords": [
      "language learning",
      "kinesthetic learning",
      "kinect",
      "full body interaction",
      "embodied interaction",
      "education games",
      "education",
      "children"
    ],
    "doi": "10.1145/2735711.2735789",
    "url": "https://doi.org/10.1145/2735711.2735789",
    "citations": 32,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "101–108",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/2735711.2735827",
    "title": "Unconscious learning of speech sounds using mismatch negativity neurofeedback",
    "authors": [
      "Ming Chang",
      "Hiroyuki Iizuka",
      "Yasushi Naruse",
      "Hideyuki Ando",
      "Taro Maeda"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Learning the speech sounds of a foreign language is difficult for adults, and often requires significant training and attention. For example, native Japanese speakers are usually unable to differentiate between the \"l\" and \"r\" sounds in English; thus, words like \"light\" and \"right\" are hardly discriminated. We previously showed that the discrimination ability for similar pure tones can be improved unconsciously using neurofeedback (NF) training with mismatch negativity (MMN), but it is not clear whether it can improve discrimination of the speech sounds of words. We examined whether MMN Neurofeedback is effective in helping native Japanese speakers discriminate 'light' and 'right' in English. Participants seemed to unconsciously improve significantly in speech sound discrimination through NF training without attention to the auditory stimuli or awareness of what was to be learnt. Individual word sound recognition also improved significantly. Furthermore, our results indicate a lasting effect of NF training.",
    "keywords": [
      "unconscious learning",
      "neurofeedback",
      "language learning",
      "interface"
    ],
    "doi": "10.1145/2735711.2735827",
    "url": "https://doi.org/10.1145/2735711.2735827",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "109–112",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735828",
    "title": "Use of an intermediate face between a learner and a teacher in second language learning with shadowing",
    "authors": [
      "Yoko Nakanishi",
      "Yasuto Nakanishi"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Shadowing is a language-learning method whereby a learner attempts to repeat, i.e., shadow, what he/she hears immediately. We propose displaying a computer-generated intermediate face between a learner and a teacher as an appropriate intermediate scaffold for shadowing. The intermediate face allows the learner to follow a teacher's face and mouth movements more effectively. We describe a prototype system that generates an intermediate face from real-time camera input and captured video. We also discuss a user study of the prototype system with crowd-sourced participants. The results of the user study suggest that the prototype system provided better pronunciation cues than video-only shadowing techniques.",
    "keywords": [
      "shadowing",
      "second language learning",
      "face tracking"
    ],
    "doi": "10.1145/2735711.2735828",
    "url": "https://doi.org/10.1145/2735711.2735828",
    "citations": 2,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "113–116",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735796",
    "title": "Assessment of stimuli for supporting speed reading on electronic devices",
    "authors": [
      "Tilman Dingler",
      "Alireza Sahami Shirazi",
      "Kai Kunze",
      "Albrecht Schmidt"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Technology has introduced multimedia to tailor information more broadly to our various senses, but by no means has the ability to consume information through reading lost its importance. To cope with the ever-growing amount of textual information to consume, different techniques have been proposed to increase reading efficiency: rapid serial visual presentation (RSVP) has been suggested to increase reading speed by effectively reducing the number of eye movements. Further, moving a pen, finger or the entire hand across text is a common technique among speed readers to help guide eye movements. We adopted these techniques for electronic devices by introducing stimuli on text that guide users' eye movements. In a series of two user studies we sped up users' reading speed to 150% of their normal rate and evaluated effects on text comprehension, mental load, eye movements and subjective perception. Results show that reading speed can be effectively increased by using such stimuli while keeping comprehension rates nearly stable. We observed initial strain on mental load which significantly decreased after a short while. Subjective feedback conveys that kinetic stimuli are better suited for long, complex text on larger displays, whereas RSVP was preferred for short text on small displays.",
    "keywords": [
      "speed reading",
      "mental load",
      "kinetic stimulus",
      "comprehension",
      "RSVP"
    ],
    "doi": "10.1145/2735711.2735796",
    "url": "https://doi.org/10.1145/2735711.2735796",
    "citations": 15,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "117–124",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735832",
    "title": "How much do you read? counting the number of words a user reads using electrooculography",
    "authors": [
      "Kai Kunze",
      "Masai Katsutoshi",
      "Yuji Uema",
      "Masahiko Inami"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We read to acquire knowledge. Reading is a common activity performed in transit and while sitting, for example during commuting to work or at home on the couch. Although reading is associated with high vocabulary skills and even with increased critical thinking, we still know very little about effective reading habits. In this paper, we argue that the first step to understanding reading habits in real life we need to quantify them with affordable and unobtrusive technology. Towards this goal, we present a system to track how many words a user reads using electrooculography sensors. Compared to previous work, we use active electrodes with a novel on-body placement optimized for both integration into glasses (or head-worn eyewear etc) and for reading detection. Using this system, we present an algorithm capable of estimating the words read by a user, evaluate it in an user independent approach over experiments with 6 users over 4 different devices (8\" and 9\" tablet, paper, laptop screen). We achieve an error rate as low as 7% (based on eye motions alone) for the word count estimation (std = 0.5%).",
    "keywords": [
      "wordcount",
      "reading",
      "eye movement analysis",
      "EOG"
    ],
    "doi": "10.1145/2735711.2735832",
    "url": "https://doi.org/10.1145/2735711.2735832",
    "citations": 13,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "125–128",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735798",
    "title": "Designable sports field: sport design by a human in accordance with the physical status of the player",
    "authors": [
      "Ayaka Sato",
      "Jun Rekimoto"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We present the Designable Sports Field (DSF), an environment where a \"designer\" designs a sports field in accordance with the physical intensity of the player. Sports motivate players to compete and interact with teammates. However, the rules are fixed; thus, people who lack experience or physical strength often do not enjoy playing. In addition, the levels of the players should preferably match. On the other hand, in coaching, a coach trains players according to their skills. However, to be a coach requires considerable experience and expertise. We present a DSF application system called SportComposer. In this system, the \"designer\" and \"player,\" roles that can be assumed even by amateur players, participate in the sport to achieve different goals. The designer designs a sports field according to the physical status of the player, such as his/her heart rate, in real time. Thus, the player can play a physical game that matches his/her physical intensity. In experiments conducted under this environment, we tested the system with persons ranging from a small child to adults who are not expert in sports and confirmed that both the roles of the designer and the player are functional and enjoyable. We also report findings from a demonstration conducted with 92 participants in a public museum.",
    "keywords": [
      "level adjustment",
      "heart rate monitoring",
      "exergames",
      "digital sports",
      "collaborative"
    ],
    "doi": "10.1145/2735711.2735798",
    "url": "https://doi.org/10.1145/2735711.2735798",
    "citations": 5,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "129–136",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735834",
    "title": "Augmented dodgeball: an approach to designing augmented sports",
    "authors": [
      "Takuya Nojima",
      "Ngoc Phuong",
      "Takahiro Kai",
      "Toshiki Sato",
      "Hideki Koike"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Ubiquitous computing offers enhanced interactive, human-centric experiences including sporting and fitness-based applications. To enhance this experience further, we consider augmenting dodgeball by adding digital elements to a traditional ball game. To achieve this, an understanding of the game mechanics with participating movable bodies, is required. This paper discusses the design process of a ball--player-centric interface that uses live data acquisition during gameplay for augmented dodgeball, which is presented as an application of augmented sports. Initial prototype testing shows that player detection can be achieved using a low-energy wireless sensor based network such as that used with fitness sensors, and a ball with an embedded sensor together with proximity tagging.",
    "keywords": [
      "proximity-based interfaces",
      "prototyping",
      "gaming",
      "dodgeball",
      "design",
      "ball interface",
      "augmented sports"
    ],
    "doi": "10.1145/2735711.2735834",
    "url": "https://doi.org/10.1145/2735711.2735834",
    "citations": 19,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "137–140",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2735711.2735836",
    "title": "A mobile augmented reality system to enhance live sporting events",
    "authors": [
      "Samantha Bielli",
      "Christopher G. Harris"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Sporting events broadcast on television or through the internet are often supplemented with statistics and background information on each player. This information is typically only available for sporting events followed by a large number of spectators. Here we describe an Android-based augmented reality (AR) tool built on the Tesseract API that can store and provide augmented information about each participant in nearly any sporting event. This AR tool provides for a more engaging spectator experience for viewing professional and amateur events alike. We also describe the preliminary field tests we have conducted, some identified limitations of our approach, and how we plan to address each in future work.",
    "keywords": [
      "sporting event analytics",
      "object recognition",
      "computer vision",
      "augmented reality"
    ],
    "doi": "10.1145/2735711.2735836",
    "url": "https://doi.org/10.1145/2735711.2735836",
    "citations": 20,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "141–144",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735794",
    "title": "A teleoperated bottom wiper",
    "authors": [
      "Takeo Hamada",
      "Hironori Mitake",
      "Shoichi Hasegawa",
      "Makoto Sato"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In order to aid elderly and/or disabled people in cleaning and drying their posterior after defecation, a teleoperated bottom wiper is proposed. The wiper enables a person sitting on the toilet seat to wipe his/her bottom by specifying the wiping position and strength with a computer mouse and keyboard. The proposed teleoperation is novel in that the operator and target are the same. The operator feels force feedback through the buttocks instead of the hands. The result of a user study confirmed that users could successfully wipe the buttocks with appropriate position and strength by teleoperation. Since it is controller by the user, the teleoperated wiper is suitable for accommodating each participant's preference of the moment.",
    "keywords": [
      "toilet aids",
      "teleoperation",
      "personal hygiene",
      "bottom wiper",
      "augmented healthcare"
    ],
    "doi": "10.1145/2735711.2735794",
    "url": "https://doi.org/10.1145/2735711.2735794",
    "citations": 2,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "145–150",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735826",
    "title": "The toilet companion: a toilet brush that should be there for you and not for others",
    "authors": [
      "Laurens Boer",
      "Nico Hansen",
      "Ragna L. Möller",
      "Ana I. C. Neto",
      "Anne H. Nielsen",
      "Robb Mitchell"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this article we present the Toilet Companion: an augmented toilet brush that aims to provide moments of joy in the toilet room, and if necessary, stimulates toilet goers to use the brush. Based upon the amount of time a user sits upon the toilet seat, the brush swings it handle with increasing speed: initially to draw attention to its presence, but over time to give a playful impression. Hereafter, the entire brush makes rapid up and downward movements to persuade the user to pick it up. In use, it generates beeps in response to human handling, to provide a sense of reward and accompanying pleasure. Despite our aims in providing joy and stimulation, participants from field trials with the Toilet Companion reported experiencing the brush as undesirable, predominantly because the sounds produced by the brush would make private toilet room activities publicly perceivable. The design intervention thus challenged the social boundaries of the otherwise private context of the toilet room, opening up an interesting area for design-ethnographic research about perception of space, where interactive artifacts can be mobilized to deliberately breach public, social, personal, and intimate spaces.",
    "keywords": [
      "space perception",
      "private space",
      "experience design",
      "HCI"
    ],
    "doi": "10.1145/2735711.2735826",
    "url": "https://doi.org/10.1145/2735711.2735826",
    "citations": 9,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "151–154",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735817",
    "title": "EcoBears: augmenting everyday appliances with symbolic and peripheral feedback",
    "authors": [
      "Nick Nielsen",
      "Sandra B. P. S. Pedersen",
      "Jens A. Sørensen",
      "Nervo Verdezoto",
      "Nikolai H. Øllegaard"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper introduces the EcoBears concept that aims to augment household appliances with functional and aesthetic features to promote their use and longevity of use to prevent their disposal. The EcoBears also aim to support the communication of environmental issues in the home setting. The initial design and implementation of the EcoBears that consist of two bear modules (a mother and her cub) is presented as well as the preliminary concept validation and lessons learned to be considered for future work.",
    "keywords": [
      "sustainability",
      "augmented appliances",
      "ambient awareness"
    ],
    "doi": "10.1145/2735711.2735817",
    "url": "https://doi.org/10.1145/2735711.2735817",
    "citations": 7,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "155–156",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735820",
    "title": "Lovable couch: mitigating distrustful feelings for couples by visualizing excitation",
    "authors": [
      "Takuya Iwamoto",
      "Soh Masuko"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Increasing percentage of unmarried individuals in Japan has triggered decline in birth rate. This is partially caused by dominating modern lifestyles that involve long working hours, as well as increasing sex segregation in social interaction. People who are singles have fewer opportunities to build romantic relationships; therefore, speed-dating services have recently become popular. However, challenges still remain in supporting dating interaction especially to determine whether a potential couple feels affection toward each other. Hence, many people feel distrust and anxiety when being approached by a dating partner thus makes them feel hesitate to move forward. In this work, we report our findings that by visualizing excitation aggregated from users' heartbeat changes potentially help users to determine whether their potential partners feel mutual affection during a date. We propose Lovable Couch, an approach to support dating session by visually actuating a sofa with user's excitement measures as a way to mitigate users' anxiety.",
    "keywords": [
      "speed dating",
      "love",
      "heartbeats",
      "distrustful feelings",
      "LF/HF"
    ],
    "doi": "10.1145/2735711.2735820",
    "url": "https://doi.org/10.1145/2735711.2735820",
    "citations": 8,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "157–158",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735818",
    "title": "Directional communication using spatial sound in human-telepresence",
    "authors": [
      "Shohei Nagai",
      "Shunichi Kasahara",
      "Jun Rekimoto"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Communication is essential for working effectively with others. We communicate with each other to share their situation and what they are thinking. Especially, using voice is one of the most common ways to communicate. In previous research, we proposed LiveSphere that shares the surrounding environment with a remote person and provides immersive experience to effectively collaborate with each other. This system realizes \"human-telepresence\" where a person can be in other person and experience the environment. However, the communication in human-telepresence has some problems. In this paper, we propose directional communication with spatial sound to alleviate the problems. We also report on the result of user study.",
    "keywords": [
      "wearable computer",
      "spatial sound",
      "remote collaboration",
      "omnidirectional image",
      "human-telepresence",
      "first person view streaming"
    ],
    "doi": "10.1145/2735711.2735818",
    "url": "https://doi.org/10.1145/2735711.2735818",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "159–160",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735813",
    "title": "PukuPuCam: a recording system from third-person view in scuba diving",
    "authors": [
      "Masaharu Hirose",
      "Yuta Sugiura",
      "Kouta Minamizawa",
      "Masahiko Inami"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper, we propose \"PukuPuCam\" system, an apparatus to record one's diving experience from a third-person view, allowing the user to recall the experience at a later time. \"PukuPuCam\" continuously captures the center of the user's view point, by attaching a floating camera to the user's body using a string. With this simple technique, it is possible to maintain the same viewpoint regardless of the diving speed or the underwater waves. Therefore, user can dive naturally without being conscious about the camera. The main aim of this system is to enhance the diving experiences by recording user's unconscious behaviour and interactions with the surrounding environment.",
    "keywords": [
      "wearable camera",
      "third-person view",
      "life log",
      "augmented sports"
    ],
    "doi": "10.1145/2735711.2735813",
    "url": "https://doi.org/10.1145/2735711.2735813",
    "citations": 6,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "161–162",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735814",
    "title": "The augmented narrative: toward estimating reader engagement",
    "authors": [
      "Kai Kunze",
      "Susana Sanchez",
      "Tilman Dingler",
      "Olivier Augereau",
      "Koichi Kise",
      "Masahiko Inami",
      "Terada Tsutomu"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We present the concept of bio-feedback driven computing to design a responsive narrative, which acts according to the readers experience. We explore on how to detect engagement and give our evaluation on the usefulness of different sensor modalities. We find temperature and blink frequency are best to estimate engagement and can classify engaging and non-engaging user-independent without error for a small user sample size (5 users).",
    "keywords": [
      "skin temperature",
      "reading",
      "eye tracking",
      "engagement",
      "blink frequency"
    ],
    "doi": "10.1145/2735711.2735814",
    "url": "https://doi.org/10.1145/2735711.2735814",
    "citations": 17,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "163–164",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735815",
    "title": "Cyrafour: an experiential activity facilitating empathic distant communication among copresent individuals",
    "authors": [
      "Enrique Encinas",
      "Robb Mitchell"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Distant communication relies mostly on a non-embodied representation of participants (e.g. textual in chats, photographic in videoconference, auditory in telephony, etc) that lessens the sensory richness of conversational interactions. Cyrafour is a novel activity that explores the implications of using human avatars (cyranoids) for empathic interpersonal remote communication. An unscripted conversation between two individuals (the sources) is transmitted through radio waves and reproduced by two copresent subjects (the cyranoids) following certain conversational guidelines. In particular, the Sources were invited to discuss about a topic, play a conversation game and comment on an opinionated video. All Cyrafour sessions were video-taped and participants interviewed afterwards in order to support analysis and discussion. Cyrafour could be considered as a playful embodied identity game in which cyranoids are simultaneously together in and aside from a conversation generated elsewhere. This puzzling circumstance seems to allow for an empathic embodiment of the meaning transmitted and appears to create a frame for further discussion on the topics raised.",
    "keywords": [
      "telepresence",
      "serious games",
      "human avatars",
      "embodied cognition",
      "cyranoids",
      "copresence"
    ],
    "doi": "10.1145/2735711.2735815",
    "url": "https://doi.org/10.1145/2735711.2735815",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "165–166",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735819",
    "title": "FootNote: designing a cost effective plantar pressure monitoring system for diabetic foot ulcer prevention",
    "authors": [
      "Kin Fuai Yong",
      "Juan Pablo Forero",
      "Shaohui Foong",
      "Suranga Nanayakkara"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Diabetic Food Ulcer (DFU) is one of the dangerous complications of Diabetes Mellitus that is notoriously progressive and high in recurrence. Peripheral neuropathy, or damage to nerves in the foot, is the culprit that leads to DFU. Many research and commercial development has attempted to mitigate the condition by establishing an artificial feedback through in-shoe pressure-sensing solutions for patients. However these solutions suffer from inherent issues of analog sensors, prohibitive price tags and inflexibility in the choice of footwear. We approached these problems by designing a prototype with fabric digital sensors. The data showed promising potential for assertion frequency tracking and user activity recognition. Although the bigger challenge lies ahead -- to correlate approximation by digital sensors to analog pressure reading, we have demonstrated that an inexpensive, more versatile and flexible solution based from digital sensors for DFU prevention is indeed feasible.",
    "keywords": [
      "wearable",
      "podiatry",
      "peripheral neuropathy",
      "diabetic management",
      "diabetic foot ulcer"
    ],
    "doi": "10.1145/2735711.2735819",
    "url": "https://doi.org/10.1145/2735711.2735819",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "167–168",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735821",
    "title": "Mudra: a multimodal interface for braille teaching",
    "authors": [
      "Aman Srivastava",
      "Sanskriti Dawle"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This poster explores how multimodal interfaces could be used to teach Braille faster and more efficiently. Mudra, an interface to teach Braille has been made intuitive by incorporating speech recognition, tactile and audio feedback. A prototype of the interface has been developed using a mobile phone application, Raspberry Pi based single cell refreshable Braille display and audio headset.",
    "keywords": [
      "multimodal",
      "human computer interaction",
      "haptic interface",
      "braille"
    ],
    "doi": "10.1145/2735711.2735821",
    "url": "https://doi.org/10.1145/2735711.2735821",
    "citations": 5,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "169–170",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735816",
    "title": "Telexistence drone: design of a flight telexistence system for immersive aerial sports experience",
    "authors": [
      "Hirohiko Hayakawa",
      "Charith Lasantha Fernando",
      "MHD Yamen Saraiji",
      "Kouta Minamizawa",
      "Susumu Tachi"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper, a new sports genre, \"Aerial Sports\" is introduced where the humans and robots collaborate to enjoy space as a whole new field. By integrating a flight unit with the user's voluntary motion, everyone can enjoy the crossing physical limitations such as height and physique. The user can dive into the drone by wearing a HMD and experience the provided binocular stereoscopic visuals and sensation of flight using his limbs effectively. In this paper, the requirements and design steps for a Synchronization of visual information and physical motion in a flight system is explained mainly for aerial sports experience. The requirements explained in this paper can be also adapted to the purpose such as search and rescue or entertainment purposes where the coupled body motion has advantages.",
    "keywords": [
      "virtual reality",
      "flight experience",
      "drone telexistence",
      "augmented sports",
      "aerial sports"
    ],
    "doi": "10.1145/2735711.2735816",
    "url": "https://doi.org/10.1145/2735711.2735816",
    "citations": 13,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "171–172",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735822",
    "title": "Really eating together: a kinetic table to synchronise social dining experiences",
    "authors": [
      "Robb Mitchell",
      "Alexandra Papadimitriou",
      "Youran You",
      "Laurens Boer"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Eating is one of the most social of human activities; yet, scant attention has been paid to coordinating meal completion speeds. Addressing this challenge, we present \"Keep Up With Me\" - a novel augmented dining table designed to guide diners in keeping pace with each other. This mechatronical table incorporates a mechanism to gauge the relative weight of food on the dishes of dining partners. Actuators gradually raise the dish of a slower eating partner, and lower the dish of a faster eater by a corresponding amount. These discrete signals may iteratively bring the eating pace of dining companions back into mutual alignment. This table is offered as a contribution toward discussions around the subtle augmentation of dining and social experiences.",
    "keywords": [
      "social interaction design",
      "interactive table",
      "food design"
    ],
    "doi": "10.1145/2735711.2735822",
    "url": "https://doi.org/10.1145/2735711.2735822",
    "citations": 30,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "173–174",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/2735711.2735799",
    "title": "Extracting users' intended nuances from their expressed movements: in quadruple movements",
    "authors": [
      "Takanori Komatsu",
      "Chihaya Kuwahara"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We propose a method for extracting users' intended nuances from their expressed quadruple movements. Specifically, this method can quantify such nuances as a four dimensional vector representation sharpness, softness, dynamics, largeness. We then show an example of a music application based on this method that changes the volume of assigned music tracks in accordance with each attribute of the vector representation extracted from their quadruple movements like a music conductor.",
    "keywords": [
      "users' intended nuances",
      "users' expressed movements",
      "quadruple movements",
      "computer music"
    ],
    "doi": "10.1145/2735711.2735799",
    "url": "https://doi.org/10.1145/2735711.2735799",
    "citations": 0,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "175–176",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735800",
    "title": "Using point-light movement as peripheral visual guidance for scooter navigation",
    "authors": [
      "Hung-Yu Tseng",
      "Rong-Hao Liang",
      "Liwei Chan",
      "Bing-Yu Chen"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This work presents a preliminary study of utilizing point-light movement in scooter drivers' peripheral vision for turn-by-turn navigation. We examine six types of basic 1D point-light movement, and the results suggests several of them can be easily picked up and comprehended by peripheral vision in parallel with the on-going foveal vision task, and can be use to provide effective and distraction-free route-guiding experiences for scooter driving.",
    "keywords": [
      "point-light movement",
      "peripheral visualization"
    ],
    "doi": "10.1145/2735711.2735800",
    "url": "https://doi.org/10.1145/2735711.2735800",
    "citations": 5,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "177–178",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735801",
    "title": "Non-invasive optical detection of hand gestures",
    "authors": [
      "Santiago Ortega-Avila",
      "Bogdana Rakova",
      "Sajid Sadi",
      "Pranav Mistry"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper we present a novel type of sensing technology for hand and finger gesture recognition that utilizes light in the invisible spectrum to detect changes in position and form of body tissue like tendons and muscles. The proposed system can be easily integrated with existing wearable devices. Our approach not only enables gesture recognition but it could potentially double to perform a variety of health related monitoring tasks (e.g. heart rate, stress).",
    "keywords": [
      "wrist interface",
      "optical imaging",
      "machine learning",
      "input technologies",
      "gesture sensing",
      "NIR"
    ],
    "doi": "10.1145/2735711.2735801",
    "url": "https://doi.org/10.1145/2735711.2735801",
    "citations": 22,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "179–180",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735802",
    "title": "DIY IR sensors for augmenting objects and human skin",
    "authors": [
      "Paul Strohmeier"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Interaction designers require simple methods of creating ad-hoc sensors for prototyping interactive objects. Methods of creating custom sensing solutions commonly include various capacitive and resistive techniques. Near-infrared (IR) sensing solutions can be used as an alternative to these established methods. There are many situations in which IR sensors may be a preferred method of input, such as grasp detection and touch interactions on the skin. In this paper we outline the general approach for designing IR sensors and discuss the design and applications of two custom sensors.",
    "keywords": [
      "on-body touch sensing",
      "multi-touch",
      "IR-sensing"
    ],
    "doi": "10.1145/2735711.2735802",
    "url": "https://doi.org/10.1145/2735711.2735802",
    "citations": 9,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "181–182",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735803",
    "title": "Walking experience by real-scene optic flow with synchronized vibrations on feet",
    "authors": [
      "Takaaki Hayashizaki",
      "Atsuhiro Fujita",
      "Junki Nozawa",
      "Shohei Ueda",
      "Koichi Hirota",
      "Yasushi Ikei",
      "Michiteru Kitazaki"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We developed a walking recording and experiencing system. For the recording we captured stereo motion images from two cameras attached to a person's forehead with synchronized data of ankles' accelerations. For the experiencing we presented 3-D motion images with binocular disparity on a head-mounted display and vibrations to user's feet. The vibration was made from a sound of shoes when a person walked. We found that users subjectively reported the 3-D motion images with synchronized foot vibrations elicited stronger feelings of walking, leg motion, footsteps, and tele-existence than without vibrations in Experiment 1. In Experiment 2, participants' self-localization drifted in the direction of virtual walking after experiencing other walker's visual sight with the synchronized foot vibrations. These results suggest that our walking experiencing system gave users somewhat active walking feelings.",
    "keywords": [
      "walking",
      "vection",
      "tele-existence",
      "stereo images",
      "self-localization",
      "footstep"
    ],
    "doi": "10.1145/2735711.2735803",
    "url": "https://doi.org/10.1145/2735711.2735803",
    "citations": 6,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "183–184",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2735711.2735804",
    "title": "AR-HITOKE: visualizing popularity of brick and mortar shops to support purchase decisions",
    "authors": [
      "Soh Masuko",
      "Ryosuke Kuroki"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We propose a shopping support system (AR-HITOKE) that visualizes the popularity of brick and mortar shops by aggregating online and offline information using augmented reality technology that can be understood intuitively. In the proposed method, 3D-animated human-shaped icons in queues and user comments are overlaid above a shop's location on a physical map. Popularity is expressed visually by adjusting the queues length depending on offline sales data. We also visualize user comments related to each shop that are extracted from online reviews using our sentiment analysis framework. The proposed method offers new evaluation information for decision making in a physical environment and new online shopping experiences. Through exhibition of the proposed system at an actual event, we found that users are able to recognize the popularity of shops intuitively.",
    "keywords": [
      "shopping assistant",
      "online to offline",
      "in-store",
      "e-commerce",
      "digital signage",
      "augmented reality"
    ],
    "doi": "10.1145/2735711.2735804",
    "url": "https://doi.org/10.1145/2735711.2735804",
    "citations": 2,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "185–186",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735805",
    "title": "Bio-Collar: a wearable optic-kinetic display for awareness of bio-status",
    "authors": [
      "Takuya Nojima",
      "Miki Yamamura",
      "Junichi Kanebako",
      "Lisako Ishigami",
      "Mage Xue",
      "Hiroko Uchiyama",
      "Naoko Yamazaki"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Advances in sensor technology allow us to wear various sensors that detect bio-signals, such as body posture, body movement, heart rate and respiration rate. Compared with the many options of wearable sensors available, the options of display methods are limited. This paper proposes the Bio-Collar, which is a novel collar-shaped wearable bio-status display. The Bio-Collar indicates the wearer's bio-status through its color and kinetic motion.",
    "keywords": [
      "wearable",
      "kinetic clothes",
      "hairlytop interface",
      "bio feedback"
    ],
    "doi": "10.1145/2735711.2735805",
    "url": "https://doi.org/10.1145/2735711.2735805",
    "citations": 5,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "187–188",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735810",
    "title": "Superimposed projection of ghosted view on real object with color correction",
    "authors": [
      "Naoto Uekusa",
      "Takafumi Koike"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We describe a spatial augmented reality system that enables superimposed projection of an internal image on a real object with color correction. Our system is a projector-camera system, which consists of a camera, a projector, and a PC. At first, we generate a first projection image from the internal image of CG and a camera image of the real object captured by the camera. Next, we project the first projection image on the real object, and again capture an image of the real object with the internal image. At last, we update the projection image with color correction on CIELUV color space and project the image on the real object. This system will be able to visualize the internal structures on various objects easily.",
    "keywords": [
      "spatial augmented reality",
      "projector-camera system",
      "image processing",
      "ghosted view"
    ],
    "doi": "10.1145/2735711.2735810",
    "url": "https://doi.org/10.1145/2735711.2735810",
    "citations": 0,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "189–190",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735811",
    "title": "Taste change of soup by the recreating of sourness and saltiness using the electrical stimulation",
    "authors": [
      "Yukika Aruga",
      "Takafumi Koike"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We change and amplify the taste of soup by stimulating the tongue electrically. Humans can feel electric taste at the moment of electrical tongue stimulation. Electric taste includes metal taste, saltiness, sourness, and bitterness. Giving electric taste while eating will enable the amplification of taste without increasing intake of sugar and salt. Our system recreates sourness and saltiness, and changes the taste of soup by giving electric taste while eating. The subject eats soup with a spoon which has an electrode attached to it. When the spoon touches his or her tongue, a circuit is formed and stimulates the tongue electrically. To evaluate this system, we performed an experiment where the subject evaluates the taste of soup when the system stimulates the tongue electrically. The experimental results show that anode stimulation amplifies acidity, saltiness and taste strength.",
    "keywords": [
      "health",
      "electric taste",
      "eating activities"
    ],
    "doi": "10.1145/2735711.2735811",
    "url": "https://doi.org/10.1145/2735711.2735811",
    "citations": 25,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "191–192",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2735711.2735812",
    "title": "Body as display: augmenting the face through transillumination",
    "authors": [
      "Daniel Wessolek",
      "Jochen Huber",
      "Pattie Maes"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper we describe our explorations of the design space offered by augmenting parts of the human face, in this case, the ears. Using light-emitting add-ons behind the ears we aim to enhance social interactions. Scenarios range from indirect notifications of events, messaging directed to the wearer but communicated via a person face to face, or adding information regarding the internal state of the wearer, like loudness discomfort levels, concentration fatigue, or emotional strain levels.",
    "keywords": [
      "transillumination",
      "indirect notification",
      "face augmentation"
    ],
    "doi": "10.1145/2735711.2735812",
    "url": "https://doi.org/10.1145/2735711.2735812",
    "citations": 0,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "193–194",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735806",
    "title": "Personally supported dynamic random dot stereogram by measuring binocular parallax",
    "authors": [
      "Shinya Kudo",
      "Ryuta Okazaki",
      "Taku Hachisu",
      "Michi Sato",
      "Hiroyuki Kajimoto"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We present a novel approach to the use of gaze tracking as a means of supporting the experience of a Random Dot Stereogram (RDS). RDS is a method for producing an apparently noisy image that actually contains a stereoscopic scene, which becomes visible under a certain parallax of the eyes [1]. Although adjustment of eye convergence is required for RDS, many people have difficulty in making this adjustment. We implement a system by which most can stably experience stereoscopic images from RDSs. We confirmed that the times users took to find stereoscopic scenes in dynamic RDSs (d-RDS) were significantly decreased compared with presenting d-RDSs with fixed parallax. We also demonstrate this system as a means of secure information display when users input a password.",
    "keywords": [
      "security",
      "random dot stereogram",
      "eye tracking"
    ],
    "doi": "10.1145/2735711.2735806",
    "url": "https://doi.org/10.1145/2735711.2735806",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "195–196",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735809",
    "title": "The mind-window: brain activity visualization using tablet-based AR and EEG for multiple users",
    "authors": [
      "Jonathan Mercier-Ganady",
      "Maud Marchal",
      "Anatole Lécuyer"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this poster we introduce a novel approach, called the \"Mind-Window\", for real-time visualization of brain activity. The Mind-Window enables one or multiple users to visualize the brain activity of another person as if her skull was transparent. Our approach relies on the use of multiple tablet PCs that the observers can move around the head of the observed person wearing an EEG cap. A 3D virtual brain model is superimposed onto the head of the observed person using augmented reality by tracking a 3D marker placed on top of the head. The EEG cap records the electrical fields emitted by the brain, and they are processed in real-time to update the display of the virtual brain model. Several visualization techniques are proposed such as an interactive cutting plane which can be manipulated with touch-based inputs on the tablet. The Mind-Window could be used for various application purposes such as for Education as teaching tool to learn brain anatomy/activity and EEG features, e.g., electrodes localization, electrical patterns, etc.",
    "keywords": [],
    "doi": "10.1145/2735711.2735809",
    "url": "https://doi.org/10.1145/2735711.2735809",
    "citations": 2,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "197–198",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735807",
    "title": "POVeye: enhancing e-commerce product visualization by providing realistic image based point-of-view",
    "authors": [
      "Shogo Yamashita",
      "Adiyan Mujibiya"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "We present POVeye, a method to help users in capturing and creating visualization of products for extensive representation of the product's material color and texture. POVeye achieve this by providing realistic images captured from various angles, which are positioned correctly based on the calculated geometrical centroid. As input, users simply provide a video or multiple images of the product taken by any camera from arbitrary angles, without requiring any pre-calibration. POVeye provides an interface that shows object-centric camera positions alongside with image taken from respective camera angle. Users are able to either manually browse through automatically detected camera positions, or visualize the product by automatically detected view-angle path. POVeye leverages Structure-from-Motion (SfM) approach to obtain camera-object map. Our approach is unique from other solutions by preserving realistic imaging condition. We observe that visualization of products from different angles that provide information of light reflection and refraction potentially helps users to identify materials, and further perceive quality of a product.",
    "keywords": [
      "visualization",
      "view angle",
      "material texture",
      "e-commerce"
    ],
    "doi": "10.1145/2735711.2735807",
    "url": "https://doi.org/10.1145/2735711.2735807",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "199–200",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735808",
    "title": "Effective napping support system by hypnagogic time estimation based on heart rate sensor",
    "authors": [
      "Daichi Nagata",
      "Yutaka Arakawa",
      "Takatomi Kubo",
      "Keiich Yasumoto"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In daily life, lack of sleep is one of the main reasons for poor concentration. To support an effective napping, considered as one of good methods for recovering insufficient sleep and enhancing a user's concentration, we propose a hypnagogic time estimation using a heart rate sensor. Because a heart rate sensor has already been common, our method can be used widely and easily in our daily life. Most of existing sleep support systems aim to provide a comfortable wake-up by observing the sleep stage. Unlike these methods, we aim to provide an appropriate sleep duration by estimating a hypnagogic timing. By using various heart rate sensors, existing sleep support systems and 64ch electroencephalography, we tried to find out the relationship between various vital signals and sleep stages during a napping. Finally, we build a hypnagogic time estimation model by using the machine learning technique.",
    "keywords": [
      "napping support",
      "machine learning",
      "heart rate"
    ],
    "doi": "10.1145/2735711.2735808",
    "url": "https://doi.org/10.1145/2735711.2735808",
    "citations": 4,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "201–202",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2737803",
    "title": "Sharing the lights: exploration on teaching electronics for sensory augmentation development",
    "authors": [
      "Matthew Swarts",
      "Nicholas Davis",
      "Chih-Pin Hsiao",
      "James Hallam"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In the spring of 2014 a workshop on Sensory Augmentation was held at the National University of Singapore's Connective Ubiquitous Technology and Embodiments (CUTE) Center. During the workshop, three tutorials were presented followed by individual and team based projects. This paper takes a look at the tutorials developed for the workshop and suggests evaluation through enactive cognition theory.",
    "keywords": [
      "workshops",
      "sensory augmentation",
      "enactive cognition",
      "electronics education",
      "creativity"
    ],
    "doi": "10.1145/2735711.2737803",
    "url": "https://doi.org/10.1145/2735711.2737803",
    "citations": 0,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "203–204",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735786",
    "title": "Nested perspective: an art installation that intersects the physical and virtual social worlds",
    "authors": [
      "Nan-Ching Tai"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This paper presents the design concept, theoretical foundation, and expected exhibition effect of the art installation SEE[N]. SEE[N] operates on the principle of linear perspective to construct a physical structure and offers perspectives of seeing from two different scales: seeing an individual and being seen by a social group. The installation engages the audience in active behavior, which is influenced by the newly developed social patterns. The audience begins by reading familiar symbols and is driven by curiosity to further explore the installation piece, concluding with gaining a full grasp of the image at the moment of taking a photo of the installation, attempting to share it on social networking sites.",
    "keywords": [
      "social pattern",
      "linear perspective",
      "light",
      "augmented art"
    ],
    "doi": "10.1145/2735711.2735786",
    "url": "https://doi.org/10.1145/2735711.2735786",
    "citations": 0,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "205–206",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735779",
    "title": "Wearable text input interface using touch typing skills",
    "authors": [
      "Kazuya Murao"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "A lot of systems and devices for text input in wearable computing environment have been proposed and released thus far, while these are not commonly used due to drawbacks such as slow input speed, long training period, low usability, and low wearability. This paper proposes a wearable text input device using touch typing skills that would have been acquired for full-size keyboard. Users who have touch typing skills can input texts without training.",
    "keywords": [
      "wearable computing",
      "touch typing skills",
      "text input",
      "keyboard",
      "input interface",
      "glove"
    ],
    "doi": "10.1145/2735711.2735779",
    "url": "https://doi.org/10.1145/2735711.2735779",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "207–208",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735780",
    "title": "Augmented non-visual distance sensing with the EyeCane",
    "authors": [
      "Galit Buchs",
      "Shachar Maidenbaum",
      "Amir Amedi"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "How can we sense distant objects without vision?Vision is the main distal sense used by humans, thus impairing distance and spatial perception for sighted individuals in the dark or for people with visual impairments.We suggest augmenting distance perception via other senses such as using auditory or haptic cues, and have created the EyeCane for this purpose. The EyeCane is a minimal Sensory Substitution Device that enables users to perform tasks such as distance estimation, and obstacle detection and avoidance up to 5m away on-visually.In the demonstration, visitors will receive a brief training with the device, and then use it to detect objects and estimate distances while blindfolded.",
    "keywords": [
      "universal access",
      "sensory substitution",
      "multimodal",
      "interface",
      "blind",
      "assistive technology"
    ],
    "doi": "10.1145/2735711.2735780",
    "url": "https://doi.org/10.1145/2735711.2735780",
    "citations": 5,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "209–210",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735775",
    "title": "A wearable stimulation device for sharing and augmenting kinesthetic feedback",
    "authors": [
      "Jun Nishida",
      "Kanako Takahashi",
      "Kenji Suzuki"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "In this paper, we introduce a wearable stimulation device that is capable of simultaneously achieving functional electrical stimulation (FES) and the measurement of electromyogram (EMG) signals. We also propose dynamically adjustable frequency stimulation over a wide range of frequencies (1-150Hz), which allows the EMG-triggered FES device to be used in various scenarios. The developed prototype can be used not only as social playware for facilitating touch communications but also as a tool for virtual experiences such as hand tremors in Parkinson's disease, and an assistive tool for sports training. The methodology, preliminarily experiments, and potential applications are described in this paper.",
    "keywords": [
      "universal design",
      "sports training",
      "social interaction",
      "kinesthesia",
      "Parkinson's disease",
      "FES",
      "EMG"
    ],
    "doi": "10.1145/2735711.2735775",
    "url": "https://doi.org/10.1145/2735711.2735775",
    "citations": 17,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "211–212",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735784",
    "title": "SHRUG: stroke haptic rehabilitation using gaming",
    "authors": [
      "Roshan Lalintha Peiris",
      "Vikum Wijesinghe",
      "Suranga Nanayakkara"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "This demonstration paper describes SHRUG, an interactive shoulder exerciser for rehabilitation. Firstly, the system's interactive and responsive elements provide just-in-time feedback to the patients and can also be used by the therapists to observe and personalise the rehabilitation program. Secondly, it has a gamified element, which is expected to engage and motivate the patient throughout the rehabilitation process. With this demonstration, the participants will be able to use the system and play the games introduced by SHRUG and observe the feedback.",
    "keywords": [
      "stroke rehabilitation",
      "serious games",
      "responsive objects"
    ],
    "doi": "10.1145/2735711.2735784",
    "url": "https://doi.org/10.1145/2735711.2735784",
    "citations": 1,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "213–214",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735776",
    "title": "Feel &amp; see the globe: a thermal, interactive installation",
    "authors": [
      "Jochen Huber",
      "Hasantha Malavipathirana",
      "Yikun Wang",
      "Xinyu Li",
      "Jody C. Fu",
      "Pattie Maes",
      "Suranga Nanayakkara"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "\"Feel &amp; See the Globe\" is a thermal, interactive installation. The central idea is to map temperature information in regions around the world from prehistoric, modern to futuristic times onto a low fidelity display. The display visually communicates global temperature rates and lets visitors experience the temperature physically through a tangible, thermal artifact. A pertinent educational aim is to inform and teach about global warming.",
    "keywords": [
      "visual",
      "thermal",
      "interactive installation",
      "exhibit"
    ],
    "doi": "10.1145/2735711.2735776",
    "url": "https://doi.org/10.1145/2735711.2735776",
    "citations": 3,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "215–216",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735785",
    "title": "Towards effective interaction with omnidirectional videos using immersive virtual reality headsets",
    "authors": [
      "Benjamin Petry",
      "Jochen Huber"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "Omnidirectional videos (ODV), also known as panoramic videos, are an emerging, new kind of media. ODVs are typically recorded with cameras that cover up to 360° of the recorded scene. Due to the limitation of the human vision, ODVs cannot be viewed as-is. There is a larger body of work that focuses on browsing ODVs on ordinary 2D displays, e.g. on an LCD using a desktop computer or on a smartphone. In this demonstration paper, we present a new approach for ODV browsing using an immersive, head-mounted system. The novelty of our implementation lies in decoupling navigation in time from navigation in space: navigation in time is mapped to gesture-based interactions and navigation in space is mapped to head movements. We argue that this enables more natural ways of interacting with ODVs.",
    "keywords": [
      "panoramic",
      "omnidirectional",
      "mobile",
      "mid-air",
      "interactive video",
      "gesture-based interaction",
      "body-based",
      "ODV"
    ],
    "doi": "10.1145/2735711.2735785",
    "url": "https://doi.org/10.1145/2735711.2735785",
    "citations": 40,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "217–218",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2735711.2735781",
    "title": "ChromaGlove: a wearable haptic feedback device for colour recognition",
    "authors": [
      "Paweł Woźniak",
      "Kristina Knaving",
      "Mohammad Obaid",
      "Marta Gonzalez Carcedo",
      "Ayça Ünlüer",
      "Morten Fjeld"
    ],
    "year": 2015,
    "conference": "AH",
    "conferenceYear": "AH '15",
    "abstract": "While colourblindness is a disability that does not prevent those suffering from it from living fruitful lives, it does cause difficulties in everyday life situations such as buying clothes. Users suffering from colourblindness may be helped by designing devices that integrate well with their daily routines. This paper introduces ChromaGlove, a wearable device that converts colour input into haptic output thus enhancing the colour-sensing ability of the user. The device uses variable pulse widths on vibration motor to communicate differences in hue. Data is obtained through an illuminated colour sensor placed on the palm. In the future, we plan to conduct studies that will show how well a haptic glove can be integrated in everyday actions.",
    "keywords": [
      "wearable technology",
      "ubiquitous computing",
      "haptic feedback",
      "colour blindness"
    ],
    "doi": "10.1145/2735711.2735781",
    "url": "https://doi.org/10.1145/2735711.2735781",
    "citations": 8,
    "booktitle": "Proceedings of the 6th Augmented Human International Conference",
    "pages": "219–220",
    "publisher": "Association for Computing Machinery",
    "location": "Singapore, Singapore",
    "articleno": "",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875195",
    "title": "Generating Materials for Augmented Reality Applications using Natural Language",
    "authors": [
      "Sebastian Buntin"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this paper, I present a novel method in parametrizing the BRDF (and other BxDF) as well as the texture map of a material by using natural language. The visual properties of a material can be described by the user using rather complex phrases in real time. These phrases then will be parsed and mapped to BRDF and texture parameters. This allows an easy way to create and specify material representations for various applications. In the context of this paper, I focus on the application in augmented reality.",
    "keywords": [],
    "doi": "10.1145/2875194.2875195",
    "url": "https://doi.org/10.1145/2875194.2875195",
    "citations": 1,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "1",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875197",
    "title": "CASPER: A Haptic Enhanced Telepresence Exercise System for Elderly People",
    "authors": [
      "Azusa Kadomura",
      "Akira Matsuda",
      "Jun Rekimoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Although the necessity and importance of exercise support for the elderly people is largely recognized, the lack of skilled and adequate instructors often limits such activities physically. Remote exercise systems can be a solution for this problem because they may be able to support exercise activities even when instructors and participants are in separate locations. However, when simply using normal video-conferencing systems, instructors and participants have difficulty understanding each side's situation, particularly during guided physical actions. In addition, remote exercise systems cannot support the adjustment of the position of each user, a task that is quite naturally performed in normal exercise activities. Our system, called CASPER, solves these problems by proposing a mirror-like image composition method in which all the participants and the instructor are shown on the same screen so that both sides can understand the situation clearly. We also introduce an airy haptic device to remotely send tactile feedback for further enhancing sensations. In this paper, we describe the system design and its evaluation. The evaluation confirms that our system could effectively allow users to perform exercise activities even at remote locations.",
    "keywords": [
      "Airy feedback",
      "Elderly people",
      "Exercise",
      "Fitness",
      "Haptic",
      "Rehabilitation",
      "Telepresence"
    ],
    "doi": "10.1145/2875194.2875197",
    "url": "https://doi.org/10.1145/2875194.2875197",
    "citations": 14,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "2",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875204",
    "title": "Workspace Awareness in Collaborative AR using HMDs: A User Study Comparing Audio and Visual Notifications",
    "authors": [
      "Marina Cidota",
      "Stephan Lukosch",
      "Dragos Datcu",
      "Heide Lukosch"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "For most professional tasks nowadays, it is necessary to work in teams. Such collaboration often requires the exchange of visual context-related information among the team members. For so-called shared workspace collaboration, awareness of other people's activities is of utmost importance. We have developed an augmented reality (AR) framework in order to support visual communication between a team of two people who are virtually co-located. We address these people as the remote user, who uses a laptop and the local user, who wears a head-mounted display (HMD) with an RGB camera. The remote user can support the local user in solving a spatial problem by providing instructions as virtual objects in the view of the local user. For placing virtual objects in the shared workspace, we use a state-of-the-art algorithm for localization and mapping without markers. In this paper, we report on a user study that explores on how automatic audio and visual notifications about the remote user's activities affect the collaboration. The results show that in our current implementation, visual notifications are preferred over audio or no notifications independent from the level of difficulty of the task.",
    "keywords": [
      "Augmented Reality",
      "Optical See-Through HMD",
      "Virtual Co-location",
      "Workspace Awareness"
    ],
    "doi": "10.1145/2875194.2875204",
    "url": "https://doi.org/10.1145/2875194.2875204",
    "citations": 32,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "3",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875207",
    "title": "Predicting Grasps with a Wearable Inertial and EMG Sensing Unit for Low-Power Detection of In-Hand Objects",
    "authors": [
      "Marian Theiss",
      "Philipp M. Scholl",
      "Kristof Van Laerhoven"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Detecting the task at hand can often be improved when it is also known what object the user is holding. Several sensing modalities have been suggested to identify handheld objects, from wrist-worn RFID readers to cameras. A critical obstacle to using such sensors, however, is that they tend to be too power hungry for continuous usage. This paper proposes a system that detects grasping using first inertial sensors and then Electromyography (EMG) on the forearm, to then selectively activate the object identification sensors. This three-tiered approach would therefore only attempt to identify in-hand objects once it is known a grasping has occurred. Our experiments show that high recall can be obtained for grasp detection, 95% on average across participants, with the grasping of lighter and smaller objects clearly being more difficult.",
    "keywords": [
      "inertial sensing",
      "wearable EMG",
      "wrist-worn object detection"
    ],
    "doi": "10.1145/2875194.2875207",
    "url": "https://doi.org/10.1145/2875194.2875207",
    "citations": 12,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "4",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875208",
    "title": "Exploring Eye-Tracking-Driven Sonification for the Visually Impaired",
    "authors": [
      "Michael Dietz",
      "Maha El Garf",
      "Ionut Damian",
      "Elisabeth André"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Most existing sonification approaches for the visually impaired restrict the user to the perception of static scenes by performing sequential scans and transformations of visual information to acoustic signals. This takes away the user's freedom to explore the environment and to decide which information is relevant at a given point in time. As a solution, we propose an eye tracking system to allow the user to choose which elements of the field of view should be sonified. More specifically, we enhance the sonification approaches for color, text and facial expressions with eye tracking mechanisms. To find out how visually impaired people might react to such a system we applied a user centered design approach. Finally, we explored the effectiveness of our concept in a user study with seven visually impaired persons. The results show that eye tracking is a very promising input method to control the sonification, but the large variety of visual impairment conditions restricts the applicability of the technology.",
    "keywords": [
      "Eye Tracking",
      "Signal Processing",
      "Sonification",
      "Sound Synthesis",
      "Visually Impaired"
    ],
    "doi": "10.1145/2875194.2875208",
    "url": "https://doi.org/10.1145/2875194.2875208",
    "citations": 7,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "5",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875209",
    "title": "Feedback for Smooth Pursuit Gaze Tracking Based Control",
    "authors": [
      "Jari Kangas",
      "Oleg Špakov",
      "Poika Isokoski",
      "Deepak Akkil",
      "Jussi Rantala",
      "Roope Raisamo"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Smart glasses, like Google Glass or Microsoft HoloLens, can be used as interfaces that expand human perceptual, cognitive, and actuation capabilities in many everyday situations. Conventional manual interaction techniques, however, are not convenient with smart glasses whereas eye trackers can be built into the frames. This makes gaze tracking a natural input technology for smart glasses. Not much is known about interaction techniques for gaze-aware smart glasses. This paper adds to this knowledge, by comparing feedback modalities (visual, auditory, haptic, none) in a continuous adjustment technique for smooth pursuit gaze tracking. Smooth pursuit based gaze tracking has been shown to enable flexible and calibration free method for spontaneous interaction situations. Continuous adjustment, on the other hand, is a technique that is needed in many everyday situations such as adjusting the volume of a sound system or the intensity of a light source. We measured user performance and preference in a task where participants matched the shades of two gray rectangles. The results showed no statistically significant differences in performance, but clear user preference and acceptability for haptic and audio feedback.",
    "keywords": [
      "gaze tracking",
      "interactive eye-wear",
      "smooth pursuit",
      "wearable computing"
    ],
    "doi": "10.1145/2875194.2875209",
    "url": "https://doi.org/10.1145/2875194.2875209",
    "citations": 28,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "6",
    "numpages": "8",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/2875194.2875212",
    "title": "Smart Handbag as a Wearable Public Display - Exploring Concepts and User Perceptions",
    "authors": [
      "Ashley Colley",
      "Minna Pakanen",
      "Saara Koskinen",
      "Kirsi Mikkonen",
      "Jonna Häkkilä"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Wearable computing has so far focused mostly on systems employing small displays, or no displays at all. In contrast, we explore the possibilities of a smart handbag that functions as a wearable public display, focusing on user perceptions of different design concepts. Our prototype smart handbag explores functionalities such as: changing the bag's appearance to match clothing, displaying textual information, creating a see-though perception enabling items inside the bag to be seen, and enabling interaction with items inside the bag. We report on the findings from a wizard-of-Oz based user study, which included the users walking in public with the smart handbag. The smart handbag concepts were positively received, especially from the utilitarian point of view, but issues related to privacy were raised. Key insights are e.g. the creation of a 'handbag mode' for smartphones placed within the smart handbag and the importance of evaluating such wearables in real-world contexts.",
    "keywords": [
      "Design",
      "Handbag",
      "User Studies",
      "Wearable computing",
      "Wearable displays",
      "Wizard of Oz"
    ],
    "doi": "10.1145/2875194.2875212",
    "url": "https://doi.org/10.1145/2875194.2875212",
    "citations": 37,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "7",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875213",
    "title": "A Lifelog System for Detecting Psychological Stress with Glass-equipped Temperature Sensors",
    "authors": [
      "Hiroki Yasufuku",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Stress is extremely harmful to one's health. It is important to know which situations or events cause us to feel stressed: if we know the factors behind the stress, we can take corrective action. However, it is hard to perceive stress in everyday life by ourselves. Automatically detecting stress from biological information is one method for dealing with this. Stress is generally detected by using a physiological index pulse, brain activity, and breathing in order to ensure universality and accuracy. This biological information reacts to sudden stressors, not chronic stressors. However, it is difficult to use measuring devices for such data in everyday life because the devices require expertise for operation and are expensive. Our goal in this study is to develop a lifelog system featuring glass-equipped sensors that can be used on a daily basis. We detect stress by examining nasal skin temperature, which is decreased by sudden stressors. In order to investigate the recognition accuracy of the proposed system, we performed experiments at the scenes of feeling stress. Results showed that the system can distinguish factors other than stress from the change in nasal skin temperature with sufficient precision. Moreover, we investigated the optimum locations to attach temperature sensors to ensure that they have both reactivity and comfort. We also implemented an application for analyzing the measured data. The application calculates the time at which a user feels stress by analyzing the measured data and extracts a stressful scene from a video recorded from the point of view of the user.",
    "keywords": [
      "Context Awareness",
      "Life Log",
      "Wearable Computing"
    ],
    "doi": "10.1145/2875194.2875213",
    "url": "https://doi.org/10.1145/2875194.2875213",
    "citations": 6,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "8",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875214",
    "title": "Finding Motifs in Large Personal Lifelogs",
    "authors": [
      "Na Li",
      "Martin Crane",
      "Cathal Gurrin",
      "Heather J. Ruskin"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "The term Visual Lifelogging is used to describe the process of tracking personal activities by using wearable cameras. A typical example of wearable cameras is Microsoft's SenseCam that can capture vast personal archives per day. A significant challenge is to organise and analyse such large volumes of lifelogging data. State-of-the-art techniques use supervised machine learning techniques to search and retrieve useful information, which requires prior knowledge about the data. We argue that these so-called rule-based and concept-based techniques may not offer the best solution for analysing large and unstructured collections of visual lifelogs. Treating lifelogs as time series data, we study in this paper how motifs techniques can be used to identify repeating events. We apply the Minimum Description Length (MDL) method to extract multi-dimensional motifs in time series data. Our initial results suggest that motifs analysis provides a useful probe for identification and interpretation of visual lifelog features, such as frequent activities and events.",
    "keywords": [
      "Lifelogging",
      "Maximum Overlap Discrete Wavelet Transform",
      "Minimum Description Length",
      "Motif",
      "TimeSeries",
      "equal-time Correlation Matrices"
    ],
    "doi": "10.1145/2875194.2875214",
    "url": "https://doi.org/10.1145/2875194.2875214",
    "citations": 11,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "9",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875216",
    "title": "Supporting Precise Manual-handling Task using Visuo-haptic Interaction",
    "authors": [
      "Akira Nomoto",
      "Yuki Ban",
      "Takuji Narumi",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Precise manual handling skills are necessary to create art and to paint models. However, these skills are difficult to learn. Some research has approached this issue using mechanical devices. However, mechanical systems have high costs and limit the user's degree-of-freedom. In our research, we propose a system using visuo-haptics to support accurate work without using any mechanical devices. We considered the principle that when a visuo-haptic force is generated on a user's hand in the opposite direction of a target path, the user moves her/his hand to the right direction reflexively to repel the force. Based on this idea, we created a system that can modify users' hand movement by showing a dummy hand using a mixed reality display, which supports precise manual-handling tasks. To demonstrate this, we performed experiments conducted with a video see-through system that uses a head mounted display (HMD). The results showed that an expansion of the deviation between the target route and the actual hand position improved accuracy up to 50%. We also saw a tendency for a lager expansion to give the most improvement in quality, but slow down working speed at the same time. According to experimental results, we find that a gain of about 2.5 gives an ideal balance between the working precision and the drawing speed.",
    "keywords": [
      "Mixed reality",
      "Pseudo-haptics",
      "Skill support",
      "Visuo-haptics"
    ],
    "doi": "10.1145/2875194.2875216",
    "url": "https://doi.org/10.1145/2875194.2875216",
    "citations": 13,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2875194.2875217",
    "title": "Success Imprinter: A Method for Controlling Mental Preparedness Using Psychological Conditioned Information",
    "authors": [
      "Kyosuke Futami",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "It is difficult to control one's mental preparedness in important situations such as sports performances. We propose Success Imprinter, a new mental control system that enables users to strengthen their mental preparedness simply by presenting information. With Success Imprinter, users can strengthen their mental preparedness more easily than by using the previous methods. We utilize the concept of conditioning, which is a learning principle. Our system presents users with a stimulus, which is presented upon the success of their performance repeatedly, to strengthen their mental preparedness. An evaluation confirmed that Success Imprinter has a consistent effect on user's mental preparedness and results of darts competition although its effect was different for each user. Moreover, we discuss a method to identify the effect on each user on the basis of their individual characteristics. From these results, we implemented a prototype that presents conditioned information automatically.",
    "keywords": [
      "Classical conditioning",
      "Information presentation",
      "Mental",
      "Wearable"
    ],
    "doi": "10.1145/2875194.2875217",
    "url": "https://doi.org/10.1145/2875194.2875217",
    "citations": 9,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "11",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875219",
    "title": "A Dance Performance Environment in which Performers Dance with Multiple Robotic Balls",
    "authors": [
      "Shuhei Tsuchida",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In recent years, as robotics technology progresses, various mobile robots have been developed to dance with humans. However, up until now there have been no system for interactively creating a performance using multiple mobile robots. Therefore, performance using multiple mobile robots is still difficult. In this study, we construct a mechanism by which a performer can interactively create a performance while he/she considers the correspondence between his/her motion and the mobile robots' movement and light. Specifically, we developed a system that enables performers to freely create performances with multiple robotics balls that can move omunidirectionnally and have full color LEDs. Performers can design both the movements of the robotic balls and the colors of the LEDs. To evaluate the effectiveness of the system, we had four performers use the system to create and demonstrate performances. Moreover, we confirmed that the system performed reliably in a real environment.",
    "keywords": [
      "Ball",
      "Choreograghy",
      "Dance",
      "Mobile robot",
      "Performance"
    ],
    "doi": "10.1145/2875194.2875219",
    "url": "https://doi.org/10.1145/2875194.2875219",
    "citations": 9,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "12",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875220",
    "title": "An Activity Recognition Method by Measuring Circumference of Body Parts",
    "authors": [
      "Kentaro Tsubaki",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper investigates an activity recognition method where the user wears stretch sensors made of conductive fabric on each part of the body. The system recognizes actions by measuring the circumference of body parts. The electrical resistivity of our stretch sensors changes in accordance with their expansion/shrinkage. The strengths of this approach are that the appearance of the user is not interfered with and that the sensor is less expensive than other sensors such as the myoelectric sensor. The results of evaluation confirmed that our method can recognize eight contexts by measuring the circumference of four body parts (abdominal region, waist, wrist, ankle) with 99.92% accuracy.",
    "keywords": [
      "Circumference",
      "Conductive fabric",
      "Context Awareness",
      "Wearable Computing"
    ],
    "doi": "10.1145/2875194.2875220",
    "url": "https://doi.org/10.1145/2875194.2875220",
    "citations": 3,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "13",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875221",
    "title": "\"DreamHouse\": NUI-based Photo-realistic AR Authoring System for Interior Design",
    "authors": [
      "Jinwoo Park",
      "Sung Sil Kim",
      "Hyerim Park",
      "Woontack Woo"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper proposes a system which enables users to have enhanced interior deigning and authoring tool through augmented reality (AR). The proposed system, which we refer to as DreamHouse, focuses on providing natural user interaction and realistic AR experience by enabling following features: 1) allowing users to utilize bare-hand interaction with an attached egocentric RGB-D camera when trying to make detail adjustments to virtual objects' size, location and orientation, 2) rendering every virtual object in consideration of an environment and lighting conditions, creating photo-realistic scenes to help users to have immersive and realistic interior designing experience. As a result, DreamHouse allows users to freely and easily interact in physical space with virtual objects using bare hands and gives immersive and realistic AR authoring experience to users through photo-realistic rendering.",
    "keywords": [
      "3D UI/UX",
      "HMD",
      "Smart interior design",
      "bare-hand interaction",
      "global illumination",
      "real-time rendering"
    ],
    "doi": "10.1145/2875194.2875221",
    "url": "https://doi.org/10.1145/2875194.2875221",
    "citations": 15,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "14",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875226",
    "title": "Empathizing Audiovisual Sense Impairments: Interactive Real-Time Illustration of Diminished Sense Perception",
    "authors": [
      "Fabian Werfel",
      "Roman Wiche",
      "Jochen Feitsch",
      "Christian Geiger"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper addresses the challenge of empathizing with audiovisual sense impairments, e.g. in the case of elderly people or people with special needs. The developed system aims at providing users with the experience of diminished sight and hearing. We designed a system that can simulate several disease patterns, focusing on but not limited to limitations experienced by older people.The system incorporates real-time usage of visual and auditory filters on the user's actual perception by combining two cameras with a head-mounted display for stereoscopic view and a pair of microphones with equalized headphones for spatial hearing. We tested the system informally with experts and non-expert users. They qualified it as useful for enhancing the empathy for audiovisual sense impairments, motivating a further development of this idea.",
    "keywords": [
      "audiovisual interface",
      "binaural spatial hearing",
      "empathy of sense impairments",
      "head-mounted display"
    ],
    "doi": "10.1145/2875194.2875226",
    "url": "https://doi.org/10.1145/2875194.2875226",
    "citations": 20,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "15",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875227",
    "title": "Laplacian Vision: Augmenting Motion Prediction via Optical See-Through Head-Mounted Displays",
    "authors": [
      "Yuta Itoh",
      "Jason Orlosky",
      "Kiyoshi Kiyokawa",
      "Gudrun Klinker"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Naïve physics [7], or folk physics, is our ability to understand physical phenomena. We regularly use this ability in life to avoid collisions in traffic, follow a tennis ball and time the return shot, or while working in dynamic industrial settings. Though this skill improves with practice, it is still imperfect, which leads to mistakes and misjudgments for time intensive tasks. People still often miss a tennis shot, which might cause them to lose the match, or fail to avoid a car or pedestrian, which can lead to injury or even death.As a step towards reducing these errors in human judgement, we present Laplacian Vision (LV), a vision augmentation system which assists the human ability to predict future trajectory information. By tracking real world objects and estimating their trajectories, we can improve a users's prediction of the landing spot of a ball or the path of an oncoming car. We have designed a system that can track a flying ball in real time, predict its future trajectory, and visualize it in the user's field of view. The system is also calibrated to account for end-to-end delays so that the trajectory appears to emanate forward from the moving object. We also conduct a user study where 29 subjects predict an object's landing spot, and show that prediction accuracy improves 3 fold using LV.",
    "keywords": [
      "HMD",
      "Vision augmentation",
      "augmented reality",
      "prediction",
      "tracking"
    ],
    "doi": "10.1145/2875194.2875227",
    "url": "https://doi.org/10.1145/2875194.2875227",
    "citations": 50,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "16",
    "numpages": "8",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/2875194.2875239",
    "title": "Enhancing Effect of Mediated Social Touch between Same Gender by Changing Gender Impression",
    "authors": [
      "Keita Suzuki",
      "Masanori Yokoyama",
      "Yuki Kinoshita",
      "Takayoshi Mochizuki",
      "Tomohiro Yamada",
      "Sho Sakurai"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This study realizes a method to enhance the effect of touch in remote same-gender communication by changing the gender impression with a voice changer during telecommunication. We focused on touch in communication. Although psychological studies have revealed that touch has various positive effects such as triggering altruistic behavior, these effects are restrained in some cases, especially in same-gender communication, because the touch between persons of the same gender tends to cause unpleasant feelings. We aimed to address this problem to utilize the effects for telecommunication purposes, such as remote medical care and remote education, by hypothesizing that the use of telepresence could change people's gender impression, reduce this unpleasantness, and enhance the effect of touch. We tested the effectiveness of this method in a situation in which a male operator asked male participants to perform a monotonous task, and the results showed that a touch by the male operator whose voice was changed to female-like could reduce the boredom of the task and improve the friendliness toward the operator.",
    "keywords": [
      "Gender",
      "Social Touch",
      "Tactile Sensation",
      "Telepresence"
    ],
    "doi": "10.1145/2875194.2875239",
    "url": "https://doi.org/10.1145/2875194.2875239",
    "citations": 5,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "17",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875240",
    "title": "TalkingCards: Using Tactile NFC Cards for Accessible Brainstorming",
    "authors": [
      "Georg Regal",
      "Elke Mattheiss",
      "David Sellitsch",
      "Manfred Tscheligi"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Few accessible methods to support brainstorming sessions for blind and visually impaired users exist. We present an approach consisting of a smartphone application and tangible near field communication (NFC) cards enhanced with an additional layer by using different tactile materials. With the smartphone application blind and visually impaired users can write information, like ideas in brainstorming sessions, on NFC cards using speech recognition or voice recording. Text stored on the cards can also be read out loud with the application by using text to speech synthesis. The approach was developed based on a user centered design process with strong participation of blind and visually impaired users. It is low cost, lightweight and easily transportable. Evaluation results show that the approach is fun, easy to use and appreciated by the users. It can be used by blind and visually impaired people for tasks which require spatial arrangement of information, thus supporting accessible brainstorming sessions.",
    "keywords": [
      "Accessibility",
      "Blind and Visually Impaired",
      "Brainstorming",
      "Co-Design",
      "Gamestorming",
      "NFC",
      "Tactile",
      "Tangible"
    ],
    "doi": "10.1145/2875194.2875240",
    "url": "https://doi.org/10.1145/2875194.2875240",
    "citations": 12,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "18",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875244",
    "title": "Usability and Cost-effectiveness in Brain-Computer Interaction: Is it User Throughput or Technology Related?",
    "authors": [
      "Athanasios Vourvopoulos",
      "Sergi Bermudez i Badia"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In recent years, Brain-Computer Interfaces (BCIs) have been steadily gaining ground in the market, used either as an implicit or explicit input method in computers for accessibility, entertainment or rehabilitation. Past research in BCI has heavily neglected the human aspect in the loop, focusing mostly in the machine layer. Further, due to the high cost of current BCI systems, many studies rely on low-cost and low-quality equipment with difficulties to provide significant advancements in physiological computing. Open-Source projects are offered as alternatives to expensive medical equipment. Nevertheless, the effectiveness of such systems over their cost is still unclear, and whether they can deliver the same level of experience as their more expensive counterparts. In this paper, we demonstrate that effective BCI interaction in a Motor-Imagery BCI paradigm can be accomplished without requiring high-end/high-cost devices, by analyzing and comparing EEG systems ranging from open source devices to medically certified systems.",
    "keywords": [
      "Brain-Computer Interaction",
      "Cost-effectiveness",
      "EEG",
      "Motor-Imagery",
      "Usability"
    ],
    "doi": "10.1145/2875194.2875244",
    "url": "https://doi.org/10.1145/2875194.2875244",
    "citations": 33,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "19",
    "numpages": "8",
    "influentialCitations": 4
  },
  {
    "id": "10.1145/2875194.2875247",
    "title": "HearThere: Networked Sensory Prosthetics Through Auditory Augmented Reality",
    "authors": [
      "Spencer Russell",
      "Gershon Dublon",
      "Joseph A. Paradiso"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this paper we present a vision for scalable indoor and outdoor auditory augmented reality (AAR), as well as HearThere, a wearable device and infrastructure demonstrating the feasibility of that vision. HearThere preserves the spatial alignment between virtual audio sources and the user's environment, using head tracking and bone conduction headphones to achieve seamless mixing of real and virtual sounds. To scale between indoor, urban, and natural environments, our system supports multi-scale location tracking, using fine-grained (20-cm) Ultra-WideBand (UWB) radio tracking when in range of our infrastructure anchors and mobile GPS otherwise. In our tests, users were able to navigate through an AAR scene and pinpoint audio source locations down to 1m. We found that bone conduction is a viable technology for producing realistic spatial sound, and show that users' audio localization ability is considerably better in UWB coverage zones than with GPS alone. HearThere is a major step towards realizing our vision of networked sensory prosthetics, in which sensor networks serve as collective sensory extensions into the world around us. In our vision, AAR would be used to mix spatialized data sonification with distributed, livestreaming microphones. In this concept, HearThere promises a more expansive perceptual world, or umwelt, where sensor data becomes immediately attributable to extrinsic phenomena, externalized in the wearer's perception. We are motivated by two goals: first, to remedy a fractured state of attention caused by existing mobile and wearable technologies; and second, to bring the distant or often invisible processes underpinning a complex natural environment more directly into human consciousness.",
    "keywords": [
      "UWB",
      "auditory augmented reality",
      "bone conduction",
      "sensory augmentation",
      "sonification"
    ],
    "doi": "10.1145/2875194.2875247",
    "url": "https://doi.org/10.1145/2875194.2875247",
    "citations": 25,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "20",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2875194.2875248",
    "title": "Wearability Factors for Skin Interfaces",
    "authors": [
      "Xin Liu",
      "Katia Vega",
      "Pattie Maes",
      "Joe A. Paradiso"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "As interfaces progress beyond wearables and into intrinsic human augmentation, the human body has become an increasingly important topic in the field of HCI. Wearables already act as a new layer of functionality located on the body that leads us to rethink the convergence between technology and fashion, not just in terms of the ability to wear, but also in how devices interact with us. Already, several options for wearable technology have emerged in the form of clothing and accessories. However, by applying sensors and other computing devices directly onto the body surface, wearables could also be designed as skin interfaces. In this paper, we review the wearability factors impacting wearables as clothes and accessories in order to discuss them in the context of skin interfaces. We classify these wearability factors in terms of body aspects (location, body movements and body characteristics) and device aspects (weight, attachment methods, accessibility, interaction, aesthetics, conductors, insulation, device care, connection, communication, battery life). We discuss these factors in the context of two different example skin interfaces: a rigid board embedded into special effects makeup and skin-mounted soft materials connected to devices.",
    "keywords": [
      "Skin interfaces",
      "Wearability",
      "Wearable computing"
    ],
    "doi": "10.1145/2875194.2875248",
    "url": "https://doi.org/10.1145/2875194.2875248",
    "citations": 69,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "21",
    "numpages": "8",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/2875194.2875196",
    "title": "Improvements on a Novel Hybrid Tracking System",
    "authors": [
      "Markus Zank",
      "Leyla Kern",
      "Andreas Kunz"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Today's tracking systems typically require a fixed installation in a room in order to not drift quadratically with time like common inertia measurement units. This makes tracking a delicate task in ad-hoc VR installations. To overcome this problem, this paper describes a novel hybrid tracking system and shows further algorithmic improvements to increase tracking accuracy.",
    "keywords": [
      "Electromagnetic tracking",
      "inertia measuring unit",
      "redirected Walking"
    ],
    "doi": "10.1145/2875194.2875196",
    "url": "https://doi.org/10.1145/2875194.2875196",
    "citations": 2,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "22",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2875194.2875199",
    "title": "Sensible Shadow: Tactile Feedback from Your Own Shadow",
    "authors": [
      "Takefumi Hiraki",
      "Shogo Fukushima",
      "Takeshi Naemura"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper proposes a new shadow interface system that provides tactile feedback from a user's shadow to physical body. When users obstruct the projected image by their bodies, wearable photoreactive tactile displays receive the light information; they decode it and transmit tactile sensation. The proposed system does not require complex sensing, complicated settings and communication systems; it can perform high speed tactile feedback with the information directly received from the projected light. Perspective projection of the projected image and occluding objects that generate shadows is preserved in principle; all the users have to do is obstructing the projected light. In this paper, we present the concept, the first prototype of the system, and basic evaluations. The system can easily serve as a human interaction system with touch sensitive shadow, and it has potential applications in the areas of interactive entertainment and user interfaces.",
    "keywords": [
      "Body augmentation",
      "digital micromirror device",
      "haptic interaction",
      "high-speed projector",
      "human-computer interaction",
      "pixel-level visible light communication",
      "shadow interface"
    ],
    "doi": "10.1145/2875194.2875199",
    "url": "https://doi.org/10.1145/2875194.2875199",
    "citations": 10,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "23",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875201",
    "title": "TombSeer: Illuminating the Dead",
    "authors": [
      "Isabel Pedersen",
      "Nathan Gale",
      "Pejman Mirza-Babaei"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "TombSeer immerses the wearer in a museum space engaging two senses (sight and touch) through a holographic, augmented reality, heads-up interface that brings virtual, historical artifacts \"back to life\" through gestural interactivity. The purpose of TombSeer is to introduce more embodied interaction to museum visits using an emerging hardware platform for 3D interactive holographic images (e.g., META Head-mounted display) in combination with customized software. The Tomb of Kitines case study was conducted at The Royal Ontario Museum in Canada. TombSeer's embodied gestural and visual augmented reality experience functions to aesthetically enhance museum exhibits.",
    "keywords": [
      "Museum",
      "augmented reality",
      "holographic computing",
      "usability",
      "wearable technology"
    ],
    "doi": "10.1145/2875194.2875201",
    "url": "https://doi.org/10.1145/2875194.2875201",
    "citations": 5,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "24",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875249",
    "title": "Haptic Assistive Bracelets for Blind Skier Guidance",
    "authors": [
      "Marco Aggravi",
      "Gionata Salvietti",
      "Domenico Prattichizzo"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Blindness dramatically limits quality of life of individuals and has profound implications for the person affected and the society as a whole. Physical mobility and exercises are strongly spurred within people, as ways to maintain health and well-being. Such activities can be really important for people with disability as well, and their increase is paramount in the well-being and assistive care system. In this work, we aim at improving the communication between the instructor and a visual impaired subject during skiing. Up to now, only the auditory channel is used to communicate basic commands to the skier. We introduce a novel use of haptic feedback in this context. In particular, the skier can receive directional information through two vibrating bracelets worn on the forearms. Haptic interaction has been proven to be processed faster by the brain demanding a less cognitive effort with respect to the auditory modality. The connection between the instructor and the skier is done by Bluetooth protocol. We tested different guiding modalities including only audio commands, audio and haptic commands and only haptic commands. Preliminary results on the use of the system reveled the haptic channel to be a promising way for guidance of blind people in winter sports.",
    "keywords": [
      "Assistive Device",
      "Blind Navigation",
      "Haptic feedback",
      "Wearable Robotics",
      "Winter sport"
    ],
    "doi": "10.1145/2875194.2875249",
    "url": "https://doi.org/10.1145/2875194.2875249",
    "citations": 32,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "25",
    "numpages": "4",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/2875194.2875223",
    "title": "We Are Super-Humans: Towards a Democratisation of the Socio-Ethical Debate on Augmented Humanity",
    "authors": [
      "Maurizio Caon",
      "Vincent Menuz",
      "Johann A. R. Roduit"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Research in human enhancement technologies (e.g., nanotechnology, genetic engineering, robotics et cetera) is exploding bringing unforeseen solutions that will expand human capabilities further. Therefore, new socio-ethical issues need to be continuously addressed. In this scenario, we argue that a revolution in addressing these issues is needed and that we should enable a democratic process to cause broader reflections on the future augmented humanity. We will present the SuperHumains.ch project as an example of educational and collaborative thinking on the future of human enhancement.",
    "keywords": [
      "Democracy",
      "Education",
      "Ethics",
      "Human enhancement technologies"
    ],
    "doi": "10.1145/2875194.2875223",
    "url": "https://doi.org/10.1145/2875194.2875223",
    "citations": 5,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "26",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875224",
    "title": "Reading-based Screenshot Summaries for Supporting Awareness of Desktop Activities",
    "authors": [
      "Tilman Dingler",
      "Passant El Agroudy",
      "Gerd Matheis",
      "Albrecht Schmidt"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Lifelogging augments people's ability to keep track of their daily activities and helps them create rich archives and foster memory. Information workers perform a lot of their key activities throughout the day on their desktop computers. We argue that activity summaries can be informed by eye-tracking data. Therefore we investigate 3 heuristics to create such summaries based on screenshots to help reconstruct people's work day: a fixed time interval, people's focus of attention as indicated by their eye gaze, and a reading detection algorithm. In a field study with 12 participants who logged their desktop activities for 3 consecutive days we evaluated the usefulness of screenshot summaries based on these heuristics. Our results show the utility of eye tracking data, and more specifically of using reading detection to determine key activities throughout the day to inform the creation of activity summaries that are more relevant and require less time to review.",
    "keywords": [
      "desktop activities",
      "lifelogging",
      "memory augmentation",
      "productivity",
      "recall",
      "smart summaries"
    ],
    "doi": "10.1145/2875194.2875224",
    "url": "https://doi.org/10.1145/2875194.2875224",
    "citations": 9,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "27",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875231",
    "title": "Charting Design Preferences on Wellness Wearables",
    "authors": [
      "Juho Rantakari",
      "Virve Inget",
      "Ashley Colley",
      "Jonna Häkkilä"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper presents a study on people's preferences with wearable wellness devices. The results are based on an online survey (n=84), where people assessed different features in wearable wellness devices. Our salient findings show that the highest rated features were the comfort of wearing the device and long battery lifetime. Altogether, factors related to the form factor and industrial design were emphasized, whereas social sharing features attracted surprisingly little attention.",
    "keywords": [
      "Wellness",
      "design",
      "user studies",
      "wearable computing"
    ],
    "doi": "10.1145/2875194.2875231",
    "url": "https://doi.org/10.1145/2875194.2875231",
    "citations": 34,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "28",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/2875194.2875232",
    "title": "Joint Trajectory Generation and Control for Overground Robot-based Gait Rehabilitation System MOPASS",
    "authors": [
      "Santiago Focke Martinez",
      "Olena Kuzmicheva",
      "Axel Graeser"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Robotic gait rehabilitation systems have appeared in the last fifteen years as an alternative to traditional physiotherapy, offering tireless and precise performance. This paper presents the design of MOPASS, a robotic system for overground gait rehabilitation. Additionally, it presents the gait trajectory generator for hip and knee motion in sagittal plane that was implemented in the system. This generator creates healthy-like gait patterns based on extrema (or inflexion) points characteristic of each joint curve. Finally, the motion controllers designed for the active joints of MOPASS are presented.",
    "keywords": [
      "\"Exoskeletons\"",
      "\"Gait\"",
      "\"Rehabilitation\"",
      "\"Robotics\""
    ],
    "doi": "10.1145/2875194.2875232",
    "url": "https://doi.org/10.1145/2875194.2875232",
    "citations": 3,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "29",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875236",
    "title": "Synthesizing Pseudo Straight View from A Spinning Camera Ball",
    "authors": [
      "Ryohei Funakoshi",
      "Yoji Okudera",
      "Hideki Koike"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "This paper proposed a spherical camera ball and its image processing algorithm, designed to provide a ball's point of view (POV) for spectating of ball sports. The proposed spherical camera ball has six cameras embedded at fixed intervals around the surface of the ball. One of the main issues for such ball-type cameras is that the device is spinning and therefore it is hard to obtain stable video stream from such spinning cameras. This paper proposed automatic selection of cameras using matching scores between the anchor frame and current frame. The resultant movie will then always shows the point of interest within the frame. We then applied image translation to obtain a pseudo straight view in which the point of interest is always shown in the center of the frame.",
    "keywords": [
      "ball camera",
      "digital sports",
      "image stitching",
      "video synthesis",
      "virtual reality"
    ],
    "doi": "10.1145/2875194.2875236",
    "url": "https://doi.org/10.1145/2875194.2875236",
    "citations": 4,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "30",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875237",
    "title": "AR-Arm: Augmented Visualization for Guiding Arm Movement in the First-Person Perspective",
    "authors": [
      "Ping-Hsuan Han",
      "Kuan-Wen Chen",
      "Chen-Hsin Hsieh",
      "Yu-Jie Huang",
      "Yi-Ping Hung"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In many activities, such as martial arts, physical exercise, and physiotherapy, the users are asked to perform a sequence of body movements with highly accurate arm positions. Sometimes, the movements are too complicated for users to learn, even by imitating the action of the coach directly. This paper presents a fully immersive augmented reality (AR) system, which provides egocentric hints to guide the arm movement of the user via a video see-through head-mounted display (HMD). By using this system, the user can perform the exactitude of arm movement simply by moving his arms to follow and match the virtual arms, rendered from coach's movement of database, in the first-person view. To ensure the rendered virtual arms correctly aligned with the user's real shoulders, a calibration method is proposed to estimate the length of the user's arms and the positions of his head and shoulders in advance. In addition, we apply the system to Tai-Chi-Chuan practicing, our preliminary study has shown that the proposed egocentric hints can provide intuitive guidance for users to follow the arm movement of the coach with exactitude.",
    "keywords": [
      "Augmented Reality",
      "Body Movement Guidance",
      "Visualization",
      "Wearable Interaction"
    ],
    "doi": "10.1145/2875194.2875237",
    "url": "https://doi.org/10.1145/2875194.2875237",
    "citations": 50,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "31",
    "numpages": "4",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/2875194.2875241",
    "title": "Come alive! Augmented Mobile Interaction with Smart Hair",
    "authors": [
      "Masaru Ohkubo",
      "Shuhei Umezu",
      "Takuya Nojima"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Many research and products have augmented mobile interaction by integrating shape changing interface to overcome limited interaction space. However, most of these control systems tend to be complex and hard to configure due to the mechanical limitations of the actuators. Furthermore there are a few studies about using a shape changing I/O integrated interface with mobile technology. Therefore, we introduce a way to augment interaction on mobile phones with shape changing interface called \"Smart Hair\". The \"Smart Hair\" is a sensor integrated actuator that curves its shape according to the intensity of light. This simple system enables the users to implement physical interaction that is synchronized with the mobile phone. Different from previous works of the research, we developed the input method for mobile use. This study describes the development of the shape changing interface on mobile phones and discusses the practical use of the developed interface. As an application example, we developed physical body of smart phone which behaves as if it is an interactive robot. In addition, we evaluated how people perceive the motion of the interface. The result showed that the behavior of the actuator could evoke subtle emotion to the users.This paper consists of three parts. First the augmentation methods are described. Then the applications are evaluated by the users and finally the possibilities and limitations of the study are discussed.",
    "keywords": [
      "Animacy",
      "Augmentation",
      "Hairlytop Interface",
      "Mobile Interaction",
      "Smart hair",
      "Tangible"
    ],
    "doi": "10.1145/2875194.2875241",
    "url": "https://doi.org/10.1145/2875194.2875241",
    "citations": 3,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "32",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875200",
    "title": "Projection Based Virtual Tablets System Involving Robust Tracking of Rectangular Objects and Hands",
    "authors": [
      "Yasushi Sugama",
      "Taichi Murase",
      "Yusaku Fujii"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "We propose a novel projection based markerless AR system that realizes multiple virtual tablets. This system detects position and posture of any rectangular objects, projects GUI to these objects, and detects touch gesture on objects. As a result, by using this system, we do not need any smart devices, but only a mere rectangular object, e.g. tissue box, book, cushion, table and so on. It does not really matter whether the tablet computer is in the living room for browsing the internet, for playing games, or for controlling consumer devices e.g. TV and air-conditioner. In order to realize this system, we developed a novel algorithm to detect arbitrary rectangular objects. This can recognize position and posture of rectangular object without markers. We measured error in the case of overlapping, as a result, experimental result shows our algorithm is more robust than existing algorithms.",
    "keywords": [
      "Projection-based augmented reality",
      "hand tracking",
      "object tracking"
    ],
    "doi": "10.1145/2875194.2875200",
    "url": "https://doi.org/10.1145/2875194.2875200",
    "citations": 3,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "33",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875202",
    "title": "Augmented Winter Ski with AR HMD",
    "authors": [
      "Kevin Fan",
      "Jean-Marc Seigneur",
      "Jonathan Guislain",
      "Suranga Nanayakkara",
      "Masahiko Inami"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "At time of writing, several affordable Head-Mounted Displays (HMD) are going to be released to the mass market, most of them for Virtual Reality (VR with Oculus Rift, Samsung Gear...) but also for indoor Augmented Reality (AR) with Hololens. We have investigated how to adapt such HMD as Oculus Rift for an outdoor AR ski slope. Rather than setting physical obstacles such as poles, our system employs AR to render dynamic obstacles by different means. During the demo, skiers will wear a video-see-through HMD while trying to ski on a real ski slope where AR obstacles are rendered.",
    "keywords": [
      "Augmented Reality (AR)",
      "Augmented Winter Sports",
      "Head-Mounted Display (HMD)",
      "Visual Augmentation",
      "Wearable Augmentation"
    ],
    "doi": "10.1145/2875194.2875202",
    "url": "https://doi.org/10.1145/2875194.2875202",
    "citations": 6,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "34",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875203",
    "title": "Electrosmog Visualization through Augmented Blurry Vision",
    "authors": [
      "Kevin Fan",
      "Jean-Marc Seigneur",
      "Suranga Nanayakkara",
      "Masahiko Inami"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Electrosmog is the electromagnetic radiation emitted from wireless technology such as Wi-Fi hotspots or cellular towers, and poses potential hazard to human. Electrosmog is invisible, and we rely on detectors which show level of electrosmog in a warning such as numbers. Our system is able to detect electrosmog level from number of Wi-Fi networks, connected cellular towers and strengths, and show in an intuitive representation by blurring the vision of the users wearing a Head-Mounted Display (HMD). The HMD displays in real-time the users' augmented surrounding environment with blurriness, as though the electrosmog actually clouds the environment. For demonstration, participants can walk in a video-see-through HMD and observe vision gradually blurred while approaching our prepared dense wireless network.",
    "keywords": [
      "Electrosmog Detection",
      "Visual Augmentation",
      "Visualizing Unseen",
      "Wearable Augmentation"
    ],
    "doi": "10.1145/2875194.2875203",
    "url": "https://doi.org/10.1145/2875194.2875203",
    "citations": 8,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "35",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875205",
    "title": "Augmentation of Human Protection Functions Using Wearable and Sensing System",
    "authors": [
      "Ryoichiro Shiraishi",
      "Takehiro Fujita",
      "Kento Inuzuka",
      "Rintaro Takashima",
      "Yoshiyuki Sankai"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Our muscles have characteristic features which are stiffness and elasticity. The reason we make the muscle's characteristic features stronger is to protect bones and organs from external attacks. The purpose of this study is to develop a voluntary protection suit that has stiffness and elasticity, and to confirm that the wearers can control it voluntarily and enhance their protective functions. The suits consist of sensing, control and pneumatic artificial muscles (PAM) units. When the sensing unit detects the wearer's protection intention based on bioelectrical signals, solenoid valve in the control unit releases compressed air in PAM. PAM, expanded by the air, generates increased stiffness and elasticity levels. The suits have PAM units around both the upper limbs and anterior and posterior the chest and the abdomen. By our basic experiment, we confirmed that the suit started putting compressed air into PAM by detecting the wearer's protection intention. The results of the wearing experiments showed that the suit could decrease the pain and pressure of wearer greatly. Based on the results, we concluded that we could develop a voluntary protection suit enhancing human protection functions. The proposed strategies could be applied for a new full-body protection system utilizing artificial muscle with stiffness and the elasticity.",
    "keywords": [
      "artificial muscle",
      "human protection functions",
      "human sensing technology",
      "wearable system"
    ],
    "doi": "10.1145/2875194.2875205",
    "url": "https://doi.org/10.1145/2875194.2875205",
    "citations": 4,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "36",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875206",
    "title": "EXILE: Experience based Interactive Learning Environment",
    "authors": [
      "Taihei Kojima",
      "Atsushi Hiyama",
      "Kenjirou Kobayashi",
      "Sachiko Kamiyama",
      "Naokata Ishii",
      "Michitaka Hirose",
      "Hiroko Akiyama"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In a hyper-aged society, health promotive activities for senior people became important social issues. In this paper, we propose an interactive virtual learning environment for health promotive exercises. It is designed for self-training at a standard home environment and to provide training effects without an informed training instructor. Proposed system consists of 3 items; a PC, a Kinect camera, and a display. In exercises, trainee posturesand ideal postures which trainees should imitate are visualized simultaneously in a 3D virtual environment. A trainee observes the differences in postures which are emphasized by real time feedback system and corrects own training postures.",
    "keywords": [
      "Elderly",
      "Interactive Learning System",
      "Resistance Training with Slow Movement",
      "Virtual Reality",
      "Visualization"
    ],
    "doi": "10.1145/2875194.2875206",
    "url": "https://doi.org/10.1145/2875194.2875206",
    "citations": 6,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "37",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875215",
    "title": "Diagram Presentation using Loudspeaker Matrix for Visually Impaired People: Sound Characteristics for their Pattern Recognition",
    "authors": [
      "Takahiro Miura",
      "Junya Suzuki",
      "Ken-ichiro Yabu",
      "Kazutaka Ueda",
      "Tohru Ifukube"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "People with total visual impairments experience difficulty in understanding graphical information. They can comprehend graphics through tactile devices including pin matrices with audio characteristics presented using voice. Although these systems can represent static figures, it is difficult to use them for presenting moving images or details of high-resolution images. For solving this issue, we focused on the characteristics of auditory perception, including time resolution and frequency detection. In this paper, we propose a loudspeaker matrix system that displays 2D patterns such as trajectories or figures. We mainly investigated the design implications of acoustic presentation methods, including readily perceivable sound paths. The results indicate that effective presentation with a loudspeaker matrix would be realized by imposing the following conditions: sound type and duration should be determined as white noise and adjusted for the individual user, respectively.",
    "keywords": [
      "Visually impaired people",
      "diagram information",
      "loudspeaker matrix",
      "sound characteristics",
      "spatial hearing"
    ],
    "doi": "10.1145/2875194.2875215",
    "url": "https://doi.org/10.1145/2875194.2875215",
    "citations": 1,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "38",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875218",
    "title": "PeaceKeeper: Augmenting the Awareness and Communication of Noise Pollution in Student Dormitories",
    "authors": [
      "Henrik Lanng",
      "Anders Lykkegaard",
      "Søren Thornholm",
      "Philip Tzannis",
      "Nervo Verdezoto"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Noise pollution can be very problematic especially in shared-living facilities such as student dormitories. After conducting interviews with nine students, we found that students are usually not aware of the level of noise pollution they are producing as part of their everyday activities. To address this challenge, we explore how to augment the awareness and communication of environmental noise among dorm residents through an ambient awareness system: PeaceKeeper. We describe the initial design, implementation and concept validation of PeaceKeeper with two students living in contiguous rooms at the same dormitory. Based on our initial findings, we highlight how ambient awareness systems can provide opportunities to make students aware of their own level of noise to avoid disturbing their neighbours.",
    "keywords": [
      "Ambient awareness",
      "noise pollution",
      "student dormitory"
    ],
    "doi": "10.1145/2875194.2875218",
    "url": "https://doi.org/10.1145/2875194.2875218",
    "citations": 3,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "39",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875222",
    "title": "Semi-automatic Multiple Player Tracking of Soccer Games using Laser Range Finders",
    "authors": [
      "Yuma Kabeya",
      "Fumiharu Tomiyasu",
      "Kenji Mase"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "The diversity of sports broadcasting formats increases fans' interest in watching games. Soccer video is one such example ---it may include data analyses of player performances, highlighting of target players, and other useful features. Such video also supports player coaching and training. Numerous studies have attempted to estimate exact trajectories using camera information. However, camera information is not robust in situations with varying brightness levels, complicated backgrounds, and so on. To address these issues, we propose a soccer player tracking method using laser range finders. Our proposed method can precisely acquire the positional information of players. Over 95 % of the temporal connections can be easily established with a graph construction method. In order to further reduce manual connecting operations, we adopt automatic corrections using a particle filter. Experimental results show significant improvements in tracking results, which can reduce manual operational costs.",
    "keywords": [
      "Laser Range Finder",
      "Multiple Object Tracking",
      "Particle Filter",
      "Soccer"
    ],
    "doi": "10.1145/2875194.2875222",
    "url": "https://doi.org/10.1145/2875194.2875222",
    "citations": 1,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "40",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875225",
    "title": "Expressing Human State via Parameterized Haptic Feedback for Mobile Remote Implicit Communication",
    "authors": [
      "Jeffrey R. Blum",
      "Jeremy R. Cooperstock"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "As part of a mobile remote implicit communication system, we use vibrotactile patterns to convey background information between two people on an ongoing basis. Unlike systems that use memorized tactons (haptic icons), we focus on methods for translating parameters of a user's state (e.g., activity level, distance, physiological state) into dynamically created patterns that summarize the state over a brief time interval. We describe the vibration pattern used in our current user study to summarize a partner's activity, as well as preliminary findings. Further, we propose additional possibilities for enriching the information content.",
    "keywords": [
      "Haptic communication",
      "haptic patterns",
      "implicit communication",
      "tactons",
      "vibrotactile patterns"
    ],
    "doi": "10.1145/2875194.2875225",
    "url": "https://doi.org/10.1145/2875194.2875225",
    "citations": 12,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "41",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875228",
    "title": "Social Sensing: a Wi-Fi based Social Sense for Perceiving the Surrounding People",
    "authors": [
      "Yoni Halperin",
      "Galit Buchs",
      "Shachar Maidenbaum",
      "Maya Amenou",
      "Amir Amedi"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "People who are blind or have social disabilities can encounter difficulties in properly sensing and interacting with surrounding people. We suggest here the use of a sensory augmentation approach, which will offer the user perceptual input via properly functioning sensory channels (e.g. visual, tactile) for this purpose. Specifically, we created a Wi-Fi signal based system to help the user determine the presence of one or more people in the room. The signal's strength determines the distance of the people in near proximity. These distances are sonified and played sequentially. The Wi-Fi signal arises from common Smartphones, and can therefore be adapted for everyday use in a simple manner.We demonstrate the use of this system by showing it's significance in determining the presence of others. Specifically, we show that it allows to determine the location (i.e. close, inside or outside) and amount of people at each distance. This system can be further adopted for purposes such as locating one's group in a crowd, following a group in a new location, enhancing identification for people with prosopagnosia, raising awareness for the presence of others as part of a rehabilitation behavioral program for people with ASD, or for real-life social networking.",
    "keywords": [
      "Blind",
      "Sensory Substitution",
      "Sensory augmentation",
      "Social"
    ],
    "doi": "10.1145/2875194.2875228",
    "url": "https://doi.org/10.1145/2875194.2875228",
    "citations": 6,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "42",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875229",
    "title": "A Pedagogical Virtual Machine for Assembling Mobile Robot using Augmented Reality",
    "authors": [
      "Malek Alrashidi",
      "Ahmed Alzahrani",
      "Michael Gardner",
      "Vic Callaghan"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this paper, we propose a pedagogical virtual machine (PVM) model that aims to link physical-object activities with learning objects to reveal the related educational value of the physical objects in question. To examine the proposed method, we present an experiment based on assembling a modularised mobile-robot task called \"Buzz-Boards.\" A between-group design method was chosen for both the experimental and control groups in this study. Participants in the experimental group used an augmented reality application to help them assemble the robot, while the control group took a paper-based approach. 10 students from University of Essex were randomly assigned to each group, for a total of 20 students' participants. The evaluation factors for this study are time completion, a post-test, cognitive overload, and the learning effectiveness of each method. In overlay, assemblers who used the augmented reality application outperformed the assemblers who use the paper-based approach.",
    "keywords": [
      "Assembling Robot",
      "Augmented Reality",
      "Buzzboards",
      "Mixed Reality",
      "Pedagogical Virtual Machine"
    ],
    "doi": "10.1145/2875194.2875229",
    "url": "https://doi.org/10.1145/2875194.2875229",
    "citations": 13,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "43",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875230",
    "title": "A Co-located Meeting Support System by Scoring Group Activity using Mobile Devices",
    "authors": [
      "Hiroyuki Adachi",
      "Seiko Myojin",
      "Nobutaka Shimada"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this paper, we present a co-located meeting support system using mobile devices such as tablets and smartphones, which can use anywhere and easily set up. We have developed a system using tablets, and we re-designed its visualization and scoring methods for brainstorming. The system provides visual feedbacks of how long a person speaks to the other person and how long the person watches him/her. The system also provides two scores based on a balance degree of individual utterance and that of pair conversation as a group activity. The system is currently under experimentation.",
    "keywords": [
      "CSCW",
      "Interaction",
      "Meeting support",
      "Visualization"
    ],
    "doi": "10.1145/2875194.2875230",
    "url": "https://doi.org/10.1145/2875194.2875230",
    "citations": 0,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "44",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875233",
    "title": "s-Helmet: A Ski Helmet for Augmenting Peripheral Perception",
    "authors": [
      "Evangelos Niforatos",
      "Ivan Elhart",
      "Anton Fedosov",
      "Marc Langheinrich"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "The growing popularity of winter sports, as well as the trend towards high speed carving skies, have increased the risk of accidents on today's ski slopes. While many skiers now wear ski helmets, their bulk might in turn lower a skier's ability to sense their surroundings, potentially leading to dangerous situations. In this demo paper, we describe our Smart Ski Helmet (s-Helmet) prototype. s-Helmet uses a set of laser range finders mounted on the back to detect skiers approaching from behind and warn the wearer about potential collisions using three LEDs. Below, we describe our motivation and how the system works.",
    "keywords": [
      "Distance tracking",
      "LIDAR",
      "Peripheral perception",
      "Peripheral vision",
      "Ski safety"
    ],
    "doi": "10.1145/2875194.2875233",
    "url": "https://doi.org/10.1145/2875194.2875233",
    "citations": 13,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "45",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875234",
    "title": "SkiAR: Wearable Augmented Reality System for Sharing Personalized Content on Ski Resort Maps",
    "authors": [
      "Anton Fedosov",
      "Ivan Elhart",
      "Evangelos Niforatos",
      "Alexander North",
      "Marc Langheinrich"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Winter sports like skiing and snowboarding are often group activities. Groups of skiers and snowboarders traditionally use folded paper maps or board-mounted larger-scale maps near ski lifts to aid decision making: which slope to take next, where to have lunch, or what hazards to avoid when going off-piste. To enrich those static maps with personal content (e.g., pictures, prior routes taken, or hazards encountered), we developed SkiAR, a wearable augmented reality system that allows groups of skiers and snowboarders to share such content in-situ on a printed resort map while on the slope.",
    "keywords": [
      "Information sharing",
      "augmented reality system",
      "skiing",
      "wearables"
    ],
    "doi": "10.1145/2875194.2875234",
    "url": "https://doi.org/10.1145/2875194.2875234",
    "citations": 10,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "46",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875235",
    "title": "3D Position Estimation of Badminton Shuttle Using Unsynchronized Multiple-View Videos",
    "authors": [
      "Hidehiko Shishido",
      "Yoshinari Kameda",
      "Itaru Kitahara",
      "Yuichi Ohta"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this paper, we introduce a method to estimate 3D position of a badminton shuttle using unsynchronized multiple-view videos. The research of object tracking for sports is conducted as an application of Computer Vision to improve the tactics involved with such sports. This paper proposes a technique to stably estimate object's position by using motion blur that used be considered as observational noise in the ordinary works. Badminton shuttle has a large variation of the moving speed, the motion trajectory is unpredictable and moreover the observation size is very small. Thus, it cannot be grasped correctly with human eyes. We apply our proposed technique to badminton shuttle tracking to confirm the ability of our method to enhance the human vision. We also consider that there is some contribution to augment sports in future.",
    "keywords": [
      "3D Trajectory Estimation",
      "Anomalously Moving",
      "Badminton Shuttlecock",
      "Motion Blur",
      "Visual Object Tracking"
    ],
    "doi": "10.1145/2875194.2875235",
    "url": "https://doi.org/10.1145/2875194.2875235",
    "citations": 6,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "47",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/2875194.2875242",
    "title": "Neurogoggles for Multimodal Augmented Reality",
    "authors": [
      "Sylvain Cardin",
      "Howard Ogden",
      "Daniel Perez-Marcos",
      "John Williams",
      "Tomo Ohno",
      "Tej Tadi"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "We present a neurogoggles system that offers truly immersive augmented and virtual reality with unique biofeedback based on brain and bodily signals. The system consists of a wearable head-mounted display that integrates two stereoscopic color cameras for video-through experience, a depth sensor for object tracking, an inertial sensor for head tracking and electrophysiological measurements for brain and bodily signals. The system is designed to ensure low-latency synchronization across all functional modules, which is a key element for multimodal real-time data analysis. We showcase this technology with an immersive experience combining multimodal data and artistic digital content using Kudan augmented reality engine. In this experience, the user is presented with an interactive world behind a brain image wall poster, which reacts to user's brain state.",
    "keywords": [
      "Augmented Reality",
      "Brain-Computer Interface",
      "Head-Mounted Display"
    ],
    "doi": "10.1145/2875194.2875242",
    "url": "https://doi.org/10.1145/2875194.2875242",
    "citations": 7,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "48",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875243",
    "title": "On Control Interfaces for the Robotic Sixth Finger",
    "authors": [
      "Irfan Hussain",
      "Gionata Salvietti",
      "Domenico Prattichizzo"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "In this demo, we present two possible control interfaces for a robotic extra-finger called the Robotic Sixth Finger. One interface is an instrumented glove able to measure the human hand posture. The aim is the integration of the motion of robotic finger with that of the human hand so to achieve complex manipulation skills. The second interface is a ring with a push button embedded so to implement a simple and intuitive control. The presence of an extra robotic finger in human hand enlarges the workspace, increases the grasping capabilities and the manipulation dexterity. We will propose a series of grasping and manipulation tasks to be performed with the help of the robotic sixth finger and the relative interface so to prove their effectiveness in augmenting the human hand capabilities.",
    "keywords": [],
    "doi": "10.1145/2875194.2875243",
    "url": "https://doi.org/10.1145/2875194.2875243",
    "citations": 10,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "49",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875245",
    "title": "Repeated Cycling Sprints with Different Restricted Blood Flow Levels",
    "authors": [
      "Sarah J. Willis",
      "Laurent Alvarez",
      "Grégoire P. Millet",
      "Fabio Borrani"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "We examined the effect of different levels of blood flow restriction (BFR) during repeated sprint performance in leg cycling. The aim of this study was to evaluate the RSA leg cycling performance between various levels of blood flow restriction in well-trained athletes. Eleven athletes (6 men; 5 women) performed four sessions (familiarization, 0% BFR, 45% BFR, 60% BFR) of repeated sprint ability (10sec sprint, followed by 20sec recovery; RSA) to exhaustion or task failure. The number of sprints across conditions decreased with higher BFR (29.8±13.7, 13.1±6.5, 7.5±6.4, respectively, P &lt; 0.05). Total work performed and maximal heart rate during the sprints were also reduced as the level of BFR increased. Mean power decreased across sprints as well as with increased occlusion (P &lt; 0.001), and a faster decrease in power output was demonstrated as occlusion increased (e.g. significantly different than sprint 1 at sprint 3 in 0% BFR (P = 0.007), sprint 3 in 45% BFR (P &lt; 0.001), and sprint 2 (P &lt; 0.05) in 60% BFR, respectively). Results indicated, as expected, that RSA performance is decreased with higher levels of BFR. It is likely that other mechanisms related to central and peripheral systems differ between each condition as suggested by the increased discomfort in the legs versus decreased discomfort in breathing as well as lower maximal HR with higher BFR. It is of interest to investigate if the response elicited with BFR is similar to that of repeated sprinting in hypoxia (i.e., is a stimulus of localized hypoxia, BFR, similar or different than a stimulus of systemic hypoxia).",
    "keywords": [
      "Vascular occlusion",
      "power output",
      "repeated sprint ability"
    ],
    "doi": "10.1145/2875194.2875245",
    "url": "https://doi.org/10.1145/2875194.2875245",
    "citations": 2,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "50",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/2875194.2875246",
    "title": "Metamorphosis Hand: Dynamically Transforming Hands",
    "authors": [
      "Nami Ogawa",
      "Yuki Ban",
      "Sho Sakurai",
      "Takuji Narumi",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2016,
    "conference": "AH",
    "conferenceYear": "AH '16",
    "abstract": "Our body image is flexible enough to be able to incorporate external objects. Moreover, a change in body image can sometimes evoke particular feelings and result in modified behavior. We focus on the body as an interface between the internal self and the external world, and have constructed a system that provides users with the interactive experience of playing a virtual piano with dynamically transforming virtual hands, whose appearance/movement are considerably different from our own. We demonstrate that even though the appearance/movement of a virtual hand differs considerably from a real hand, we feel a strong body ownership. This illustrates the possibility of body ownership regarding a peculiar body image through a performance on the piano.",
    "keywords": [
      "3D-Interface",
      "Body Ownership",
      "Interactive Arts"
    ],
    "doi": "10.1145/2875194.2875246",
    "url": "https://doi.org/10.1145/2875194.2875246",
    "citations": 17,
    "booktitle": "Proceedings of the 7th Augmented Human International Conference 2016",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Geneva, Switzerland",
    "articleno": "51",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041170",
    "title": "Design method for gushed light field: aerosol-based aerial and instant display",
    "authors": [
      "Ippei Suzuki",
      "Shuntarou Yoshimitsu",
      "Keisuke Kawahara",
      "Nobutaka Ito",
      "Atsushi Shinoda",
      "Akira Ishii",
      "Takatoshi Yoshida",
      "Yoichi Ochiai"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We present a new method to render aerial images using aerosol-based fog screens. Conventional fog screens are easily affected by air flow, and their fog generators occupy large areas. In this study, we propose to add new tradeoffs between the display time and the payloads. We employ aerosol distribution from off-the-shelf sprays as a fog screen that can resist wind and has high portability. Results showed that the minimum weight of the entire system is approximately 600 g including all components, the screen raise time is approximately 0.5 s, the disappearance time is approximately 0.4 s, and the maximum wind speed at which we can project images is approximately 10 m/s. We conducted user studies on wearable applications, aerial imaging with a drone or radio-controlled model car, multi-viewpoint display, and a display embedded in the environment. This study will contribute to the exploration of new application areas for fog displays, and will augment expressions of entertainment and interactivity.",
    "keywords": [
      "aerial imaging",
      "communication",
      "design method",
      "display",
      "entertainment",
      "fog screen",
      "multi-copter",
      "projection"
    ],
    "doi": "10.1145/3041164.3041170",
    "url": "https://doi.org/10.1145/3041164.3041170",
    "citations": 7,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "1",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041171",
    "title": "Visualizing water flows with transparent tracer particles for a surround-screen swimming pool",
    "authors": [
      "Shogo Yamashita",
      "Xinlei Zhang",
      "Takashi Miyaki",
      "Shunichi Suwa",
      "Jun Rekimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "A surround-screen swimming pool can realize various forms of underwater entertainment and enable enhanced swimming training with supplemental visual information during underwater activities. However, one of the big challenges for such an augmented swimming pool is user interaction because the surround screen and water can make existing position-tracking methods unusable. In this paper, we propose a water flow visualization method with transparent tracer particles to enhance interactivity. We used an optical property of clear plastics called birefringence that provides vivid colors on transparent tracer particles when they are between two circular polarization sheets. Tracing objects using cameras in front of a complex background is not a stable method, but this technology enables visible tracer particles on a simple and dark background. For underwater entertainment, the water flow tracing works as a user interface because the transparent tracer particles do not stop users from viewing the images on the screen. For enhanced swimming training, swimmers can view visualized water flow caused by strokes in the augmented swimming pool. From the results of our stability evaluation of water flow tracing, the proposed method is valid even for complex backgrounds. We also conducted a feasibility test of the enhanced swimming training. According to the trial, the tracing particles could visualize the water flow caused by the strokes made by a swimmer.",
    "keywords": [
      "augmented sports",
      "swimming",
      "underwater entertainment",
      "water flow visualization"
    ],
    "doi": "10.1145/3041164.3041171",
    "url": "https://doi.org/10.1145/3041164.3041171",
    "citations": 4,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "2",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041172",
    "title": "GestGlove: a wearable device with gesture based touchless interaction",
    "authors": [
      "Sonu Agarwal",
      "Arindam Mondal",
      "Gurdeepak Joshi",
      "Gaurav Gupta"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Wearable devices have gained significant popularity in recent years and have enabled new modes of human computer interaction. Wearables have been explored to track user movements and gestures but typically require large arm motions. In this paper we propose a glove based wearable that is designed for active users like cyclists, motorists and skiers who typically wear a glove during their activity. The GestGlove has a gesture based user interface that enables single-handed touchless interaction. Inertial and bend sensors mounted on the device track subtle wrist and finger movements which are intuitive and convenient. We use the popular approach of Dynamic Time Warping (DTW) for the classification of this multivariate time series data. A Mahalanobis distance measure for DTW is proposed that uses the DTW warp path iteratively for metric calculation, an aspect that has not been explored in prior work.",
    "keywords": [
      "dynamic time warping",
      "gesture based interaction",
      "gesture detection and classification",
      "wearable devices"
    ],
    "doi": "10.1145/3041164.3041172",
    "url": "https://doi.org/10.1145/3041164.3041172",
    "citations": 6,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "3",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041173",
    "title": "Embodied interface for levitation and navigation in a 3D large space",
    "authors": [
      "M. Perusquía-Hernández",
      "T. Enomoto",
      "T. Martins",
      "M. Otsuki",
      "H. Iwata",
      "K. Suzuki"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We propose an embodied interface that allows both physical and virtual displacement within an Immersive Virtual Environment (IVE). It consists of a modular wearable used to control a mechanical motion base from which a user is suspended. The motion base is able to navigate in 3D through seven wires whose length is adjusted via a parallel link manipulator. Furthermore, an IMU-based body posture detection enables users to \"fly\" within the IVE at their own will, providing hands-free navigation, and facilitating other tasks' interactions. To assess the usability of this embodied interface, we compared it with a Joystick-based navigation control. The results showed that this interface allows effective navigation towards several targets located in 3D space. Even though the efficiency of target reach of the Joystick-based interaction is higher, a subjective assessment shows that the interface is comparable to the Joystick in hedonic qualities and attractiveness. Further analysis showed that with more practice, participants might navigate with a performance comparable with the Joystick. Finally, we analyzed the embodied behavior during 3D space navigation. This sheds light on requirements for further 3D navigation design.",
    "keywords": [
      "3D space navigation",
      "embodiment",
      "head anticipation"
    ],
    "doi": "10.1145/3041164.3041173",
    "url": "https://doi.org/10.1145/3041164.3041173",
    "citations": 9,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "4",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041174",
    "title": "Leaked light field from everyday material: designing material property remained light-field display",
    "authors": [
      "Kazuki Takazawa",
      "Kenta Suzuki",
      "Shinji Sakamoto",
      "Ryuichiro Sasaki",
      "Yoshikuni Hashimoto",
      "Yoichi Ochiai"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This paper introduces novel methods to add new material properties to a light field display, using computationally designed and fabricated pinholes. Most of the surface materials commonly used for LCDs are limited to smooth, transparent materials, such as glass or plastic. In conventional studies, a pinhole-based parallax barrier is employed in the design light field displays. We redefine these method to transform a non-transparent and non-smooth material, such as wood or stone, into a light field display. In this paper, we report on the fabrication process for the pinhole display, and evaluate the relationship between the pinhole and the optical characteristics of fabricated materials. We also report interactive applications including mirror-based light field displays \"Infinite AR\". It realize to change the indicate that depending on viewpoints. We propose the wood display and the stone display as some porpose. We demonstrated a plate fabricated with 100--200 μm m diameter pinholes.",
    "keywords": [
      "display",
      "fabrication",
      "light field display"
    ],
    "doi": "10.1145/3041164.3041174",
    "url": "https://doi.org/10.1145/3041164.3041174",
    "citations": 0,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "5",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041175",
    "title": "Intentiō: power distribution through a potentialized human body",
    "authors": [
      "Michinari Kono",
      "Hiromi Nakamura",
      "Jun Rekimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We present Intentiō, which allows a potentialized human body to activate low-power electronic devices by touching them. A potentialized human body, i.e., one carrying our device, acts as a common power source and distributes power to electronic devices through their body. An electronic device generally requires its own power source that needs to be managed individually. The diversity of battery sources, which must also be frequently charged or exchanged, has introduced complications for users who manage them. Providing a common power source may help users more easily manage electric devices because the devices are passive, with no power source of their own. This paper presents Intentiō's design and implementation, followed by several proof-of-concept applications based on a potentialized human body. We also discuss the safety concerns of our method. Intentiō is a concept and framework for augmenting humans so that they may interact with and handle electronic devices with power distribution, where the electric devices can passively exist without complex wiring or power sources.",
    "keywords": [
      "augmented human",
      "human-centered interaction",
      "low-power electronics",
      "passive activation",
      "potentialized human body",
      "power distribution"
    ],
    "doi": "10.1145/3041164.3041175",
    "url": "https://doi.org/10.1145/3041164.3041175",
    "citations": 3,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "6",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041176",
    "title": "Directional cueing of gaze with a vibrotactile headband",
    "authors": [
      "Jussi Rantala",
      "Jari Kangas",
      "Roope Raisamo"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Augmented attention is one of the ways human action can be enhanced with technologies. Still, more research is needed to find out effective ways to direct human attention to objects that have been determined as important to pay attention to. We investigated how vibrotactile stimulation given to the forehead could be used to cue gaze direction. We built a headband with an array of six vibrotactile actuators that presented short, tap like cues on the forehead. In an experiment, a horizontal line was shown on a computer display, and the participant's task was to look at the point of the line that they thought the vibrotactile cue was pointing to. Information of participant's gaze points was recorded using an eye tracker attached to the display. Analysis of the gaze points showed that for the majority of participants there were statistically significant differences between cues from different actuators. This indicated that the six actuators could successfully direct the participant's gaze to different areas of the visual field. On average, the precision of gaze points related to each actuator was comparable to the width of two to three fingers at arm's length. The findings of this study showed that there is potential in using vibrotactile cueing of gaze direction, for example, for directing visual attention and providing navigation cues with wearable devices such as headbands, head-mounted displays, and virtual reality headsets.",
    "keywords": [
      "attention pointing",
      "gaze",
      "gaze movements",
      "haptics",
      "tactile augmentation",
      "tactile cueing",
      "vibrotactile actuators",
      "vibrotactile cueing"
    ],
    "doi": "10.1145/3041164.3041176",
    "url": "https://doi.org/10.1145/3041164.3041176",
    "citations": 9,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "7",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041177",
    "title": "Whole-body interaction in natural environments benefits children's cognitive function compared to sedentary interaction indoors",
    "authors": [
      "Marta Ferraz",
      "Paul E. Resta",
      "Afonso O'neill"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We compared the effects of whole-body interaction in a natural environment versus sedentary interaction indoors on cognitive function in a group of 10 children aged 7 to 8 years. Neurophysiological response (sustained attention and working memory load) and episodic memory were evaluated while subjects interacted with a whole-body motion screen-based computer device, in a natural environment, and a sedentary screen-based computer device, based on hand-eye coordination skills, in a classroom setting - video game play. Children's expectations, preferences and opinions regarding the interaction devices were also evaluated. Results indicate a trend for natural environment whole-body interaction to increase sustained attention in children over time, compared to indoor sedentary interaction. In turn, increases in sustained attention were associated with improvements in episodic memory during video game play. This study shows that the current Child-Computer Interaction paradigm - interaction with sedentary devices, indoors - may be failing to optimize children's cognitive function.",
    "keywords": [
      "child-computer interaction",
      "cognitive function",
      "indoors",
      "natural environments",
      "sedentary",
      "whole-body"
    ],
    "doi": "10.1145/3041164.3041177",
    "url": "https://doi.org/10.1145/3041164.3041177",
    "citations": 3,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "8",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041178",
    "title": "AdaptiVisor: assisting eye adaptation via occlusive optical see-through head-mounted displays",
    "authors": [
      "Yuichi Hiroi",
      "Yuta Itoh",
      "Takumi Hamasaki",
      "Maki Sugimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Brightness adaptation is a fundamental ability in human visual system, and adjusts various levels of darkness and light. While this ability is continuously used, and it can mostly handle sudden lighting changes in the environment, the adaptation could still take several minutes. Moreover, during the adaptation, the color perception changes as well. This slow reactivity and perception change of the eyes could lead to mistakes for tasks performed in dazzling or temporally high-contrast environments such as when driving into the sun or during a welding process.We present AdaptiVisor, a vision augmentation system that assists the brightness adaptation of the eye. Our system selectively modulates the intensity of the light coming into the eyes via occlusion-capable Optical See-Through Head-Mounted Displays (OST-HMD). An integrated camera captures highlights and brightness in the environment via high-dynamic range capture, and our display system selectively dims or enhances part of field of views so that the user would not perceive rapid brightness changes. We build a proof-of-concept system to evaluate the feasibility of the adaptation assistance by combining a transmissive LCD panel and an OST-HMD, and test it with a user-perspective, view-point camera. The evaluation shows that the system decreases the overexposed area in a scene to 1/15th, and enhances the color by reducing majorly underexposed area to half. We also include a preliminary user trial and it indicates that the system also works for real eyes for the HMD part and to some extent for the LCD.",
    "keywords": [
      "AdaptiVisor",
      "augmented reality",
      "brightness adaptation",
      "head-mounted displays",
      "near-eye displays",
      "occlusive HMD",
      "optical see-through",
      "vision augmentation"
    ],
    "doi": "10.1145/3041164.3041178",
    "url": "https://doi.org/10.1145/3041164.3041178",
    "citations": 16,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "9",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3041164.3041179",
    "title": "Position shift of phosphene and attention attraction in arbitrary direction with galvanic retina stimulation",
    "authors": [
      "Daiki Higuchi",
      "Kazuma Aoyama",
      "Masahiro Furukawa",
      "Taro Maeda",
      "Hideyuki Ando"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Recently, a Head Mounted Display (HMD) has become popular in applications such as Virtual Reality (VR) and Augmented Reality (AR). For example, Google Glass and Microsoft HoloLens are popular as HMDs specialized for AR. The HMDs for AR give information by convoluting annotations with objects in the real world. Therefore, the objects to attach annotations are required to exist within the range HMDs can present images. However, the devices can mostly present a narrow range of images. Thus, an HMD user cannot frequently obtain necessary information because the object on which an annotation should be displayed does not exist within the visual field range of the HMD. Then, if the head of the user is guided so that the object enters within an HMD's visual field range, the problem will be solved without increasing the weight of the devices. In this work, we focus on the visual presentation method called Galvanic Retina Stimulation (GRS), and we verify the effect using GRS for presenting a clue to guide the subjects' heads.",
    "keywords": [
      "galvanic retina stimulation",
      "phosphene",
      "visual display"
    ],
    "doi": "10.1145/3041164.3041179",
    "url": "https://doi.org/10.1145/3041164.3041179",
    "citations": 12,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "10",
    "numpages": "6",
    "influentialCitations": 4
  },
  {
    "id": "10.1145/3041164.3041180",
    "title": "Early warning of task failure using task processing logs",
    "authors": [
      "Ryosuke Mita",
      "Toshiki Taleuchi",
      "Tomohiro Tanikawa",
      "Takuji Narumi",
      "Michitaka Hirose"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This study realizes a task management system that enhances people's work productivity by evaluating the task process situation accurately with respect to the deadline. Often, tasks are not completed before their deadline because of poor time management, i.e., miscalculation of the total time required to finish the tasks. This type of mistake occurs repeatedly, because of a cognitive bias called palnning fallacy, and users fail to notice the situation until it is too late. In this study, we design a system that can correct the task process behavior of users and improve their time management skills by providing information on the future success or failure of their tasks in the early phase of processing. To accomplish this, we analyzed lifelogging data related to processing of 288 tasks undertaken by from 153 people, and investigate two types of prediction methods. The first prediction method evaluates the reliability of the task processing plan, and the second evaluates the progress situation. Simulation results show that the proposed method can predict the future of tasks with an accuracy of 77.3% when the task processing plan is provided as input, and with an accuracy of 95.0% when the plan and subjective progress rate of a task after the third time working on the task is known. The results suggest that the outcome of tasks in progress can be predicted from user inputs such as the subjective progress rate and cumulative work time. Using these results, we construct a system that provides support for decision making on task process behavior.",
    "keywords": [
      "early warning",
      "future prediction",
      "lifelog",
      "task management"
    ],
    "doi": "10.1145/3041164.3041180",
    "url": "https://doi.org/10.1145/3041164.3041180",
    "citations": 3,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "11",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041181",
    "title": "Insights of the augmented dodgeball game design and play test",
    "authors": [
      "Kadri Rebane",
      "Takahiro Kai",
      "Naoki Endo",
      "Tomonari Imai",
      "Takuya Nojima",
      "Yohei Yanase"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Modern technology enables us to create novel devices and provide exciting new user experiences. In today's world where few people get engaged to physical activities regularly and in the amounts recommended by the doctors, it is important to find ways to encourage more active lifestyles. As fun, enjoyment and social aspects of the activity have a great impact on the decision on whether to exercise or not, it is wise to unite the two fields of technology and sports. Augmented Dodgeball was designed to enrich the experience of playing dodgeball and make it more attractive to participants. Specifically we focused on designing a physical game that players with different skill levels and physiology could enjoy playing together. For balancing between the players, virtual parameters that the players are aware of are used. In this paper we describe the game design and the development of the Dodgeball Game simulator as well as the observations and results from the play test.",
    "keywords": [
      "augmented dodgeball",
      "augmented sports",
      "exertion games",
      "sports",
      "superhuman sports",
      "video gaming"
    ],
    "doi": "10.1145/3041164.3041181",
    "url": "https://doi.org/10.1145/3041164.3041181",
    "citations": 9,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "12",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041182",
    "title": "ScalableBody: a telepresence robot that supports face position matching using a vertical actuator",
    "authors": [
      "Akira Matsuda",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Seeing one's partner's face during remote conversation is one of the most important factors for effective communication. When using a telepresence robot, matching face positions with one's partner is sometimes difficult, because face position varies in different situations (e.g., standing or sitting). However, existing telepresence robots cannot change their height. Moreover, due to limited camera angle, the conversation partner's face is often partly cut off in the camera view. Therefore, users cannot communicate while seeing each other's faces. To overcome these problems, we designed a telepresence robot called ScalableBody. ScalableBody has a vertical actuator that allows it to change its height and an omnidirectional camera that provides a wide view. The robot facilitates communication for different contexts using vertical actuation to match the conversation partners' face positions. Furthermore, the operator can see a partner's face in any direction through an omnidirectional camera. This approach can also provide users with the experience of being a different height, as if a giant or a child. In this paper, we describe the vertical actuator mechanism and report our user study on the telepresence robot.",
    "keywords": [
      "eye-contact",
      "face position matching",
      "remote communication",
      "surrogate robots",
      "telepresence"
    ],
    "doi": "10.1145/3041164.3041182",
    "url": "https://doi.org/10.1145/3041164.3041182",
    "citations": 14,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "13",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041183",
    "title": "JackIn space: designing a seamless transition between first and third person view for effective telepresence collaborations",
    "authors": [
      "Ryohei Komiyama",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Traditional telepresence systems only supported first person view and users had difficulty in recognizing the surrounding situation of their remote workspace. JackIn Space is a telepresence system that solves this problem by seamlessly integrating first person view with third person view. With a head-mounted first person camera and multiple depth sensors installed in the environment, the surrogate user's first person view smoothly changes to the out-of-body third person view, and the user connected to the surrogate user can virtually look around the environment. The user can also dive into other surrogate users to gain different perspectives. Our evaluation supports that the concept and the function of JackIn Space was quite well accepted and our prototype system provides a more natural viewpoint selection. Overall, JackIn Space supports better remote collaborations.",
    "keywords": [
      "3D modeling",
      "augmented reality",
      "first person view",
      "kinect",
      "remote collaboration",
      "telepresence"
    ],
    "doi": "10.1145/3041164.3041183",
    "url": "https://doi.org/10.1145/3041164.3041183",
    "citations": 58,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "14",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041184",
    "title": "MuscleVR: detecting muscle shape deformation using a full body suit",
    "authors": [
      "Arashi Shimazaki",
      "Yuta Sugiura",
      "Dan Mikami",
      "Toshitaka Kimura",
      "Maki Sugimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "The research proposes a full body suit to detect muscle shape deformations. The suit consists of a stretchable sensor composed of a photo reflector (made up of a photo transistor and an infrared(IR) LED) and a spring. The photoreflector is embedded in the spring. When the spring is not stretched, the emitted IR light will reflect back to the phototransistor. However, when the spring is stretched, some of the emitted IR light can flow out through the aperture of the spring, reducing the light reflecting back to the transistor. Therefore, the more the spring is stretched, the less the amount of light would be detected. Here, a virtual reality(VR) system that can synchronize avatar bodies with users' muscle shape deformations.",
    "keywords": [
      "muscle shape deformation",
      "phototransistor",
      "spring",
      "stretchable sensor"
    ],
    "doi": "10.1145/3041164.3041184",
    "url": "https://doi.org/10.1145/3041164.3041184",
    "citations": 2,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "15",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041185",
    "title": "Automated capture and delivery of assistive task guidance with an eyewear computer: the GlaciAR system",
    "authors": [
      "Teesid Leelasawassuk",
      "Dima Damen",
      "Walterio Mayol-Cuevas"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In this paper we describe and evaluate an assistive mixed reality system that aims to augment users in tasks by combining automated and unsupervised information collection with minimally invasive video guides. The result is a fully self-contained system that we call GlaciAR (Glass-enabled Contextual Interactions for Augmented Reality). It operates by extracting contextual interactions from observing users performing actions. GlaciAR is able to i) automatically determine moments of relevance based on a head motion attention model, ii) automatically produce video guidance information, iii) trigger these guides based on an object detection method, iv) learn without supervision from observing multiple users and v) operate fully on-board a current eyewear computer (Google Glass). We describe the components of GlaciAR together with user evaluations on three tasks. We see this work as a first step toward scaling up the notoriously difficult authoring problem in guidance systems and an exploration of enhancing user natural abilities via minimally invasive visual cues.",
    "keywords": [
      "assistive computing",
      "augmented reality",
      "eyewear computing",
      "task guidance"
    ],
    "doi": "10.1145/3041164.3041185",
    "url": "https://doi.org/10.1145/3041164.3041185",
    "citations": 18,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "16",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3041164.3041186",
    "title": "TennisMaster: an IMU-based online serve performance evaluation system",
    "authors": [
      "Disheng Yang",
      "Jian Tang",
      "Yang Huang",
      "Chao Xu",
      "Jinyang Li",
      "Liang Hu",
      "Guobin Shen",
      "Chieh-Jan Mike Liang",
      "Hengchang Liu"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Tennis sport has become more popular all over the world in recent years. While tennis lovers wish to improve their tennis skill set for better performance, unfortunately only few of them could be guided under professional training. Especially, serve is probably the most important skill in tennis skill set. In this paper, we present TennisMaster, an online diagnosis and feedback system, that aims at performing online assessment of tennis serve during the training process using IMU sensors. In particular, we propose a hierarchical evaluation approach based on the fusion of two IMU sensors mounted on the racket and shank of the player. In order to achieve online serve assessment, we first develop an online serve extraction algorithm to identify the serve segments and filter the non-serve events. Then we use Hidden Markov Model (HMM) to segment the serve process into eight stages. By extracting unique features on the basis of the serve segmentation, we build a regression model which outputs the score of a serve. We conduct experiments to collect 1,030 serves involving 12 subjects at various professional levels. Evaluation results show that our system achieves high accuracy of performance assessment for tennis serves.",
    "keywords": [
      "activity recognition",
      "skill assessment",
      "tennis",
      "wearable device"
    ],
    "doi": "10.1145/3041164.3041186",
    "url": "https://doi.org/10.1145/3041164.3041186",
    "citations": 23,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "17",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041187",
    "title": "Routine++: implementing pre-performance routine in a short time with an artificial success simulator",
    "authors": [
      "Shoichi Tagami",
      "Shigeo Yoshida",
      "Nami Ogawa",
      "Takuji Narumi",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This study proposes \"Routine++,\" a new technique to implement a pre-performance routine (PPR) in a short period of time by providing a user artificial successful experiences via a simulator. Implementing a PPR, which is the conventional approach for controlling a user's own mental state and improving performance in competitive sport, requires identifying the user's action that results in success. However, implementing PPRs is time-consuming because consistently achieving success in the real world is difficult. Therefore, the proposed technique relates user actions with artificial successes, and lets users perform a PPR in a relatively short time. In this study, we focused on putter golf as a task to evaluate the effectiveness of Routine++ because it requires consistent actions to achieve a good result. We then developed a virtual golf simulator that provides the user false feedback in terms of the user's success at achieving the objectives of golf without a feeling of incongruity. The results of our user study indicate that Routine++ helps to improve the performance of novice golfers playing under pressure. Furthermore, with this work we propose the effectiveness of practicing with computer technology to improve not only technical skills like body movements but also mental skills.",
    "keywords": [
      "artificial success",
      "false feedback",
      "mental",
      "pre-performance routine",
      "sports"
    ],
    "doi": "10.1145/3041164.3041187",
    "url": "https://doi.org/10.1145/3041164.3041187",
    "citations": 8,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "18",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041188",
    "title": "Body cyberization by spatial augmented reality for reaching unreachable world",
    "authors": [
      "Yuta Ueda",
      "Yuki Asai",
      "Ryuichi Enomoto",
      "Kai Wang",
      "Daisuke Iwai",
      "Kosuke Sato"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This paper presents a novel body cyberization technique, in which we visually extend the length of a user's arm by projected imagery to allow the user to reach unreachable objects. We thoroughly design the body cyberization model such as graphical representations, action components, and interaction techniques. Through a psychophysical experiment, we investigate if a user can feel a sense of ownership for a projected hand. We also conduct a user study to make it clear that projection-based visualization of extended arm provides users with better usability. We build various application systems and demonstrate the feasibility and effectiveness of the proposal.",
    "keywords": [
      "augmented human",
      "body extension",
      "body ownership",
      "cybernetics",
      "projection mapping",
      "spatial augmented reality"
    ],
    "doi": "10.1145/3041164.3041188",
    "url": "https://doi.org/10.1145/3041164.3041188",
    "citations": 16,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "19",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041189",
    "title": "Generating furniture for augmented reality applications using natural language",
    "authors": [
      "Sebastian Buntin"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In this paper, I present a method for modeling furniture using natural language as a high level description for different furniture parts. Furthermore I show how to use these phrases for the interactive and procedural modeling of 3D furniture objects. Thereafter I will give an introduction outlining how to apply this system to real time augmented reality applications, e.g. in the furniture industry.",
    "keywords": [
      "augmented reality",
      "furniture modeling",
      "natural language procesing",
      "procedural modeling"
    ],
    "doi": "10.1145/3041164.3041189",
    "url": "https://doi.org/10.1145/3041164.3041189",
    "citations": 1,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "20",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041190",
    "title": "SonicSG: from floating to sounding pixels",
    "authors": [
      "Suranga Nanayakkara",
      "Thomas Schroepfer",
      "Lonce Wyse",
      "Aloysius Lian",
      "Anusha Withana"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "SonicSG aimed at fostering a holistic understanding of the ways in which technology is changing our thinking about design in high-density urban city and how its creative use can reflect a sense of place. The project consisted of a large-scale interactive light installation that consisted on 1,800 floating LED lights in the shape of the island nation. These lights were individually addressable through the network and used to generate light and sound effects. The field of light was extended with \"sonified personal pixels\" that were created by the audience through personal mobile devices. These personal pixels generated a light and sound \"texture\" that connected visitors to the light field in the river and to each other. In this paper, we describe the design concept, prototyping and, implementation of as well as the user reactions to this interactive public light installation.",
    "keywords": [
      "experience design",
      "interactive public installation",
      "placemaking",
      "smart city",
      "sonification",
      "urban lighting"
    ],
    "doi": "10.1145/3041164.3041190",
    "url": "https://doi.org/10.1145/3041164.3041190",
    "citations": 2,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "21",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041191",
    "title": "Towards understanding of play with augmented toys",
    "authors": [
      "Priyashri Kamlesh Sridhar",
      "Suranga Nanayakkara",
      "Jochen Huber"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This work is directed towards understanding how the transformation of a regular object/traditional toy into an augmented toy may affect the dynamics of play behavior. We present an observational user study with 8 children from kindergarten to understand the play value of SparKubes. SparKubes are stand-alone tangible objects that accept light from one direction and pass it on in another direction. We found that children who were aware of the SparKubes' interactivity features displayed more variety of patterns and showed greater interaction with SparKubes as compared to the control group who were not aware of the features. The play behaviour revealed that SparKubes have constructive play value on the play pyramid and that adding light features changed the patterns of constructions by children. This knowledge opens up an exciting area of research in technology-mediated play and designing augmented toys for children.",
    "keywords": [
      "SparKubes",
      "play",
      "play pyramid",
      "tangible interaction"
    ],
    "doi": "10.1145/3041164.3041191",
    "url": "https://doi.org/10.1145/3041164.3041191",
    "citations": 6,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "22",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041192",
    "title": "Evaluation of unplugged powered suit with pneumatic gel muscles",
    "authors": [
      "Chetan Thakur",
      "Kazunori Ogawa",
      "Tomohiro Ikeda",
      "Toshio Tsuji",
      "Yuichi Kurita"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Assistive suits are useful in situations such as injury, muscle fatigue, stressful work environment in factories and are also useful for different age groups in these situations. In our research, we use pneumatic gel muscles (PGM) to develop assistive suit to enhance walking gait experience. We focused on assisting the swing phase of the gait cycle as it accounts for higher metabolic costs during gait. The pneumatic gel muscles are actuated with small pumps fitted in the shoe of the contralateral foot of assisted leg. By doing this we can take advantage of dual support phase in gait cycle and provide required power for PGM. This process provides assisting force during swing phase and thus helps user with improved walking experience. To test the effectiveness of this suit we identified muscles with reduced force and activation for assisted gait using Opensim simulation and measured EMG for these muscles during experiment. Similar patterns in both simulations and experimental results were observed.",
    "keywords": [
      "artificial muscle",
      "assistive suit",
      "improved gait cycle",
      "low powered",
      "pneumatic artificial muscle",
      "pneumatic gel muscle"
    ],
    "doi": "10.1145/3041164.3041192",
    "url": "https://doi.org/10.1145/3041164.3041192",
    "citations": 2,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "23",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041193",
    "title": "Vibrat-o-matic: producing vocal vibrato using EMS",
    "authors": [
      "Ryohei Fushimi",
      "Eisuke Fujinawa",
      "Takuji Narumi",
      "Tomohiro Tanikawa",
      "Michitaka Hirose"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Vibrato is one of the most popular vocal techniques. Based on the fact that vibrato is caused by periodic pulsation of the cricothyroid and diaphragm muscles, we explore the possibility of reproducing vibrato by actuating these muscles with electrical muscle stimulation. We present \"Vibrat-o-matic\", a system that enables users to interactively control their vocal vibrato while singing. We test the system on four participants, including two semi-professional singers, under two conditions of the electrode arrangement (stomach and neck). Although the muscles actuated by this system are not exactly the same as those used for natural vibrato, we succeeded in reproducing similar periodic modulation of volume, especially by stimulating the stomach. Moreover, although it is not suitable for training natural vibrato, it is suggested that this system could provide notion of when and where to use vibrato while singing.",
    "keywords": [
      "electrical muscle stimulation",
      "musical performance",
      "vibrato"
    ],
    "doi": "10.1145/3041164.3041193",
    "url": "https://doi.org/10.1145/3041164.3041193",
    "citations": 9,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "24",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041194",
    "title": "My Tai-Chi coaches: an augmented-learning tool for practicing Tai-Chi Chuan",
    "authors": [
      "Ping-Hsuan Han",
      "Yang-Sheng Chen",
      "Yilun Zhong",
      "Han-Lei Wang",
      "Yi-Ping Hung"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Tai-Chi Chuan (TCC) is a famous physical exercise and well-known for being able to effectively promote physical well-being. Many people have been interested in learning TCC at the beginning, but eventually failed in mastering it due to the lack of a constantly accompanying master on the side. In this paper, we present an augmented-learning tool, called \"My Tai-Chi Coaches\", for learning TCC. By wearing an optical see-through Head-Mounted Display (HMD), the users can have their own private coaches-on-demand that will guide them in practicing TCC. To solve the \"attention-sticking\" problem, we propose the use of \"redundant coaches\" and high-lighting the primary coach at every instant. When the user wants to adjust his posture to mimic the coach's movement, he can simply suspend his motion, and then the drone will fly to a proper position to capture the images of the user's posture, and display them on an augmented mirror placed near by the highlighted or gazed coach. In addition to learning TCC, the proposed augmented-learning tool can also be used for learning dancing, yoga, sporting, and for rehabilitation.",
    "keywords": [
      "Tai-Chi Chuan",
      "augmented mirror",
      "drone",
      "mixed reality",
      "physical activity learning"
    ],
    "doi": "10.1145/3041164.3041194",
    "url": "https://doi.org/10.1145/3041164.3041194",
    "citations": 59,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "25",
    "numpages": "4",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/3041164.3041195",
    "title": "InSight: a systematic approach to create dynamic human-controller-interactions",
    "authors": [
      "Roger Boldu",
      "Haimo Zhang",
      "Juan Pablo Forero Cortés",
      "Sachith Muthukumarana",
      "Suranga Nanayakkara"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We present InSight, an intuitive technique to control smart objects with existing input devices in the environment, while simply looking at them. By leveraging the user's line of sight as a heuristic of gaze and attention, the InSight system directs input focus from input devices to the device that the user is looking at, thus creating an intuitive metaphor: you control the object you are looking at. In this paper, we contribute with technical details of the hardware and software implementation, and a discussion of single user and multi-user interaction possibilities.",
    "keywords": [
      "gaze based interactions",
      "home automation",
      "sensors"
    ],
    "doi": "10.1145/3041164.3041195",
    "url": "https://doi.org/10.1145/3041164.3041195",
    "citations": 6,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "26",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3041164.3041196",
    "title": "Disambiguating touch with a smart-ring",
    "authors": [
      "Andrea Bianchi",
      "Seungwoo Je"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Capacitive touchscreens have changed the way in which people interact with computational devices. In fact, direct touch input on screens is immediately understandable and appealing to both novice and advanced users and, more importantly, it leverages people's natural ability to use multiple fingers for input gestures. However, currently off-the-shelves touchscreens are unable to disambiguate among different fingers or to determine whether different touch-points belong to the same user, reducing the expressiveness of finger touches to that of multiple pointers. In this paper, we propose to augment human touch using a smart-ring. When the finger wearing the ring is in contact with the touchscreen, a unique ID is transmitted through vibration patterns from the ring to the touched device. It therefore becomes possible to distinguish touch between different users, or to associate different meanings to different touches for the same user. In this paper, we explore this design space and present a set of applications to demonstrate the feasibility of this technique.",
    "keywords": [
      "augmentation",
      "smart-ring",
      "touch disambiguation",
      "wearable"
    ],
    "doi": "10.1145/3041164.3041196",
    "url": "https://doi.org/10.1145/3041164.3041196",
    "citations": 12,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "27",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041197",
    "title": "Trajectory adjustment system for learning based on electrical stimulation",
    "authors": [
      "Sho Tatsuno",
      "Tomohiko Hayakawa",
      "Masatoshi Ishikawa"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Electrical stimulation is a well-known technology in medicine used for stimulating human muscles that has been applied in rehabilitation. Recently, electrical stimulation has been spotlighted for generating haptic sensations in human interface applications. Existing methods of generating haptic sensations are mainly mechanical. However, electrical stimulation can generate much stronger stimulation than mechanical force generators with the same energy. In this study, we consider applications of electrical stimulation to skill learning. Through simple tasks to learn trajectory, we assessed the learning rate using electrical stimulation compared with learning using vibration.",
    "keywords": [],
    "doi": "10.1145/3041164.3041197",
    "url": "https://doi.org/10.1145/3041164.3041197",
    "citations": 7,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "28",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041198",
    "title": "Intuitive visualization method for locating off-screen objects inspired by motion perception in peripheral vision",
    "authors": [
      "Shizuko Matsuzoe",
      "Shan Jiang",
      "Miwa Ueki",
      "Keiju Okabayashi"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This paper describes an intuitive user interface (UI) that guides and attracts a user's attention toward off-screen objects using a vibrating icon. Mobile augmented reality is often used for finding various annotations that are overlaid on a real world image, such as operating instructions for maintenance and inspection. However, owing to the limited scope of the device screen, it is difficult for the user to convey annotations located off-screen. In this work, we focused on motion perception and cognition properties and attempted to show off-screen information intuitively using a simple approach. To achieve the intuitive off-screen information display, we used the motion of a small, circular, vibrating icon. Experiments with tasks involving locating an off-screen target and memorizing numbers were conducted to evaluate the usability of the proposed guidance UI. The results show that users using the proposed UI can locate a target more quickly, memorize numbers more precisely, and provided a high user satisfaction more easily, than with other conventional guidance UIs.",
    "keywords": [
      "guidance method",
      "motion perception",
      "off-screen",
      "peripheral vision",
      "visual field"
    ],
    "doi": "10.1145/3041164.3041198",
    "url": "https://doi.org/10.1145/3041164.3041198",
    "citations": 13,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "29",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041199",
    "title": "GrabAmps: grab a wire to sense the current flow",
    "authors": [
      "Don Samitha Elvitigala",
      "Roshan Peiris",
      "Erik Wilhelm",
      "Shaohui Foong",
      "Suranga Nanayakkara"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In this paper, we present GrabAmps, an intuitive interface that allows users to simply grab a bundled cable and get feedback on the AC (Alternating Current) current flow through it. Single phase and three phase AC was estimated with a regression model developed using principles of applied electromagnetism. This regression model is embedded into a standalone glove with a display attached at the rear so that the users can intuitively read the current consumption information. The users may configure the glove to detect AC single phase or AC three phase current using the same sensor setup on the glove. End users such as electrical engineers and electricians who frequently wear gloves during their work can benefit from GrabAmp to identify a wire that is live, or even grab and move along a wire to trace any potential failures. We believe GrabAmps can potentially speed up maintenance processes and monitor equipment more efficiently without downtime, which is increasingly important for data centres and other critical infrastructure.",
    "keywords": [
      "augmented glove",
      "intuitive interfaces",
      "non-invasive current sensing"
    ],
    "doi": "10.1145/3041164.3041199",
    "url": "https://doi.org/10.1145/3041164.3041199",
    "citations": 0,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "30",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041200",
    "title": "ACTUATE racket: designing intervention of user's performance through controlling angle of racket surface",
    "authors": [
      "Katsutoshi Masai",
      "Yuta Sugiura",
      "Maki Sugimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In this study, we introduce a novel way to interact with sports players during the activities through actuating sports equipment. We propose ACTUATE Racket, a new type of sports racket that can change its angle of the surface. We made the table tennis racket prototype that has two servo motors for controlling the angle of its surface. The goal of this research is to explore the design opportunity of the relationships between user and racket by creating new interactions with the racket. We investigate practice methods of racket sports introductory activity: bouncing a ball on the racket rhythmically. We present three approaches with the racket. (1) One approach is to stabilize the angle of the surface during the activity. (2) Another approach is to amplify the angle of the surface regardless of the grip angle while user bounces the ball. (3) The other approach is to intentionally change the surface of the racket for each swing so that the user have to adjust the angle of its surface. We report the results and insights and we wrap up the design opportunities of the device with future scenarios.",
    "keywords": [
      "augmented sports",
      "smart racket"
    ],
    "doi": "10.1145/3041164.3041200",
    "url": "https://doi.org/10.1145/3041164.3041200",
    "citations": 1,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "31",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041201",
    "title": "Synthesizing fixed point of views from a spinning omnidirectional ball camera",
    "authors": [
      "Masakazu Nakazawa",
      "Hideki Koike"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Recent years have seen the emergence of a new way to watch sports, that is, watching them from the ball's point of view (POV) taken with cameras embedded in the ball. The problem, however, is that the ball's POV is unstable since the ball spins. In this paper, we describe how we developed a prototype of the ball camera with an omnidirectional camera and propose an algorithm to fix the ball's POV to an arbitrary direction. Feature points are extracted from an entire omnidirectional image and the pose change between frames is estimated by feature matching and the five-point algorithm. The POV of the current frame is modified to the one of the initial frame by applying the product of estimated rotation matrices between all adjacent frames up to the current one. Our algorithm is not limited to our hardware and can be applied to any omnidirectional videos taken with different camera systems.",
    "keywords": [
      "ball camera",
      "digital sports",
      "omnidirectional camera",
      "pose estimation",
      "video synthesis",
      "virtual reality"
    ],
    "doi": "10.1145/3041164.3041201",
    "url": "https://doi.org/10.1145/3041164.3041201",
    "citations": 8,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "32",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041202",
    "title": "Stimulated percussions: method to control human for learning music by using electrical muscle stimulation",
    "authors": [
      "Ayaka Ebisu",
      "Satoshi Hashizume",
      "Kenta Suzuki",
      "Akira Ishii",
      "Mose Sakashita",
      "Yoichi Ochiai"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In musical performances, it is important to produce rhythms correctly. However, when beginners play musical instruments, it can be difficult for them to understand rhythms using only visual and auditory rhythm information. To solve this problem, we propose the Stimulated Percussions (SP) system, which generates rhythms on a computer and transfers them to a user's muscles. In this study, we control the user's arms and legs using electrical muscle stimulation (EMS). We attach electrodes near certain arm and leg muscles, and provide stimulation in a manner that allows users to reproduce the correct movement when they play instruments. Our system enables a single player or multiple players to correctly reproduce generated rhythms. Experimental results show that our system is useful for beginners learning musical instruments, because it allows accurate rhythms to be mastered through bodily sensations.",
    "keywords": [
      "electrical muscle stimulation (EMS)",
      "musical performance",
      "rhythm learning"
    ],
    "doi": "10.1145/3041164.3041202",
    "url": "https://doi.org/10.1145/3041164.3041202",
    "citations": 40,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "33",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3041164.3041203",
    "title": "HandshakAR: wearable augmented reality system for effortless information sharing",
    "authors": [
      "Mihai Bâce",
      "Gábor Sörös",
      "Sander Staal",
      "Giorgio Corbellini"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "When people are introduced to each other, exchanging contact information happens either via smartphone interactions or via more traditional business cards. Crowded social events make it more challenging to keep track of all the new contacts. We introduce HandshakAR, a novel wearable augmented reality application that enables effortless sharing of digital information. When two people share the same greeting gesture (e.g., shaking hands) and are physically close to each other, their contact information is effortlessly exchanged. There is no instrumentation in the environment required, our approach works on the users' wearable devices. Physical proximity is detected via inaudible acoustic signals, hand gestures are recognized from motion sensors, the communication between devices is handled over Bluetooth, and contact information is displayed on smartglasses. We describe the concept, the design, and an implementation of our system on unmodified wearable devices.",
    "keywords": [
      "contact exchange",
      "gesture recognition",
      "proxemics",
      "wearable"
    ],
    "doi": "10.1145/3041164.3041203",
    "url": "https://doi.org/10.1145/3041164.3041203",
    "citations": 8,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "34",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3041164.3041204",
    "title": "Distortion in perceived size and body-based scaling in virtual environments",
    "authors": [
      "Nami Ogawa",
      "Takuji Narumi",
      "Michitaka Hirose"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "In this paper, we report findings pertaining to the size perception of objects and hands in a Virtual Environment(VE). First, we found that size perception is distorted in a VE and the effect is different between objects and hands. We perceive our virtual hands as larger and objects as smaller in VEs than in real environments(REs). However, when hands interact with objects, our body is used as a metric to scale the apparent sizes of objects in the environment (body-based scaling; BBS). We also found that not only does the size of our hands influence the perceived size of the environment, but the size of familiar-sized objects influences the perceived size of our hands as well. In summary, in contrast to the independent traits of size perception of hands and objects in VEs, we tend to perceive the size based on what we see at first, either hands or objects, when we interact with the objects. These findings provide a benchmark for scale adjustment for interactive scale-sensitive virtual reality applications so as to create perceptually more precise representations of virtual objects and bodies.",
    "keywords": [
      "body-based scaling",
      "embodied perception",
      "size perception"
    ],
    "doi": "10.1145/3041164.3041204",
    "url": "https://doi.org/10.1145/3041164.3041204",
    "citations": 32,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "35",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3041164.3041212",
    "title": "Frustration suppression for wellbeing in decision making",
    "authors": [
      "Makoto Mizukami",
      "Kazuma Aoyama",
      "Masahiro Furukawa",
      "Taro Maeda",
      "Hideyuki Ando"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "This paper assesses subjective and behavioral responses against supraliminal and subliminal visual stimuli during decision making, aiming to enhance human psychological wellbeing. We conducted an experiment in which we assessed variations of response time and subjective task load evaluation, changing visual stimuli. The paper concludes that subjective strength of frustration depends on an amount of attention attraction.",
    "keywords": [
      "decision making",
      "task load",
      "wellbeing"
    ],
    "doi": "10.1145/3041164.3041212",
    "url": "https://doi.org/10.1145/3041164.3041212",
    "citations": 1,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "36",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041213",
    "title": "Towards soft landing in an online dating service: bridging the ideal-real gap",
    "authors": [
      "Takuya Iwamoto",
      "Kazutaka Kurihara",
      "Maya Esora",
      "Kazushi Nishimoto"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "With the penetration of the internet, the popularity of online dating services has surged, beginning with the United States as one of the main markets. Previous research has shown that the responses to the online profile increase with the attractiveness of the profile picture. This has led some users, particularly women, to use photo manipulating applications to beautify their photos. However, online dating eventually leads to meetings in person, and beautified pictures can generate negative impressions when the user is not as appealing in real life as in the profile images. This paper proposes a solution where, over time and with frequent interaction, users' profile photos are reversed to the original versions before heavy editing. Experiments have proven the potential of this method as test subjects could barely perceive the gradual change in profile pictures. Implementation of this method in dating services can prevent negative impressions that arise due to the gap between the beautified picture and the real life appearance and even allows for a better first impression.",
    "keywords": [
      "dating science",
      "love",
      "online dating"
    ],
    "doi": "10.1145/3041164.3041213",
    "url": "https://doi.org/10.1145/3041164.3041213",
    "citations": 1,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "37",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041208",
    "title": "5th limb: development of augmented expression system using extra limb",
    "authors": [
      "Shori Kano",
      "Rintaro Takashima",
      "Yasunari Asakura",
      "Ryoichiro Shiraishi"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "Technology for augmenting expressions is actively researched in fields such as social media, entertainment, and arts. It is being established as a business. Dancing to videos that are programmed in advance is one example. This method effectively augments the expressions in dancing; however, it is difficult to improvise. Although it can be interactive, it requires large-scale devices such as motion captures or a large number of cameras. The purpose of this study is to develop a wearable device that augments expressions and evaluate the effectiveness of this device for augmentation of expressions. The shape of this device is identical to the tail of an animal. The shape of the device changes using information from the body of the wearer and acts as a \"5th\" limb. It then augments the expressions of the wearer. The required information for changing shape includes back muscle tension, gait, and angle of the arm. This information is collected through measurements and estimates of bioelectrical signals, sole pressure, and angles of the arms. The device estimates the emotions and behavior of the user. In the experiments, the device is attached to a performer. The values of the sensor and the motions of the device are measured. Hence, it is confirmed that the device moves when it detects the behavior of the performer. In other words, the device mirrors the behavior of the people who use the system. Therefore, this system augments the expression of performers. In conclusion, a wearable device for augmenting expressions is developed, and it is considered that this system can augment the expression of performers on stage.",
    "keywords": [
      "artificial limb",
      "augmented expression",
      "human sensing technology",
      "wearable system"
    ],
    "doi": "10.1145/3041164.3041208",
    "url": "https://doi.org/10.1145/3041164.3041208",
    "citations": 0,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "38",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041209",
    "title": "A body odyssey: exploring the human body as digested food",
    "authors": [
      "Satoshi Fujisawa",
      "Takeo Hamada",
      "Ryota Kondo",
      "Ryohei Okamoto",
      "Michiteru Kitazaki"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We developed a full-body telepresence system through which users can crawl to travel through the human body, and gain insight into the structures of digestive organs through visual, tactile, and auditory sensations. Digestive organs (esophagus, stomach, duodenum, small intestine, large intestine, rectum) are created in detail with 3-D images so that users can view them using a head-mounted display, and an appropriate feeling of touch is presented to the abdomens and thighs of users by vibrotactile devices. Pressure sensors detect the crawling action of users so that they can travel in the digestive organs while hearing realistic 3-D sounds. Participants feel as if their own body becomes smaller while moving inside of another person's body. Thus, this system gives users the sensation of a reduced body size and provides a telepresence at improbable locations such as the stomach. This system can serve as an interactive, educational experience by simulating the contents of the human digestive system.",
    "keywords": [
      "crawling",
      "education",
      "locomotion interface",
      "multimodal sensation",
      "vibrotactile stimulus"
    ],
    "doi": "10.1145/3041164.3041209",
    "url": "https://doi.org/10.1145/3041164.3041209",
    "citations": 3,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "39",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3041164.3041210",
    "title": "Video segmentation and stabilization for BallCam",
    "authors": [
      "Ryohei Funakoshi",
      "Vishnu Naresh Boddeti",
      "Kris Kitani",
      "Hideki Koike"
    ],
    "year": 2017,
    "conference": "AH",
    "conferenceYear": "AH '17",
    "abstract": "We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow.",
    "keywords": [
      "BallCam",
      "activity segmentation",
      "video stabilization"
    ],
    "doi": "10.1145/3041164.3041210",
    "url": "https://doi.org/10.1145/3041164.3041210",
    "citations": 1,
    "booktitle": "Proceedings of the 8th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Silicon Valley, California, USA",
    "articleno": "40",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174911",
    "title": "Air Mounted Eyepiece: Optical See-Through HMD Design with Aerial Optical Functions",
    "authors": [
      "Kazuki Otao",
      "Yuta Itoh",
      "Kazuki Takazawa",
      "Hiroyuki Osone",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "We propose a novel method of implementing an optical see-through (OST) head-mounted display (HMD) with a wide viewing angle and high resolution for augmented reality, called an Air Mounted Eyepiece (AME). In past years, many optical elements, such as transmissive liquid-crystal display (LCD), half-mirror, and waveguide have been adopted for OST-HMD. To achieve the AME design, we employ an off-the-shelf HMD and Transmissive Mirror Device (TMD), which is used in aerial real-imaging systems, instead of conventional optical elements. In the proposed method, we present 'Virtual lens,\" which has the same function as the HMD lens in front of the eyes. By using TMD, it is possible to shorten the optical length between the virtual lens and the eye. Therefore, the aerial lens provides an immersive image with see-through capability. In this paper, we describe a detailed design method of TMD-based HMD, and compare it to previous half mirror-based HMD and convex mirror-based HMD. Then, we construct a fabricated prototype of the OST-HMD using TMD. We aim to contribute to the field of human-computer interaction and the research on eyepiece interfaces by discussing the advantages and the limitations through simulations and experiments.",
    "keywords": [
      "Transmissive Mirror Device",
      "Optical See-Through Display",
      "Near-Eye Display",
      "Augmented Reality"
    ],
    "doi": "10.1145/3174910.3174911",
    "url": "https://doi.org/10.1145/3174910.3174911",
    "citations": 21,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "1",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174918",
    "title": "Investigation of Tracer Particles Realizing 3-Dimensional Water Flow Measurement for Augmented Swimming Training",
    "authors": [
      "Shogo Yamashita",
      "Shunichi Suwa",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Previous studies have revealed that understanding the 3D movement of water contributes to improving propulsion in the water when swimming. Fluid measurements are made by scattering tracer particles into a liquid, and cameras track the movement of these particles to measure the fluid's flow. Strong lasers illuminate tracer particles to make them visible to cameras, but 3D water flow measurements in wide spaces like swimming pools have not yet been successful. This can be owed to the limitations of current optical systems impacting the measurable parameters of existing methods, such as the laser capacity and lens size. Moreover, visualized tracer particles buoyed in swimming pools can affect the swimmer's view and may be harmful to humans when swallowed. Therefore, we propose a 3D water flow tracing technology with tracer particles suitable for a swimming pool. We use an optical property called optical rotation to track the tracer particles. This method would be effective in extending the measurable area of water flow than previous methods because it does not require the use of optical systems, which are technically difficult to expand. In this study, we investigated the materials and processing methods for creating tracer particles for augmented swimming training.",
    "keywords": [
      "Underwater Virtual Reality",
      "Underwater Spatial Interaction",
      "Swimming",
      "Fluid Measurement",
      "Augmented Sports"
    ],
    "doi": "10.1145/3174910.3174918",
    "url": "https://doi.org/10.1145/3174910.3174918",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "2",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174930",
    "title": "Couples Designing their Living Room Together: a Study with Collaborative Handheld Augmented Reality",
    "authors": [
      "Joon Gi Shin",
      "Gary Ng",
      "Daniel Saakes"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we investigate the use of Augmented Reality for users designing together. We present a design application that runs on multiple synchronized and spatially aware tablets to support couples making interior decisions in and for their future living room. Based on the prior art into collaborative design, we suggest a novel design interface that deals with situated design and supports virtual workspaces. We asked six couples to design together with our prototype and analyzed their design process, the roles they took, and how they communicated. The results suggest that the social practice of couples designing in and for their home differs from professional design teams and involves more than just positioning furniture in space. We use the design, the prototype and the study to discuss implications for spatial in-situ tools concerning intimacy, collaboration, and design process. The findings are useful for future applications that deal with collaborative applications for casual users.",
    "keywords": [
      "User Study",
      "Multi User",
      "Interior Design",
      "Augmented Reality"
    ],
    "doi": "10.1145/3174910.3174930",
    "url": "https://doi.org/10.1145/3174910.3174930",
    "citations": 16,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "3",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174920",
    "title": "Augmenting Memory Recall in Work Meetings: Establishing a Quantifiable Baseline",
    "authors": [
      "Evangelos Niforatos",
      "Matías Laporte",
      "Agon Bexheti",
      "Marc Langheinrich"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "The proliferation of ubiquitous technologies has allowed us to capture increasing amounts of our daily life in digital format (\"lifelogging\"). While much work in lifelogging has focused on augmenting human memory in clinical settings, e.g., for people with dementia, the ability to recall our past obviously has also importance in one's work life. Being able to better recall a work meeting could improve coordination and collaboration among peers, ultimately raising overall productivity. To better understand the potential of memory augmentation technology for improving work meetings, we conducted a multi-week study with seven groups. Using a within-subjects design, participants in the experimental condition were, prior to a meeting, briefly presented with an automatically created memory augmentation aid (slides), based on captured data from a prior meeting. Our results show that a 3-4 minute exposure to our simple image-keywords slide deck prior to a meeting, increased our participants' ability to recall their previous meeting by up to 15 %. Our findings serve as an initial baseline against which future memory augmentation systems can be compared.",
    "keywords": [
      "Work Meetings",
      "Lifelogging",
      "Human Memory Augmentation"
    ],
    "doi": "10.1145/3174910.3174920",
    "url": "https://doi.org/10.1145/3174910.3174920",
    "citations": 9,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "4",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174946",
    "title": "Virtual Participation in Ukiyo-e Appreciation using Body Motion",
    "authors": [
      "Shota Kusajima",
      "Takuya Takahashi",
      "Yasuyuki Sumi"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "This paper presents a novel method of art appreciation by participating in artwork using virtual reality technology. We chose a famous ukiyo-e work titled \"many people are assembled to create one good person\" drawn by Kuniyoshi Utagawa in the Edo period of Japan. The painting provides an illusion that it seems a one man but shows many (over ten) persons assembled to create the one person. We developed a 3D CG game where its player can join to create the big person by virtually participating into the artwork as the part of the big person. The system enabled its players to enjoy the game with their own body movements, which extends their experience of art appreciation. This paper describes an attempt to enrich our game players' experience under very limited temporal and spatial constraints given at a contest exhibition.",
    "keywords": [
      "Virtual reality",
      "Content design",
      "Bodily kinesthetic game",
      "Art appreciation"
    ],
    "doi": "10.1145/3174910.3174946",
    "url": "https://doi.org/10.1145/3174910.3174946",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "5",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174919",
    "title": "AR Timewarping: A Temporal Synchronization Framework for Real-Time Sensor Fusion in Head-Mounted Displays",
    "authors": [
      "Peter Kim",
      "Jason Orlosky",
      "Kiyoshi Kiyokawa"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "A significant issue associated with the use of head-mounted displays for augmented reality is the presence of latency between the real world and the augmented images displayed to the headset, which, if uncompensated for, results in registration error that may limit the effectiveness of the augmented information as well as bring about user discomfort. In addition, further temporal discrepancies arise in the case when fusing information from multiple cameras of different capture frequencies to construct the augmented image as, in addition to temporal misalignment between the real world and the augmented image, there is desynchronization among the different sensors that may also lead to registration mismatch.In order to address these temporal inconsistencies, we present AR Timewarping, a novel temporal synchronization framework that is particularly adapted for video see-through (VST) head-mounted displays and consists of two main algorithms, one for head motion and one for scene motion, that together act to temporally warp and merge information from multiple sensors to significantly improve registration in the augmented image.System tests of our algorithms show that we can reduce registration error between two unsynchronized video streams by 87.04% and 81.64% for registration error arising from head motion and scene motion, respectively. Results from a user experiment show that subjects' abilities to track a moving object were significantly improved by our algorithms, with a 32.14% average reduction in angular tracking error, and furthermore subjects rated the combined algorithms better overall than the base case in terms of both image clarity and user comfort.",
    "keywords": [
      "sensor fusion",
      "delay compensation",
      "augmented reality",
      "Vision augmentation"
    ],
    "doi": "10.1145/3174910.3174919",
    "url": "https://doi.org/10.1145/3174910.3174919",
    "citations": 5,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "6",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174914",
    "title": "Telewheelchair: the Remote Controllable Electric Wheelchair System combined Human and Machine Intelligence",
    "authors": [
      "Satoshi Hashizume",
      "Ippei Suzuki",
      "Kazuki Takazawa",
      "Ryuichiro Sasaki",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Wheelchairs are essential means of transport for the elderly people and the physically challenged. However, wheelchairs need to be accompanied by caregivers. As society ages and the number of care recipients increases, the burden on caregivers is expected to increase. In order to reduce the burden on caregivers, we present Telewheelchair, an electric wheelchair equipped with a remote control function and computational operation assistance function. The caregiver can remotely control the Telewheelchair by means of a head mounted display (HMD). In addition, the proposed system is equipped with a human detection system to stop the wheelchair automatically and avoid collisions. We conducted a user study on the wheelchair in four types of systems and investigated the time taken to achieve tasks. Telewheelchair will enhance geriatric mobility and improve society by combining human intelligence and machine intelligence.",
    "keywords": [
      "virtual reality",
      "telepresence",
      "nursing",
      "Wheelchair"
    ],
    "doi": "10.1145/3174910.3174914",
    "url": "https://doi.org/10.1145/3174910.3174914",
    "citations": 20,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "7",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174940",
    "title": "Command Selection in Gaze-based See-through Virtual Image-Guided Environments",
    "authors": [
      "Hoorieh Afkari",
      "David Gil de Gómez Pérez",
      "Roman Bednarik"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Embedded close-to-the-eye gaze tracking permits new types of interaction in see-through augmented and virtual environments. It is however unclear how gaze-input can be used to select and confirm commands when the tracking technologies are located at close proximity to user's eyes, but cannot utilize fixed geometry as in screen-based environments. We conducted a study in a simulated image-guided medical environment where users employed gaze-input to control an on-screen display. The current hand-based interaction of such views is a frequent source of interruption and thus feasibility of alternative input modalities has to be evaluated. We created a three-stage gaze-based confirmation mechanism and evaluated its robustness and the limits of the target size. Two sizes of the target for command selection were evaluated, occupying 12 and 6 degrees of visual angle at the 30cm distance. The results show the time to perform an action using gaze input is shorter than in hand-based interaction with the real-world device, confirming that this input modality is feasible. The size of target has little effect on the interaction and the completion-error is low. The findings have implications on the design of future gaze-based input methods for these devices.",
    "keywords": [
      "surgical microscope",
      "surgical image-guided techniques",
      "gaze-based interaction",
      "VR",
      "Gaze interaction"
    ],
    "doi": "10.1145/3174910.3174940",
    "url": "https://doi.org/10.1145/3174910.3174940",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "8",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174935",
    "title": "HIVE Tracker: a tiny, low-cost, and scalable device for sub-millimetric 3D positioning",
    "authors": [
      "Darío R. Quiñones",
      "Gonçalo Lopes",
      "Danbee Kim",
      "Cédric Honnet",
      "David Moratal",
      "Adam Kampff"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Positional tracking systems could hugely benefit a number of niches, including performance art, athletics, neuroscience, and medicine. Commercial solutions can precisely track a human inside a room with sub-millimetric precision. However, these systems can track only a few objects at a time; are too expensive to be easily accessible; and their controllers or trackers are too large and inaccurate for research or clinical use. We present a light and small wireless device that piggybacks on current commercial solutions to provide affordable, scalable, and highly accurate positional tracking. This device can be used to track small and precise human movements, to easily embed custom objects inside of a VR system, or to track freely moving subjects for research purposes.",
    "keywords": [
      "Wireless-Sensor",
      "Virtual Reality",
      "Tracker",
      "Open Source",
      "Neuroscience",
      "Motion Capture",
      "Low cost",
      "Indoor"
    ],
    "doi": "10.1145/3174910.3174935",
    "url": "https://doi.org/10.1145/3174910.3174935",
    "citations": 10,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "9",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174938",
    "title": "A Motion Recognition Method Using Foot Pressure Sensors",
    "authors": [
      "Ayumi Ohnishi",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes a method for recognizing postures and gestures using foot pressure sensors, and we investigate optimal positions for pressure sensors on soles are the best for motion recognition. In experiments, the recognition accuracies of 22 kinds of daily postures and gestures were evaluated from foot-pressure sensor values. Furthermore, the optimum measurement points for high recognition accuracy were examined by evaluating combinations of two foot pressure measurement areas on a round-robin basis. As a result, when selecting the optimum two points for a user, the recognition accuracy was about 93.6% on average. Although individual differences were seen, the best combinations of areas for each subject were largely divided into two major patterns. When two points were chosen, combinations of the near thenar, which is located near the thumb ball, and near the heel or point of the outside of the middle of the foot were highly recognized. Of the best two points, one was commonly the near thenar for subjects. By taking three points of data and covering these two combinations, it will be possible to cope with individual differences. The recognition accuracy of the averaged combinations of the best two combinations for all subjects was classified with an accuracy of about 91.0% on average. On the basis of these results, two types of pressure sensing shoes were developed.",
    "keywords": [
      "Shoes device",
      "Pressure sensor",
      "Posture recognition",
      "Insole",
      "Gesture recognition",
      "Foot Pressure"
    ],
    "doi": "10.1145/3174910.3174938",
    "url": "https://doi.org/10.1145/3174910.3174938",
    "citations": 13,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174913",
    "title": "Design and Study of a Multi-Channel Electrical Muscle Stimulation Toolkit for Human Augmentation",
    "authors": [
      "Michinari Kono",
      "Yoshio Ishiguro",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Electrical Muscle Stimulation (EMS) has attracted many users and researchers to apply the technique for various usages. However, EMS products for research purpose are limited and open source hardware still has limitations. We present a multi-channel EMS toolkit for researchers and designers to develop their original ideas. The toolkit was designed to have isolated multiple channels to be manipulated simultaneously, which allows simultaneous control of multiple body parts and multiple users. We organized a workshop with a theme of human augmentation, where EMS was not compelled to use. As a result, several groups found interest in using EMS for their development, and participants successfully used our toolkit for their presentation. We found that multi-channel EMS has a significant demand for human augmentation purposes.",
    "keywords": [
      "Wearable",
      "Toolkit",
      "Prototyping",
      "Human Augmentation",
      "Haptic Feedback",
      "Electrical Muscle Stimulation"
    ],
    "doi": "10.1145/3174910.3174913",
    "url": "https://doi.org/10.1145/3174910.3174913",
    "citations": 22,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "11",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174927",
    "title": "Sync Class: Visualization System for In-Class Student Synchronization",
    "authors": [
      "Katsuya Fujii",
      "Plivelic Marian",
      "Dav Clark",
      "Yoshi Okamoto",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "The engagement of students in a classroom is an important factor in education. Teachers are not only expected to possess strong teaching skills but also be simultaneously attentive to their students. Researchers have found that the most effective teaching happens when students in the room are in sync with one another.[1]However, assessing the classroom environment is not always easy for teachers especially when they teach multiple students at a time. The number of students per classroom can vary from a handful of student to over 50, this in turn can cause difficulty for teachers to provide an equal amount of attention per student, or even to be clearly attentive to the entire class environment.In this paper, we introduce an intelligent support system called \"Sync Class\" that helps teachers have access to a quantitative representation of how much students are in sync in a classroom. The system has a web camera placed in front of the classroom observing each student's face to detect students' engagement with the teacher. The system visualizes students' engagement in real time and alternatively, the system can also be used to analyze each student's attentiveness after class.We conducted several experiments to assess and evaluate the accuracy and capability of the system while also establishing usability for teachers. The teachers who tested our system, especially novice teachers, find it helpful to better understand student engagement whilst teaching. They are eager to utilize the system in future classes to respond to students, especially those who are disengaged, and better support their learning by effectively and promptly deploying interventions when needed.",
    "keywords": [
      "User Interface",
      "Human Computer Interaction",
      "Education",
      "Classroom"
    ],
    "doi": "10.1145/3174910.3174927",
    "url": "https://doi.org/10.1145/3174910.3174927",
    "citations": 23,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "12",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174926",
    "title": "AVATAREX: Telexistence System based on Virtual Avatars",
    "authors": [
      "Timo Koskela",
      "Mounib Mazouzi",
      "Paula Alavesa",
      "Minna Pakanen",
      "Ilya Minyaev",
      "Eero Paavola",
      "Jere Tuliniemi"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "The telexistence technology can provide many kinds of benefits for the society. These include new ways of remote work, empowerment of handicapped and elderly people, and creation of new immersive and environmentally-friendly forms of tourism, travel, shopping, sports and leisure time activities. In this paper, we introduce AVATAREX, a telexistence system based on virtual avatars. AVATAREX provides means for connecting users that are simultaneously occupying the same space in the real world and its virtual replica. Using an indoor prototype implementation of AVATAREX and a simple collaborative game, we investigated how users experience co-presence in a telexistence system based on virtual avatars and measured the performance of AVATAREX on high-end smart glasses. Based on our findings, users wearing virtual reality gear reported a stronger sense of co-presence compared to users wearing augmented reality gear. Unexpectedly, users wearing smart glasses reported a lower sense of co-presence than users using a tablet for augmented reality experience.",
    "keywords": [
      "virtual reality",
      "user study",
      "mixed reality",
      "augmented reality"
    ],
    "doi": "10.1145/3174910.3174926",
    "url": "https://doi.org/10.1145/3174910.3174926",
    "citations": 12,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "13",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174937",
    "title": "V8 Storming: How Far Should Two Ideas Be?",
    "authors": [
      "Yui Kita",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "All innovative ideas are said to be pairings of existing ideas. In collaborative ideation, various methods are used to suggest words for stimulating ideation based on the theory that suggesting strongly related words to the ideation topic will activate and enhance the ideation. However, the relationship between the words suggested by the system and the words that users associate remains unclear. Solving this issue will help selecting words suggestion in computer-supported ideation systems, in which the information flow is fluid and, therefore, users have strictly limited time for interaction. To address this issue, we studied the relationship between pairs of words suggested by computer and the quantity of associated ideas by users. We also conducted a user study on practical applications.",
    "keywords": [
      "Human Computer Interaction",
      "Collaborative Ideation",
      "Association"
    ],
    "doi": "10.1145/3174910.3174937",
    "url": "https://doi.org/10.1145/3174910.3174937",
    "citations": 9,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174922",
    "title": "Seeing is Smelling: Localizing Odor-Related Objects in Images",
    "authors": [
      "Sangyun Kim",
      "Junseok Park",
      "Junseong Bang",
      "Haeryong Lee"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Research on cross-modal associations between vision and olfaction has shown that odor perception can be strongly influenced by vision including odor-evoked objects. Recently, convolutional neural networks (CNNs), which is mainly applied to image recognition by learning image features, is driving the development of a visual recognition method with promising results. However, there is no attempts to recognition and localization of odor-related objects which allow us to facilitate detection of objects on scene when a congruent odor stimuli is released from scent device. The existing object localization methods require bounding-box annotation indicating the presence of object and location information in an image which is too costly because it is a time-consuming process. Image-level annotation, which indicates the presence of objects in an image, is easier to obtain than bounding-box annotation. In this work, we perform weakly supervised object localization using features of CNNs with only image-level annotation data to detect odor-related objects. We propose a method to classify and localize odor-related objects, and describe its implementation in detail. The experimental results indicate that its performance is comparable to previous CNNs for the classification of odor-related objects.",
    "keywords": [
      "Weakly supervised object localization",
      "Olfaction-enhanced multimedia",
      "Olfaction and vision cross-modal",
      "Convolutional neural networks"
    ],
    "doi": "10.1145/3174910.3174922",
    "url": "https://doi.org/10.1145/3174910.3174922",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "15",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174936",
    "title": "GlassPass: Tapping Gestures to Unlock Smart Glasses",
    "authors": [
      "MD. Rasel Islam",
      "Doyoung Lee",
      "Liza Suraiya Jahan",
      "Ian Oakley"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Wearable technologies such as smart-glasses can sense, store and display sensitive personal contents. In order to protect this data, users need to securely authenticate to their devices. However, current authentication techniques, such as passwords or PINs, are a poor fit for the limited input and output spaces available on wearables. This paper focuses on eyewear and addresses this problem with a novel authentication system that uses an alphabet of simple tapping patterns optimized for rapid and accurate input on the temples (or arms) of glasses. Furthermore, it explores how an eyewear display can support password memorization by privately presenting a visualization of entered symbols. A pair of empirical studies confirm that performance during input of both individual password symbols and full passwords is rapid and accurate. A follow-up session one week after the main study suggests using a private display to show entered password symbols effectively supports memorization.",
    "keywords": [
      "Visual feedback",
      "Usability",
      "Smart glasses",
      "Security",
      "PIN entry",
      "Memorability",
      "Authentication"
    ],
    "doi": "10.1145/3174910.3174936",
    "url": "https://doi.org/10.1145/3174910.3174936",
    "citations": 17,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "16",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174929",
    "title": "Open Palm Menu: A Virtual Menu Placed in Front of the Palm",
    "authors": [
      "Takumi Azai",
      "Mai Otsuki",
      "Fumihisa Shibata",
      "Asako Kimura"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Virtual and mixed realities make it possible to view and interact with virtual objects in a three-dimensional space. However, the location to display menus in three-dimensional space and the means of manipulating them are often problems. Existing studies developed methods of displaying a menu in the air or on the body. In this paper, a menu system is proposed that appears in front of the user's palm (of the non-dominant hand) when he/she opens that hand. The user employs the other hand (dominant hand) to then interact with the menu. Using the space around the body as opposed to projections onto actual limbs makes it possible to display more items in more varied layouts. Additionally, the user can control rendering of the menu by opening or closing the hand. Being adjacent to the palm of the open hand also enables the user to adjust the hand position to operate the menu more comfortably or to move the menu to an area where it is easier to view. In this study, we conducted an experiment to develop the menu design guidelines to ensure smooth menu operation, designed an optimal menu, and tested that menu by determining the ease of use in interacting with our modeling application.",
    "keywords": [
      "Virtual reality",
      "Mixed reality",
      "Menu",
      "Head-mounted display",
      "Gestural input"
    ],
    "doi": "10.1145/3174910.3174929",
    "url": "https://doi.org/10.1145/3174910.3174929",
    "citations": 30,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174944",
    "title": "Recognition and Feedback of Vowel Utterance with a Good Mouth Shape Based on Sensing Platysma Muscle Bulging",
    "authors": [
      "Yukihiro Nishimura",
      "Tomoko Hashida"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In public speaking, speakers are evaluated on verbal delivery and nonverbal delivery, and in particular, the mouth shape has an important role to support both of these. The mouth shape is mainly set during vowel utterance. We define the mouth shape, which can prompt the pronunciation of the speaker clearly and enrich the facial expression, as a good mouth shape in this research. The authors assume that a good mouth shape can be inferred from the bulging of the platysma muscle in the neck. We aim to support vowel utterances with a good mouth shape, and propose a system to recognize them. Specifically, we measure the uplift of the platysma muscle with photoreflectors and apply a machine learning method to implement a system to judge whether vowel utterances are being performed with a good shape. We conduct an accuracy measurement experiment of the proposed system and report the result. Finally, we describe the application that provides feedback of vowel utterances with a good mouth shape.",
    "keywords": [
      "Public Speech",
      "Presentation Training",
      "Mouth Shape",
      "Machine Learning"
    ],
    "doi": "10.1145/3174910.3174944",
    "url": "https://doi.org/10.1145/3174910.3174944",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "18",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174948",
    "title": "Wanding Through Space: Interactive Calibration for Electric Muscle Stimulation",
    "authors": [
      "Henning Pohl",
      "Kasper Hornbæk",
      "Jarrod Knibbe"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Electric Muscle Stimulation (EMS) has emerged as an interaction paradigm for HCI. It has been used to confer object affordance, provide walking directions, and assist with sketching. However, the electrical signals used for EMS are multi-dimensional and require expert calibration before use. To date, this calibration has occurred as a collaboration between the experimenter, or interaction designer, and the user/participant. However, this is time-consuming, results in sampling only a limited space of possible signal configurations, and removes control from the participant. We present a calibration and signal exploration technique that both enables the user to control their own stimulation and thus comfort, and supports exploration of the continuous space of stimulation signals.",
    "keywords": [
      "haptic feedback",
      "functional electrical stimulation",
      "calibration",
      "Electric muscle stimulation",
      "EMS"
    ],
    "doi": "10.1145/3174910.3174948",
    "url": "https://doi.org/10.1145/3174910.3174948",
    "citations": 14,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "19",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174916",
    "title": "GoalBaural: A Training Application for Goalball-related Aural Sense",
    "authors": [
      "Takahiro Miura",
      "Shimpei Soga",
      "Masaki Matsuo",
      "Masatsugu Sakajiri",
      "Junji Onishi",
      "Tsukasa Ono"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Goalball, one of the official Paralympic events, is popular with visually impaired people all over the world. The purpose of goalball is to throw the specialized ball, with bells inside it, to the goal line of the opponents as many times as possible while defenders try to block the thrown ball with their bodies. Since goalball players cannot rely on visual information, they need to grasp the game situation using their auditory sense. However, it is hard, especially for beginners, to perceive the direction and distance of the thrown ball. In addition, they generally tend to be afraid of the approaching ball because, without visual information, they could be hit by a high-speed ball. In this paper, our goal is to develop an application called GoalBaural (Goalball + aural) that enables goalball players to improve the recognizability of the direction and distance of a thrown ball without going onto the court and playing goalball. The evaluation result indicated that our application would be efficient in improving the speed and the accuracy of locating the balls.",
    "keywords": [
      "visually impaired people",
      "acoustical virtual reality",
      "Goalball"
    ],
    "doi": "10.1145/3174910.3174916",
    "url": "https://doi.org/10.1145/3174910.3174916",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "20",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174934",
    "title": "An Interactive 4D Vision Augmentation of Rapid Motion",
    "authors": [
      "Tao Tao",
      "Photchara Ratsamee",
      "Yuki Uranishi",
      "Kiyoshi Kiyokawa",
      "Tomohiro Mashita",
      "Haruo Takemura"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "We propose an interactive 4D visualization (3D space with an additional time dimension) for the purpose of understanding rapid motion in dynamic 3D information. With an interactive 4D visualization, the user can not only observe dynamic 3D motion from different viewpoints, but also freely adapt the speed of visualization on each viewpoint. We developed a system by reconstructing 3D data from an RGBD camera into a VR environment so that the user can visualize the 3D information via an HMD. The experiment results show that our proposed system outperforms conventional 2D and 3D visualizations in terms of both the user's recognition accuracy and view counts when observing rapid motion.",
    "keywords": [
      "Slow Motion",
      "Interactive 4D Visualization",
      "Augmented Reality",
      "3D Motion Data"
    ],
    "doi": "10.1145/3174910.3174934",
    "url": "https://doi.org/10.1145/3174910.3174934",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "21",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174923",
    "title": "A Detachable Exoskeleton Interface that Duplicates the User's Hand Posture and Motions",
    "authors": [
      "Genki Toyama",
      "Tomoko Hashida"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "We propose a technology that duplicates the posture and motions of a person's fingers and separates it from the original fingers, thereby enabling it to be used as a tool. In this study, we present a detachable exoskeleton interface that can record and replay the posture and motions of fingers using a micro servo motor and a telescopic mechanism. The micro servo motor is improved so that both the input and output of angles can be performed, and the telescopic mechanism has a variable length between joints. Wearing this system and recording the posture and motions of the fingers enabled us to test the mechanism for arranging and reproducing them in the real world independently of the user. In this paper, we describe the system implementation, an experiment to check its accuracy, two types of application, and a user study to check its usefulness.",
    "keywords": [
      "telescopic mechanism",
      "micro servo motor",
      "hand posture and motions",
      "Exoskeleton"
    ],
    "doi": "10.1145/3174910.3174923",
    "url": "https://doi.org/10.1145/3174910.3174923",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "22",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174924",
    "title": "FaceRubbing: Input Technique by Rubbing Face using Optical Sensors on Smart Eyewear for Facial Expression Recognition",
    "authors": [
      "Katsutoshi Masai",
      "Yuta Sugiura",
      "Maki Sugimoto"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "With the emergence of the wearable devices, the method to make use of the limited input space is required. This paper presents an input technique to a computer by rubbing face using optical sensors on smart eyewear. Since rubbing gesture occurs in daily life, our system enables a subtle interaction between the user and a computer. We used the smart eyewear based on the work by [5]. Although the device is developed for facial expression recognition, our method can recognize rubbing gesture independent from facial expression recognition.The embedded optical sensors measure the skin deformation caused by rubbing on the face. We detect the gestures using principal component analysis (PCA) and peak detection. we classify the area of the gesture with a random forest classifier. The accuracy of detecting rubbing gesture is 97.5%. The classification accuracy of 10 gesture area is 88.7% with user-independent training. The system can open up a new interaction method for smart glasses.",
    "keywords": [
      "Wearable Computing",
      "Input Technique",
      "Eyewear Computing"
    ],
    "doi": "10.1145/3174910.3174924",
    "url": "https://doi.org/10.1145/3174910.3174924",
    "citations": 19,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "23",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174933",
    "title": "A soft exoskeleton suit to reduce muscle fatigue with pneumatic artificial muscles",
    "authors": [
      "Kosuke Tsuneyasu",
      "Ayumu Ohno",
      "Yoshiyuki Fukuda",
      "Kazunori Ogawa",
      "Toshio Tsuji",
      "Yuichi Kurita"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "A major issue at work sites corresponds to aging workers, and the associated back pain. In this research, we develop an assistive suit with light-weight and flexible pneumatic rubber artificial muscles to reduce muscle load. Two assist forces are designed to control the artificial muscles with PWM-control based on: 1) flexion angle, and 2) estimated torque of the hip joint. The experimental results show that the proposed suit can reduce muscle activity during bending and stretching motions.",
    "keywords": [
      "pneumatic artificial muscle",
      "assistive suit",
      "artificial muscle"
    ],
    "doi": "10.1145/3174910.3174933",
    "url": "https://doi.org/10.1145/3174910.3174933",
    "citations": 12,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "24",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174921",
    "title": "Object-wise 3D Gaze Mapping in Physical Workspace",
    "authors": [
      "Kakeru Hagihara",
      "Keiichiro Taniguchi",
      "Irshad Abibouraguimane",
      "Yuta Itoh",
      "Keita Higuchi",
      "Jiu Otsuka",
      "Maki Sugimoto",
      "Yoichi Sato"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Understanding the intention of other people is a fundamental social skill in human communication. Eye behavior is an important, yet implicit communication cue. In this work, we focus on enabling people to see the users' gaze associated with objects in the 3D space, namely, we present users the history of gaze linked to real 3D objects. Our 3D gaze visualization system automatically segments objects in the workspace and projects user's gaze trajectory onto the objects in 3D for visualizing user's intention. By combining automated object segmentation and head tracking via the first-person video from a wearable eye tracker, our system can visualize user's gaze behavior more intuitively and efficiently compared to 2D based methods and 3D methods with manual annotation. We performed an evaluation of the system to measure the accuracy of object-wise gaze mapping. In the evaluation, the system achieved 94% accuracy of gaze mapping onto 40, 30, 20, 10-centimeter cubes. We also conducted a case study of through a case study where the user looks at food products, we showed that our system was able to predict products that the user is interested in.",
    "keywords": [
      "gaze mapping",
      "3D segmentation",
      "3D gaze"
    ],
    "doi": "10.1145/3174910.3174921",
    "url": "https://doi.org/10.1145/3174910.3174921",
    "citations": 8,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "25",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174928",
    "title": "Enhanced Pressure-Based Multimodal Immersive Experiences",
    "authors": [
      "Taeyong Kim",
      "Jeremy R. Cooperstock"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Haptic feedback to the feet has been explored in the context of ground surface simulation, but existing solutions have generally not taken into account the effects of variable foot pressure during naturalistic stepping movements. We present here our approach to simulating the surface of a frozen pond, including ice cracking under increased foot pressure. This serves as a compelling example of a wearable mobile augmented foot-based surface simulator, whose response varies as a function of applied foot pressure. To enhance the illusion, we render multi-sensory feedback, comprising audio, visual, and haptic effects. We describe the hardware employed, the algorithm used to determine the distribution of foot pressure, and suggest potential possibilities for such a foot-pressure-based augmented reality system.",
    "keywords": [
      "Wearable Computing",
      "Surface Textures",
      "Immersive Environment",
      "Haptic Effect",
      "Foot-based Interface",
      "Augmented Human"
    ],
    "doi": "10.1145/3174910.3174928",
    "url": "https://doi.org/10.1145/3174910.3174928",
    "citations": 6,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "26",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174912",
    "title": "VR Planning Toolkit to Simulate Physical and Virtual Configurations: A Case Study of an Indoor VR Roller Coaster Augmenting Experience",
    "authors": [
      "Dongsik Jo",
      "Yongwan Kim",
      "Woojin Jeon",
      "Yongsun Kim",
      "Hongki Kim",
      "Ki-Hong Kim",
      "Seungho Kwak"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes a novel VR planning toolkit to simulate physical configurations of the actual space such as physical sensors of user's surrounding space (e.g. tracking sensors, IoT devices, muscle interfaces, exoskeletons) in advance, and simultaneously create a VR content mixed for the configured environment. This toolkit provides guidelines for configurations of real and virtual spaces at the same time, and allows the user to bridging two spaces that need to be simulated. As a case study of our proposed VR planning toolkit, we have developed a prototype to test indoor VR roller coaster, and applied it as a usage scenario that can be experienced.",
    "keywords": [
      "simulation",
      "planning",
      "coaster",
      "authoring",
      "Virtual reality"
    ],
    "doi": "10.1145/3174910.3174912",
    "url": "https://doi.org/10.1145/3174910.3174912",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "27",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174943",
    "title": "Augmenting Smart Object Interactions with Smart Audio",
    "authors": [
      "Jing Yang",
      "Gábor Sörös"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "The auditory output channel is rather under-utilized in smart object to human communication. One reason is that in a smart environment, multiple overlapping audio sources can be disturbing to people. We propose a wearable audio augmentation system which allows people to effortlessly select and switch between sound sources given their interest. Our system leverages visual contact via the head pose as a measure of interest towards a smart object. We demonstrate a prototype implementation in three application scenarios and a preliminary user evaluation.",
    "keywords": [
      "Smart objects",
      "Human-object communication",
      "Audio augmentation"
    ],
    "doi": "10.1145/3174910.3174943",
    "url": "https://doi.org/10.1145/3174910.3174943",
    "citations": 1,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "28",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174939",
    "title": "Towards Enhancing Emotional Responses to Media using Auto-Calibrating Electric Muscle Stimulation (EMS)",
    "authors": [
      "Takashi Goto",
      "Benjamin Tag",
      "Kai Kunze",
      "Tilman Dingler"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "We evaluate the use of Electric Muscle Stimulation (EMS) as a method of amplifying emotional responses to multimedia content. This paper presents an auto-calibration method to stimulate two facial expressions using EMS. We focus on two expressions: frown and smile. We attempted control of facial muscles with facial feedback for automatically calibrating these facial expressions: our computer vision system detects the facial expression and auto-calibrates the EMS parameters (intensity and duration) based on the user's current facial expression. We present results from a pilot study with four participants evaluating the auto-calibration system and collecting initial feedback on the use of EMS to augment, for example, media experiences: while watching movies we can enhance the emotional response of the users during happy and sad scenes by stimulating corresponding face muscles.",
    "keywords": [
      "facial feedback",
      "emotion",
      "Electrical Muscle Stimulation (EMS)",
      "Affective computing"
    ],
    "doi": "10.1145/3174910.3174939",
    "url": "https://doi.org/10.1145/3174910.3174939",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "29",
    "numpages": "2",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174932",
    "title": "Reinforced Suit Using Low Pressure Driven Artificial Muscles For Baseball Bat Swing",
    "authors": [
      "Watura Sakoda",
      "Antonio Vega Ramirez",
      "Kazunori Ogawa",
      "Toshio Tsuji",
      "Yuichi Kurita"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes an assistive device to augment the sports activity. This experiment presents a bat swing augmentation suit for baseball that can provide automatically the assistive timing required to improve the performance of the user, this action is determined by the system. To generate the operation at the appropriate timing, an acceleration sensor, electric valves, and a microcomputer are attached on the suit. We conduct the experiment to confirm the performance of the suit by measuring the swinging speed. The experimental results showed that the device can improve the swinging speed of experienced subjects about 3 km/h.",
    "keywords": [
      "Smart Suit",
      "Powered Suit",
      "Pneumatic Artificial Muscle",
      "Human Extension",
      "Baseball"
    ],
    "doi": "10.1145/3174910.3174932",
    "url": "https://doi.org/10.1145/3174910.3174932",
    "citations": 7,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "30",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174931",
    "title": "Visual Field Visualizer: Easier &amp; Scalable way to be Aware of the Visual Field",
    "authors": [
      "Ngoc Thi Nguyen",
      "Suranga Nanayakkara",
      "Hyowon Lee"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Current practices in visual field tests require skilled eye care professionals and expensive machine setup in a controlled environment [1], thus, limit their availability within eye care clinics and hospitals. With a low-cost Virtual Reality (VR) device and its immersive capability, we developed Visual Field Visualizer (VFV), a prototype VR system that enables easy and scalable visual field screening. This paper aims to discuss our iterative design approach on user interfaces and usability improvement of visual field tests. Two pilot studies, conducted with 39 participants, received high users' acceptance.",
    "keywords": [
      "Visual field",
      "Virtual Reality",
      "VR Glasses",
      "Eye Screening",
      "Automated Perimetry",
      "Augmented Awareness"
    ],
    "doi": "10.1145/3174910.3174931",
    "url": "https://doi.org/10.1145/3174910.3174931",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "31",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174915",
    "title": "Exo-Balancer: Design Method of Personalized Stabilizers for Shooting Actions",
    "authors": [
      "Ryota Kawamura",
      "Kazuki Takazawa",
      "Riku Iwasaki",
      "Kenta Yamamoto",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In photography and videography, it is a huge challenge to align sight towards a moving target continuously and steadily often requiring considerable practice and experience. Blurry photos are often taken by camera users who lack requisite skills. To address this problem, stabilizers have been designed. Conventional stabilizers introduce a steep learning curve, because they are designed to be mass-produced and not tailored to the individual. Therefore, we present a design method of personalized stabilizers for shooting actions. Our system requires users to input their body data. Then, the system proposes the suitable position of the camera to be harnessed to the user considering the moments of force of both the user and camera.",
    "keywords": [],
    "doi": "10.1145/3174910.3174915",
    "url": "https://doi.org/10.1145/3174910.3174915",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "32",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174941",
    "title": "Restorative Effects of Exercise in Virtual Environments",
    "authors": [
      "Taekyu Kim",
      "Sangwon Lee"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Development of virtual reality devices cannot consider effects of exercise experiences by virtual environments. In this paper, we compared exercise experience by 3 environments (Indoor, Virtual urban, Virtual nature), focusing on restorative effects. We conducted the experiment that rode a bicycle while watching 360 degrees video in each environment. And then, participants conducted the survey about calmness and measured their inter-beat interval from heart-rate. The result showed that calmness is high in virtual nature environments and correlates with inter-beat interval. Our results demonstrate that restorative effects with exercise experiences can be appeared through embodiment with virtual nature. This paper suggests the importance of considering virtual environments and a possibility of application of natural environments to mixed or augmented reality.",
    "keywords": [
      "Virtual Reality Exercise",
      "Virtual Environments",
      "Restorative Effects",
      "Exercise Experience"
    ],
    "doi": "10.1145/3174910.3174941",
    "url": "https://doi.org/10.1145/3174910.3174941",
    "citations": 6,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "33",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174947",
    "title": "TouchSense: Classifying and Measuring the Force of Finger Touches with an Electromyography Armband",
    "authors": [
      "Vincent Becker",
      "Pietro Oldrati",
      "Liliana Barrios",
      "Gábor Sörös"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "We present TouchSense, a system to classify and to compute the force of finger touches using an inexpensive, off-the-shelf electromyography (EMG) armband. From EMG input only, we classify the finger touches and estimate the force applied when pressing an object or surface with the thumb, forefinger, or middle finger. We propose a novel neural network architecture for finger classification using EMG data. Our system runs in real time and only utilizes the Thalmic Labs Myo EMG armband and an Android smartphone, thereby being wearable and mobile. We showcase one application for our system, which controls the brightness of a lamp.",
    "keywords": [
      "Wearable Computing",
      "Touch-based interfaces",
      "Interaction",
      "EMG",
      "CNN"
    ],
    "doi": "10.1145/3174910.3174947",
    "url": "https://doi.org/10.1145/3174910.3174947",
    "citations": 15,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "34",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174951",
    "title": "Paralogue: A Remote Conversation System Using a Hand Avatar which Postures are Controlled with Electrical Muscle Stimulation",
    "authors": [
      "Shin Hanagata",
      "Yasuaki Kakehi"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we propose a new method of communication with a remote partner. The system, which we call \"Paralogue\" (parasitic + dialogue), utilizes the user's arm and a hand as an avatar that represents the remote conversation partner. This method is realized by controlling the movement of the user's arm with electrical muscle stimulations(EMS) and making the user's arm behave as if the remote conversation partner were speaking to the user. Therefore, it is possible to add the physical presence of the remote conversation partner to the situation. This can also be a telepresence system using real human arms. However, unlike many telepresence systems, external actuators and large-sized devices are not required for this system and can be completed by only utilizing physical interaction. In this paper, we present its design and implementation.",
    "keywords": [
      "Telepresence",
      "Hand Gesture",
      "Electrical Muscle Stimulation"
    ],
    "doi": "10.1145/3174910.3174951",
    "url": "https://doi.org/10.1145/3174910.3174951",
    "citations": 13,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "35",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174957",
    "title": "MOYA: Interactive AI toy for children to develop their language skills",
    "authors": [
      "Ji Yoon Ahn",
      "Dong Wan Kim",
      "Yong Hyeon Lee",
      "Woori Kim",
      "Jeong Kuk Hong",
      "Yeonbo Shim",
      "Jin Ho Kim",
      "Juhyun Eune",
      "Seong-Woo Kim"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "The main concept of MOYA, the interactive AI toy, is to provide an answer to the children's curiosity about an object's name. MOYA is developed on the basis of four functionalities: hand motion tracking, image classification, robot motion control and graphic user interface. Raspberry PI, Ubuntu Mate OS, Arduino, OpenCV, TensorFlow, RealSense SDK and ROS are integrated for MOYA to accomplish some interactive tasks.",
    "keywords": [
      "Motion control",
      "Interactive robot",
      "Image Classification Algorithm",
      "Hand motion tracking",
      "Deep Learning",
      "Artificial Intelligence"
    ],
    "doi": "10.1145/3174910.3174957",
    "url": "https://doi.org/10.1145/3174910.3174957",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "36",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174942",
    "title": "e-mmersive Book: the AR book that assists the syntopical reading",
    "authors": [
      "Shinhyo Kim",
      "Jihyun Park",
      "Jusub Kim"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Recently, researchers have actively explored ways to enhance reading experiences by utilizing Augmented Reality (AR) technologies. Augmenting a 3D virtual model on top of the real story book or science textbook is the typical way of using AR to enhance user experiences of reading activities. We present a new AR-based book system, e-mmersive Book, that assists the syntopical reading, where users read more than one book in an inspectional way to find answers to particular questions. Our new AR-based book system provides new possibilities to enhance reading experiences for students and researchers.",
    "keywords": [
      "Reading experience",
      "Augmented Reality",
      "AR books"
    ],
    "doi": "10.1145/3174910.3174942",
    "url": "https://doi.org/10.1145/3174910.3174942",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "37",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174955",
    "title": "Unconstrained Neck: Omnidirectional Observation from an Extra Robotic Neck",
    "authors": [
      "Lichao Shen",
      "Mhd Yamen Saraji",
      "Kai Kunze",
      "Kouta Minamizawa"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Humans are born with physiological limitations in terms of the sensory and motor abilities. Due to the narrow range of motion of the neck and the small visual field of the eyes, the human visual sense is limited in terms of the spatial range. We address this visual limitation by proposing a programmable neck that can leverage the range of motion limits. Unconstrained Neck, a head-mounted robotic neck, is a substitution neck system which provides a wider range of motion enabling humans to overcome the physical constraints of the neck. Using this robotic neck, it is possible to control the visual/motor gain which allows the user to thus control the range and speed of his effective neck motion or visual motion.",
    "keywords": [
      "vision expansion",
      "robotic neck",
      "human augmentation",
      "biomimetics"
    ],
    "doi": "10.1145/3174910.3174955",
    "url": "https://doi.org/10.1145/3174910.3174955",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "38",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174952",
    "title": "Distributed Metaverse: Creating Decentralized Blockchain-based Model for Peer-to-peer Sharing of Virtual Spaces for Mixed Reality Applications",
    "authors": [
      "Bektur Ryskeldiev",
      "Yoichi Ochiai",
      "Michael Cohen",
      "Jens Herder"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Mixed reality telepresence is becoming an increasingly popular form of interaction in social and collaborative applications. We are interested in how created virtual spaces can be archived, mapped, shared, and reused among different applications. Therefore, we propose a decentralized blockchain-based peer-to-peer model of distribution, with virtual spaces represented as blocks. We demonstrate the integration of our system in a collaborative mixed reality application and discuss the benefits and limitations of our approach.",
    "keywords": [
      "Telepresence",
      "Spatial Media",
      "Social Media",
      "Photospherical Imagery",
      "Mobile Computing",
      "Mixed Reality",
      "Groupware",
      "Blockchain"
    ],
    "doi": "10.1145/3174910.3174952",
    "url": "https://doi.org/10.1145/3174910.3174952",
    "citations": 77,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "39",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174954",
    "title": "Introducing Smart Pillow Using Actuator Mechanism, Pressure Sensors, and Deep Learning-Based ASR",
    "authors": [
      "Seung Hee Yang",
      "Sangwoo Park",
      "Taemyung Yang",
      "Ilhyung Jin",
      "Wooil Kim",
      "Chingwei Liu",
      "Seong-Woo Kim",
      "Juhyun Eune"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Sleeping is one of the most essential activities in human lives. It plays a vital role in good health and well-being of human lives. However, WHO (World Health Organization)'s research shows that approximately 27 percent of worldwide population are suffering from sleeping problems, which may affect everyday activities. Amongst various sleeping aid products, pillows can provide the solution to alleviate sleep disorders, neck pain, and chronic fatigue. The adjustable or smart pillows in today's market have some limitations. For instance, there are only few fixed forms and there is only one way interaction from human to the smart pillow. We further improve the existing smart pillow technology in terms of higher transformability and better human-pillow interactions which is bidirectional, thanks to the actuator mechanism, pressure sensors and ASR system.",
    "keywords": [
      "Smart Pillow",
      "Sleeping quality",
      "Pressure sensor",
      "Automatic speech recognition",
      "Augmented Human",
      "Actuator mechanism"
    ],
    "doi": "10.1145/3174910.3174954",
    "url": "https://doi.org/10.1145/3174910.3174954",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "40",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174945",
    "title": "Augmented Memory: Site-Specific Social Media with AR",
    "authors": [
      "Seonghun Park",
      "Jusub Kim"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we propose a new social media application which can allow one to save site-specific memories in virtual space and share them with other users. This application allows users to leave traces in visiting places avoiding damaging the physical places while satisfying the human desire to leave and share their traces. The AR-based site-specific social media application provides new opportunities for making social relationship.",
    "keywords": [
      "Social Media",
      "Mixed Reality",
      "Augmented Reality"
    ],
    "doi": "10.1145/3174910.3174945",
    "url": "https://doi.org/10.1145/3174910.3174945",
    "citations": 1,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "41",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174949",
    "title": "SmartFiber: Reconfigurable Shape Changing Interface",
    "authors": [
      "Masaru Ohkubo",
      "Takuya Nojima"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "As a way to augment shape and movement of a tangible object, we propose an actuator interface called \"SmartFiber\". Different from our previous work, SmartHair[7], this interface integrates an actuation and information network. The SmartFiber is composed of a silicon tube and two kinds of wires: (1) a pair of shape memory alloy wires and (2) copper wires. These copper wires contain a signal bus, which comprises an information network of shape changing interfaces. Within the network, actuators bend, shrink and rotate according to a control signal on the bus. By connecting to a power supply and sensor-integrated connectors, the user can compose a physically augmented object that has additional shape and movement. This system contributes to composing an augmented interface, which is a material-controllable extension of an object. The chain and mesh of the interface enable reconfiguration of the tangible object's functions by attaching new shapes and movements above its surface. In this paper we present a summary of the concept and the current prototype.",
    "keywords": [
      "Tangible",
      "SmartHair",
      "Hairlytop Interface",
      "Augmentation"
    ],
    "doi": "10.1145/3174910.3174949",
    "url": "https://doi.org/10.1145/3174910.3174949",
    "citations": 2,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "42",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174953",
    "title": "Automated Data Gathering and Training Tool for Personalized \"Itchy Nose\"",
    "authors": [
      "Juyoung Lee",
      "Hui-Shyong Yeo",
      "Thad Starner",
      "Aaron Quigley",
      "Kai Kunze",
      "Woontack Woo"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "In \"Itchy Nose\" we proposed a sensing technique for detecting finger movements on the nose for supporting subtle and discreet interaction. It uses the electrooculography sensors embedded in the frame of a pair of eyeglasses for data gathering and uses machine-learning technique to classify different gestures. Here we further propose an automated training and visualization tool for its classifier. This tool guides the user to make the gesture in proper timing and records the sensor data. It automatically picks the ground truth and trains a machine-learning classifier with it. With this tool, we can quickly create trained classifier that is personalized for the user and test various gestures.",
    "keywords": [
      "wearable computer",
      "subtle interaction",
      "smart eyewear",
      "smart eyeglasses",
      "online classification",
      "Training tool",
      "Nose gesture",
      "EOG"
    ],
    "doi": "10.1145/3174910.3174953",
    "url": "https://doi.org/10.1145/3174910.3174953",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "43",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174956",
    "title": "ToolShaker: Presentation Technique for \"as-is\" Display of Daily Commodities",
    "authors": [
      "Hayato Dogai",
      "Maho Oki",
      "Koji Tsukada"
    ],
    "year": 2018,
    "conference": "AH",
    "conferenceYear": "AH '18",
    "abstract": "Recently, various research projects have proposed presentation techniques for displaying products in our day-to-day environments. These techniques involve embedding sensors and actuators in the commonly used commodities. However, such methods have difficulty in maintain the usability of these everyday objects because of their effect on the product size and weight. In this study, we propose a presentation technique, namely, the TookShaker, which can physically control the daily used items \"as-is\" without attaching specific devices. We develop a prototype to control ferromagnetic objects (e.g., tools) placed on a wall surface by using electromagnets.",
    "keywords": [
      "electromagnet",
      "daily commodities",
      "and ambient display",
      "Display technique"
    ],
    "doi": "10.1145/3174910.3174956",
    "url": "https://doi.org/10.1145/3174910.3174956",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "44",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311831",
    "title": "TongueBoard: An Oral Interface for Subtle Input",
    "authors": [
      "Richard Li",
      "Jason Wu",
      "Thad Starner"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "We present TongueBoard, a retainer form-factor device for recognizing non-vocalized speech. TongueBoard enables absolute position tracking of the tongue by placing capacitive touch sensors on the roof of the mouth. We collect a dataset of 21 common words from four user study participants (two native American English speakers and two non-native speakers with severe hearing loss). We train a classifier that is able to recognize the words with 91.01% accuracy for the native speakers and 77.76% accuracy for the non-native speakers in a user dependent, offline setting. The native English speakers then participate in a user study involving operating a calculator application with 15 non-vocalized words and two tongue gestures at a desktop and with a mobile phone while walking. TongueBoard consistently maintains an information transfer rate of 3.78 bits per decision (number of choices = 17, accuracy = 97.1%) and 2.18 bits per second across stationary and mobile contexts, which is comparable to our control conditions of mouse (desktop) and touchpad (mobile) input.",
    "keywords": [
      "wearable devices",
      "subtle gestures",
      "silent speech interface",
      "oral sensing",
      "input interaction"
    ],
    "doi": "10.1145/3311823.3311831",
    "url": "https://doi.org/10.1145/3311823.3311831",
    "citations": 75,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "1",
    "numpages": "9",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/3311823.3311824",
    "title": "Estimation of Fingertip Contact Force by Measuring Skin Deformation and Posture with Photo-reflective Sensors",
    "authors": [
      "Ayane Saito",
      "Wakaba Kuno",
      "Wataru Kawai",
      "Natsuki Miyata",
      "Yuta Sugiura"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "A wearable device for measuring skin deformation of the fingertip---to obtain contact force when the finger touches an object---was prototyped and experimentally evaluated. The device is attached to the fingertip and uses multiple photo-reflective sensors (PRSs) to measures the distance from the PRSs to the side surface of the fingertip. The sensors do not touch the contact surface between the fingertip and the object; as a result, the contact force is obtained without changing the user's tactile sensation. In addition, the accuracy of estimated contact force was improved by determining the posture of the fingertip by measuring the distance between the fingertip and the contact surface. Based on the prototyped device, a system for estimating three-dimensional contact force on the fingertip was implemented.",
    "keywords": [
      "wearable device",
      "photo-reflective sensor",
      "force sensing"
    ],
    "doi": "10.1145/3311823.3311824",
    "url": "https://doi.org/10.1145/3311823.3311824",
    "citations": 14,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "2",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311832",
    "title": "2bit-TactileHand: Evaluating Tactons for On-Body Vibrotactile Displays on the Hand and Wrist",
    "authors": [
      "Don Samitha Elvitigala",
      "Denys J. C. Matthies",
      "Vipula Dissanayaka",
      "Chamod Weerasinghe",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Visual interfaces can provide a great density of information. However, the required focused visual attention results in a high cognitive effort. This cognitive load significantly increases when multiple tasks are performed that also require visual attention. In this paper, we evaluate the perceptual abilities of 2bit tactons on the wrist and the hand as a type of complementary feedback. Based on our evaluation, 2bit tactons are reasonably high perceivable (≈ 92%) at the hand distributed among several fingers. Additionally, the data concluded that vibrotactile feedback on hand is significantly more accurate than the wrist, which coincides with the subjects' preference. TactileHand's feasibility was demonstrated in three pilot studies, encoding ambient, explicit and implicit information into 2bit tactons in different scenarios.",
    "keywords": [
      "Vibrotactile Display",
      "Tactons",
      "On-body Feedback",
      "Low-density Information",
      "Information Presentation Interface"
    ],
    "doi": "10.1145/3311823.3311832",
    "url": "https://doi.org/10.1145/3311823.3311832",
    "citations": 16,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "3",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311834",
    "title": "StockSense: A Wrist-Worn Vibrotactile Display for tracking Volatile Markets",
    "authors": [
      "Erik Pescara",
      "Ilya Fillipov",
      "Michael Beigl"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents StockSense, a prototype of a haptic wristband for tracking volatile markets. StockSense informs the user about current price changes via structured vibrotactile feedback. This system can be used for any volatile market: cryptocurrency market, stock market, etc. A preliminary study was carried out with 5 participants to investigate whether vibrotactile displays of smartphones could be integrated into the system for additional information transmission. Afterwards the system was evaluated in a study with 20 participants. The participants became familiar with the system within a short period of time and were able to recognize the displayed information on the vibrotactile display reliably. The participants rated the system as positive. Overall, the participants appreciated the functionality of the system and considered it to be suitable for this field of application.",
    "keywords": [
      "Volatile Markets",
      "Tactile Interfaces",
      "Haptics"
    ],
    "doi": "10.1145/3311823.3311834",
    "url": "https://doi.org/10.1145/3311823.3311834",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "4",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311837",
    "title": "Evaluation of a device reproducing the pseudo-force sensation caused by a clothespin",
    "authors": [
      "Masahiro Miyakami",
      "Takuto Nakamura",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "A pseudo-force sensation can be elicited by pinching a finger with a clothespin. When the clothespin is used to pinch the finger from the palm side, a pseudo-force is felt in the direction towards the palm side, and when it is used to pinch the finger from the back side of the hand, the pseudo-force is felt in the extension direction. Here, as a first step to utilizing this phenomenon in human-machine interfaces, we developed a device that reproduces the clothespin phenomenon and confirmed the occurrence rate of the pseudo-force sensation.",
    "keywords": [
      "Virtual reality",
      "Pseudo force",
      "Human interface",
      "Clothespin"
    ],
    "doi": "10.1145/3311823.3311837",
    "url": "https://doi.org/10.1145/3311823.3311837",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "5",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311838",
    "title": "Grip Force Modulation by Finger Posture",
    "authors": [
      "Hideyuki Asazu",
      "Masahiro Miyakami",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Modulation of grip force has several applications such as sports. We speculated that as mupltiple fingers mechanically interact with each other, it might be possible to adjust grip strength by controlling the movement of single finger. We also speculated that changing grip strength may affect the weight perception of an object on grasping. In this paper, we investigated whether grip strength could be modulated by a specific finger posture, and found that it can be reduced by stretching some fingers, specifically middle finger and ring finger. Preliminary result of weight perception modulation is also reported.",
    "keywords": [
      "Weight perception",
      "Virtual reality",
      "Grip force",
      "Finger posture"
    ],
    "doi": "10.1145/3311823.3311838",
    "url": "https://doi.org/10.1145/3311823.3311838",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "6",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311835",
    "title": "Guided Walking to Direct Pedestrians toward the Same Destination",
    "authors": [
      "Nobuhito Sakamoto",
      "Masahiro Furukawa",
      "Masataka Kurokawa",
      "Taro Maeda"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we propose a floor covering-type walking guidance sheet to direct pedestrians without requiring attachment/detachment. Polarity is reversed with respect to the direction of walking in the guidance sheet such that a pedestrian travelling in any direction can be guided toward a given point. In experiments, our system successfully guided a pedestrian along the same direction regardless of the direction of travel using the walking guidance sheet. The induction effect of the proposed method was also evaluated.",
    "keywords": [
      "walking guidance",
      "visual stimulus",
      "vection",
      "pedestrian"
    ],
    "doi": "10.1145/3311823.3311835",
    "url": "https://doi.org/10.1145/3311823.3311835",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "7",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311857",
    "title": "Detection Threshold of the Height Difference between a Visual and Physical Step",
    "authors": [
      "Masato Kobayashi",
      "Yuki Kon",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In recent years, virtual reality (VR) applications that accompany real-space walking have become popular. In these applications, the expression of steps, such as a stairway, is a technical challenge. Preparing a real step with the same scale as that of the step in the VR space is one alternative; however, it is costly and impractical. We propose using a real step, but one physical step for the expression of various steps, by manipulating the viewpoint and foot position when ascending and descending real steps. The hypothesis is that the height of a step can be complemented to some extent visually, even if the heights of the real step and that in the VR space are different. In this paper, we first propose a viewpoint and foot position manipulation algorithm. T hen we measure the detection threshold of the height difference between the visual and physical step when ascending and descending the physical step using our manipulation algorithm. As a result, we found that the difference can be detected if there is a difference of approximately 1.0 cm between the VR space and the real space, irrespective of the height of the physical step.",
    "keywords": [
      "Virtual reality",
      "Viewpoint manipulation",
      "Redirected walking",
      "Height difference"
    ],
    "doi": "10.1145/3311823.3311857",
    "url": "https://doi.org/10.1145/3311823.3311857",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "8",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311836",
    "title": "Enhancement of Subjective Mechanical Tactile Intensity via Electrical Stimulation",
    "authors": [
      "Ryo Mizuhara",
      "Akifumi Takahashi",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Naturalistic tactile sensations can be elicited by mechanical stimuli because mechanical stimulation reproduces a natural physical phenomenon. However, a mechanical stimulation that is too strong may cause injury. Although electrical stimulation can elicit strong tactile sensations without damaging the skin, electrical stimulation is inferior in terms of naturalness. Here, we propose and validate a haptic method for presenting naturalistic and intense sensations by combining electrical and mechanical stimulation. Prior to the main experiment, we measured the appropriate temporal gap between the two stimuli such that they are perceived as simultaneous, since nerve activity directly elicited by electrical stimulation is generally considered to be perceived faster than mechanical stimulation. We confirmed that enhancement of subjective strength took place when two stimuli were given simultaneously. The main experiment with simultaneous electrical and mechanical stimulation confirmed that addition of electrical stimulation enhances the sensation of mechanical stimulation, and participants' comments implied that electrical stimulation was interpreted as part of the mechanical stimulation.",
    "keywords": [
      "Virtual Reality",
      "Tactile",
      "Mechanical stimulation",
      "Electrical stimulation"
    ],
    "doi": "10.1145/3311823.3311836",
    "url": "https://doi.org/10.1145/3311823.3311836",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "9",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311842",
    "title": "Glove-Through Tactile Information Transmission System",
    "authors": [
      "Hideki Kawai",
      "Hidenori Itoh",
      "Takuya Nakano",
      "Hiroyuki Kajimoto",
      "Yasuyuki Yanagida"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "By covering the skin with clothes such as gloves, humans have managed to survive in severe environments and undertake dangerous work. However, when we wear covering materials, tactile information is lost; this may decrease working efficiency and degrade the performance of object perception at the moment of contact. In order to address this issue, the concept of a \"haptic-through\" system that transmits tactile information through a covering material has been proposed. However, the effectiveness of this concept has been verified only by vertically pushing a surface of an object with one finger; there have been no systems proposed that can be used in actual complex finger motion. We constructed a glove-style haptic-through system that can be used in practical finger motion. By using this system, we conducted an experiment to investigate the discrimination threshold of the angle of a rod-shaped object. As a result, the angle discrimination threshold through a glove, with the support of our system, was equivalent to that obtained with bare hands.",
    "keywords": [
      "Tactile Sensor",
      "Tactile Display",
      "Haptic-Through",
      "Glove-Through"
    ],
    "doi": "10.1145/3311823.3311842",
    "url": "https://doi.org/10.1145/3311823.3311842",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311839",
    "title": "fSense: Unlocking the Dimension of Force for Gestural Interactions using Smartwatch PPG Sensor",
    "authors": [
      "Thisum Buddhika",
      "Haimo Zhang",
      "Samantha W. T. Chan",
      "Vipula Dissanayake",
      "Suranga Nanayakkara",
      "Roger Zimmermann"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "While most existing gestural interfaces focus on the static posture or the dynamic action of the hand, few have investigated the feasibility of using the forces that are exerted while performing gestures. Using the photoplethysmogram (PPG) sensor of off-the-shelf smartwatches, we show that, it is possible to recognize the force of a gesture as an independent channel of input. Based on a user study with 12 participants, we found that users were able to reliably produce two levels of force across several types of common gestures. We demonstrate a few interaction scenarios where the force is either used as a standalone input or to complement existing input modalities.",
    "keywords": [
      "Wearable Computing",
      "Smartwatch",
      "PPG Sensor",
      "Mobile Sensors",
      "Gesture Interaction"
    ],
    "doi": "10.1145/3311823.3311839",
    "url": "https://doi.org/10.1145/3311823.3311839",
    "citations": 25,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "11",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311840",
    "title": "Haptic Collar: Vibrotactile Feedback around the Neck for Guidance Applications",
    "authors": [
      "Stefanie Schaack",
      "George Chernyshov",
      "Kirill Ragozin",
      "Benjamin Tag",
      "Roshan Peiris",
      "Kai Kunze"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we present a Haptic Collar prototype, a neck worn band with vibrotactile actuators for eyes-free haptic navigation. We evaluate the system for guidance applications on over 11 users, analyzing 4 different tactile patterns regarding comfort and ease of understanding as well as the number of actuators to encode 8 directions (4, 6 and 8). Overall, users can recognize the directional signs well (up to 95 % recognition rates for over 528 triggers). We also present a use case applying our prototype for a haptic navigation walk.",
    "keywords": [
      "Vibrotactile Actuators",
      "Sensitivity",
      "Neck",
      "Navigation",
      "Haptics",
      "Haptic Feedback",
      "Collar"
    ],
    "doi": "10.1145/3311823.3311840",
    "url": "https://doi.org/10.1145/3311823.3311840",
    "citations": 28,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "12",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311841",
    "title": "OSense: Object-activity Identification Based on Gasping Posture and Motion",
    "authors": [
      "Thisum Buddhika",
      "Haimo Zhang",
      "Chamod Weerasinghe",
      "Suranga Nanayakkara",
      "Roger Zimmermann"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Observing that, how we grasp objects is highly correlated with geometric shapes and interactions, we propose the use of hand postures and motions as an indirect source of inputs for object-activity recognition. This paradigm treats the human hand as an always-available sensor, and transforms all sensing problems to the data analysis for the \"sensor hand\". We envision this paradigm to be generalizable for all objects regardless of whether they are acoustically or electromagnetically active, and that it detects different motions while holding the same object. Our proof-of-concept setup consists of six IMU sensors mounted on the fingers and back of the hand. Our experiments show that when the posture is combined with the motion, the personalized object-activity detection accuracy increases from 80% to 87%.",
    "keywords": [
      "Wearable Computing",
      "Object Identification",
      "Activity Recognition"
    ],
    "doi": "10.1145/3311823.3311841",
    "url": "https://doi.org/10.1145/3311823.3311841",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "13",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311826",
    "title": "TherModule: Wearable and Modular Thermal Feedback System based on a Wireless Platform",
    "authors": [
      "Tomosuke Maeda",
      "Tetsuo Kurahashi"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Humans have specific sensory organs and they can feel tactile sensation on the whole body. However, many haptic devices have limitations due to the location of the body part and might not provide natural haptic feedback. Thus, we propose a novel interface, TherModule, which is a wearable and modular thermal feedback system for embodied interactions based on a wireless platform. TherModule can be worn on multiple body parts such as the wrist, forearm, ankle, and neck. In this paper, we describe the system concept, module implementation, and applications. To demonstrate and explore the embodied interaction with thermal feedback, we implemented prototype applications, such as movie experiences, projector-based augmented reality, navigation, and notification based on a wireless platform, with TherModule on multiple parts of the body. The result of an experiment on movie experience showed that participants felt more interactions between temperature and visual stimulus.",
    "keywords": [
      "Wearable",
      "Haptics",
      "Embodied interaction"
    ],
    "doi": "10.1145/3311823.3311826",
    "url": "https://doi.org/10.1145/3311823.3311826",
    "citations": 35,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311843",
    "title": "Augmented Recreational Volleyball Court: Supporting the Beginners' Landing Position Prediction Skill by Providing Peripheral Visual Feedback",
    "authors": [
      "Koya Sato",
      "Yuji Sano",
      "Mai Otsuki",
      "Mizuki Oka",
      "Kazuhiko Kato"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Volleyball is widely popular as a way to share a sense of unity and achievement with others. However, errors detract beginners from enjoying the game. To overcome this issue, we developed a system that supports the beginners' skill to predict the ball landing position by indicating the predicted ball landing position on the floor as a visual feedback. In volleyball, it is necessary to pay attention to the ball that has been launched in air, and visual feedback on the floor surface must be perceived through peripheral vision. The effect of such visual feedback in supporting beginners' prediction skill was not clear. Therefore, we evaluated the effectiveness of the proposed system via a simulated serve-reception experiment. As a result, we confirmed that the proposed system improved the prediction skill in terms of the prediction speed and accuracy in the left-right direction, and that beginners felt an improvement in the prediction accuracy and ease of ball manipulation, thereby increasing the enjoyment. These results also indicate that it is possible to utilize peripheral vision supports in other disciplines in which there is a distance between the object of attention and the sports field on which visual feedback can be presented.",
    "keywords": [
      "visual feedback",
      "recreational volleyball",
      "peripheral vision",
      "floor projection",
      "augmented sports",
      "Augmented reality"
    ],
    "doi": "10.1145/3311823.3311843",
    "url": "https://doi.org/10.1145/3311823.3311843",
    "citations": 17,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "15",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311844",
    "title": "Prediction of Volleyball Trajectory Using Skeletal Motions of Setter Player",
    "authors": [
      "Shuya Suda",
      "Yasutoshi Makino",
      "Hiroyuki Shinoda"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we present a method that predicts the ball trajectory of a volleyball toss 0.3 s before the actual toss by observing the motion of the setter player. We input 3D data of body joints obtained using Kinect into a simple neural network, and 2D data estimated using OpenPose is used for comparison. We created simple neural networks for the two players and tested them. The trajectory of a volleyball toss is properly predicted by the proposed method and the error of the toss trajectory was approximately equal to the size of the ball. This technology can provide a new spectating experience in sports by superimposing the predicted images onto a live broadcast. We also show that this method can be used to identify the important body parts that contribute to the toss prediction. A professional volleyball analyst stated that this technology can be used for analyzing the peculiarities of opponent players.",
    "keywords": [
      "Volleyball",
      "Machine learning",
      "Ball trajectory prediction"
    ],
    "doi": "10.1145/3311823.3311844",
    "url": "https://doi.org/10.1145/3311823.3311844",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "16",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311845",
    "title": "Identifying Muscle Fatigue and Hyperthermia in Sports Activities Using Thermal Imaging and Facial Recognition Software",
    "authors": [
      "Christopher G. Harris"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Hyperthermia and muscle fatigue during sports activities are a challenge to detect because body temperatures cannot be unobtrusively evaluated from the field of play. Recently, inexpensive portable thermal imaging devices have become available, allowing accurate monitoring of players from a distance. In this paper, we examine the accuracy of thermal imaging devices by distance and monitoring angle. Using thermal imaging, object recognition, and facial recognition techniques, we develop a visualization tool to display core body temperature information for each player. This augmented display allows us to detect potential heat-related player injuries on a sports field in near real time.",
    "keywords": [
      "visualization",
      "sports medicine",
      "muscle fatigue",
      "hyperthermia",
      "augmented health",
      "Thermal imaging"
    ],
    "doi": "10.1145/3311823.3311845",
    "url": "https://doi.org/10.1145/3311823.3311845",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311861",
    "title": "Virtual Super-Leaping: Immersive Extreme Jumping in VR",
    "authors": [
      "Tomoya Sasaki",
      "Kao-Hua Liu",
      "Taiki Hasegawa",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "People sometimes imagine and yearn for a \"Super Power,\" an ability they do not have naturally. In this paper, we propose Virtual Super-Leaping (VSL) as an immersive virtual experience that provides the feeling of extreme jumping in the sky. First, we define the necessary feedback elements and classify the action sequence of Super-Leaping, including the design of the multimodal feedback for each action state. Then, we describe the design of the VSL system, which has two components: (i) visual a head-mounted display-based feedback, and (ii) a VSL-enabling haptic device, which provides both kinesthesia and airflow using multiple synchronized propeller units. We end by reporting on our technical evaluation and public demonstrations. This work contributes to the enhancement of immersive virtual experiences and development of devices for human augmentation.",
    "keywords": [
      "Kinesthesia",
      "Illusion of Self-motion",
      "Haptic Device",
      "Airflow"
    ],
    "doi": "10.1145/3311823.3311861",
    "url": "https://doi.org/10.1145/3311823.3311861",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "18",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311846",
    "title": "Social Activity Measurement by Counting Faces Captured in First-Person View Lifelogging Video",
    "authors": [
      "Akane Okuno",
      "Yasuyuki Sumi"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "This paper proposes a method to measure the daily face-to-face social activity of a camera wearer by detecting faces captured in first-person view lifelogging videos. This study was inspired by pedometers used to estimate the amount of physical activity by counting the number of steps detected by accelerometers, which is effective for reflecting individual health and facilitating behavior change. We investigated whether we can estimate the amount of social activity by counting the number of faces captured in the first-person view videos like a pedometer. Our system counts not only the number of faces but also weighs in the numbers according to the size of the face (corresponding to a face's closeness) and the amount of time it was shown in the video. By doing so, we confirmed that we can measure the amount of social activity based on the quality of each interaction. For example, if we simply count the number of faces, we overestimate social activities while passing through a crowd of people. Our system, on the other hand, gives a higher score to a social actitivity even when speaking with a single person for a long time, which was also positively evaluated by experiment participants who viewed the lifelogging videos. Through evaluation experiments, many evaluators evaluated the social activity high when the camera wearer speaks. An interesting feature of the proposed system is that it can correctly evaluate such scenes higher as the camera wearer actively engages in conversations with others, even though the system does not measure the camera wearer's utterances. This is because the conversation partners tend to turn their faces towards to the camera wearer, and that increases the number of detected faces as a result. However, the present system fails to correctly estimate the depth of social activity compared to what the camera wearer recalls especially when the conversation partners are standing out of the camera's field of view. The paper briefly descibes how the results can be improved by widening the camera's field of view.",
    "keywords": [
      "social health",
      "quantified self",
      "lifelogging",
      "first-person view video",
      "face detection",
      "Social activity measurement"
    ],
    "doi": "10.1145/3311823.3311846",
    "url": "https://doi.org/10.1145/3311823.3311846",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "19",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311871",
    "title": "Augmented taste of wine by artificial climate room: Influence of temperature and humidity on taste evaluation",
    "authors": [
      "Toshiharu Igarashi",
      "Tatsuya Minagawa",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In previous research, there is a augmenting device limited taste influences due to limited contact with utensils. However, in the situation such as enjoying wine while talking with other people and matching cheese with wine, the solution that limits human behaviors must not have been acceptable. So, we focused on changing the temperature and humidity when drinking wine.To study the influence of temperature and humidity on the ingredients and subjective taste of wine, we conducted wine tasting experiments with 16 subjects using an artificial climate room. For the environmental settings, three conditions, i.e., a room temperature of 14°C and humidity of 35%, 17°C and 40% humidity, and 26°C and 40% humidity, were evaluated. In one of the two wines used in the experiment, significant differences in [Color intensity], [Smell development] and [Body] were detected among conditions (p &lt; 0.05). We further investigated changes in the components of the two wines at different temperature conditions (14°C, 17°C, 23°C, and 26°C). Malic acid, protocatechuic acid, gallic acid, and epicatechin were related to temperature in the former wine only.In conclusion, we confirmed that we can change the taste evaluation of wine by adjusting temperature and humidity using the artificial climate room, without attaching the device to human beings themselves. This suggests the possibility to serve wine in a more optimal environment if we can identify the type of wine and person's preference.",
    "keywords": [
      "Taste of wine",
      "Human-centered computing",
      "Empirical studies in HCI",
      "Augmented taste",
      "Artificial climate room"
    ],
    "doi": "10.1145/3311823.3311871",
    "url": "https://doi.org/10.1145/3311823.3311871",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "20",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311863",
    "title": "GANs-based Clothes Design: Pattern Maker Is All You Need to Design Clothing",
    "authors": [
      "Natsumi Kato",
      "Hiroyuki Osone",
      "Kotaro Oomori",
      "Chun Wei Ooi",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Machine learning have been recently applied to multiple areas, including fashion. Fashion design by generated images makes it possible to inherit design without fashion designer and get inspiration, however, little research has been done on usage of machine learning for creation of designer clothing. The state-of-the-art works aim for high-definition output images. However in fashion design image generation, it has not been thoroughly investigated to what extent the quality of the generated image should be provided to the pattern makers that draw the costume pattern from the design images. Therefore, in this paper we propose a method of generation of clothing images for pattern makers using Progressive Growing of GANs (P-GANs) and conduct a user study to investigate whether the different image quality factors such as epoch and resolution affect the participants' confidence score. We discuss the results and possible applications of the developed method.",
    "keywords": [
      "pattern maker",
      "pattern",
      "fashion",
      "deep neural network",
      "clothes",
      "Generative Adversarial Networks"
    ],
    "doi": "10.1145/3311823.3311863",
    "url": "https://doi.org/10.1145/3311823.3311863",
    "citations": 35,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "21",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311864",
    "title": "Brain Computer Interface for Neuro-rehabilitation With Deep Learning Classification and Virtual Reality Feedback",
    "authors": [
      "Tamás Karácsony",
      "John Paulin Hansen",
      "Helle Klingenberg Iversen",
      "Sadasivan Puthusserypady"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Though Motor Imagery (MI) stroke rehabilitation effectively promotes neural reorganization, current therapeutic methods are immeasurable and their repetitiveness can be demotivating. In this work, a real-time electroencephalogram (EEG) based MI-BCI (Brain Computer Interface) system with a virtual reality (VR) game as a motivational feedback has been developed for stroke rehabilitation. If the subject successfully hits one of the targets, it explodes and thus providing feedback on a successfully imagined and virtually executed movement of hands or feet. Novel classification algorithms with deep learning (DL) and convolutional neural network (CNN) architecture with a unique trial onset detection technique was used. Our classifiers performed better than the previous architectures on datasets from PhysioNet offline database. It provided fine classification in the real-time game setting using a 0.5 second 16 channel input for the CNN architectures. Ten participants reported the training to be interesting, fun and immersive. \"It is a bit weird, because it feels like it would be my hands\", was one of the comments from a test person. The VR system induced a slight discomfort and a moderate effort for MI activations was reported. We conclude that MI-BCI-VR systems with classifiers based on DL for real-time game applications should be considered for motivating MI stroke rehabilitation.",
    "keywords": [
      "Virtual Reality",
      "Online EEG classification",
      "Motor Imagery",
      "Deep learning",
      "CNN",
      "Brain Computer Interface"
    ],
    "doi": "10.1145/3311823.3311864",
    "url": "https://doi.org/10.1145/3311823.3311864",
    "citations": 62,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "22",
    "numpages": "8",
    "influentialCitations": 8
  },
  {
    "id": "10.1145/3311823.3311865",
    "title": "SubMe: An Interactive Subtitle System with English Skill Estimation Using Eye Tracking",
    "authors": [
      "Katsuya Fujii",
      "Jun Rekimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Owing to the improvement in accuracy of eye tracking devices, eye gaze movements occurring while conducting tasks are now a part of physical activities that can be monitored just like other life-logging data. Analyzing eye gaze movement data to predict reading comprehension has been widely explored and researchers have proven the potential of utilizing computers to estimate the skills and expertise level of users in various categories, including language skills. However, though many researchers have worked specifically on written texts to improve the reading skills of users, little research has been conducted to analyze eye gaze movements in correlation to watching movies, a medium which is known to be a popular and successful method of studying English as it includes reading, listening, and even speaking, the later of which is attributed to language shadowing. In this research, we focus on movies with subtitles due to the fact that they are very useful in order to grasp what is occurring on screen, and therefore, overall understanding of the content. We realized that the viewers' eye gaze movements are distinct depending on their English level. After retrieving the viewers' eye gaze movement data, we implemented a machine learning algorithm to detect their English levels and created a smart subtitle system called SubMe. The goal of this research is to estimate English levels through tracking eye movement. This was conducted by allowing the users to view a movie with subtitles. Our aim is create a system that can give the user certain feedback that can help improve their English studying methods.",
    "keywords": [
      "User Interface",
      "Learning",
      "Human Computer Interaction"
    ],
    "doi": "10.1145/3311823.3311865",
    "url": "https://doi.org/10.1145/3311823.3311865",
    "citations": 15,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "23",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311868",
    "title": "Sentiment Pen: Recognizing Emotional Context Based on Handwriting Features",
    "authors": [
      "Jiawen Han",
      "George Chernyshov",
      "Dingding Zheng",
      "Peizhong Gao",
      "Takuji Narumi",
      "Katrin Wolf",
      "Kai Kunze"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we discuss the assessment of the emotional state of the user from digitized handwriting for implicit human-computer interaction. The proposed concept exemplifies how a digital system could recognize the emotional context of the interaction. We discuss our approach to emotion recognition and the underlying neurophysiological mechanisms. To verify the viability of our approach, we have conducted a series of tests where participants were asked to perform simple writing tasks after being exposed to a series of emotionally-stimulating video clips from EMDB[6], one set of four clips per each quadrant on the circumplex model of emotion[28]. The user-independent Support Vector Classifier (SVC) built using the recorded data shows up to 66% accuracy for certain types of writing tasks for 1 in 4 classification (1. High Valence, High Arousal; 2. High Valence, Low Arousal; 3. Low Valence, High Arousal; 4. Low Valence, Low Arousal). In the same conditions, a user-dependent classifier reaches an average of 70% accuracy across all 12 study participants. While future work is required to improve the classification rate, this work should be seen as proof-of-concept for emotion assessment of users while handwriting aiming to motivate research on implicit interaction while writing to enable emotion-sensitivity in mobile and ubiquitous computing.",
    "keywords": [
      "Handwriting Analysis",
      "Emotional Recognition",
      "Affective Computing"
    ],
    "doi": "10.1145/3311823.3311868",
    "url": "https://doi.org/10.1145/3311823.3311868",
    "citations": 10,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "24",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311869",
    "title": "Automatic Smile and Frown Recognition with Kinetic Earables",
    "authors": [
      "Seungchul Lee",
      "Chulhong Min",
      "Alessandro Montanari",
      "Akhil Mathur",
      "Youngjae Chang",
      "Junehwa Song",
      "Fahim Kawsar"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we introduce inertial signals obtained from an earable placed in the ear canal as a new compelling sensing modality for recognising two key facial expressions: smile and frown. Borrowing principles from Facial Action Coding Systems, we first demonstrate that an inertial measurement unit of an earable can capture facial muscle deformation activated by a set of temporal micro-expressions. Building on these observations, we then present three different learning schemes - shallow models with statistical features, hidden Markov model, and deep neural networks to automatically recognise smile and frown expressions from inertial signals. The experimental results show that in controlled non-conversational settings, we can identify smile and frown with high accuracy (F1 score: 0.85).",
    "keywords": [
      "smile and frown recognition",
      "kinetic modeling",
      "earable",
      "FACS"
    ],
    "doi": "10.1145/3311823.3311869",
    "url": "https://doi.org/10.1145/3311823.3311869",
    "citations": 27,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "25",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311870",
    "title": "Prospero: A Personal Wearable Memory Coach",
    "authors": [
      "Samantha W. T. Chan",
      "Haimo Zhang",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Prospective memory, which involves remembering to perform intended actions, is essential for independent daily living especially as we grow older. Yet, majority of everyday memory failures are due to prospective memory lapses. Memory strategy training can help to tackle such lapses. We present Prospero, a wearable virtual memory coach that guides users to learn and apply a memory technique through conversation in natural language. Using physiological signals, Prospero proactively initiates practice of the technique during opportune times where user attention and cognitive load have more bandwidth. This could be a step towards creating more natural and effective digital memory training that could eventually reduce memory decline. In this paper, we contribute with details of its implementation and conversation design.",
    "keywords": [
      "Memory Training",
      "Memory Coach",
      "Conversational Agent"
    ],
    "doi": "10.1145/3311823.3311870",
    "url": "https://doi.org/10.1145/3311823.3311870",
    "citations": 8,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "26",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311875",
    "title": "An Implicit Dialogue Injection System for Interruption Management",
    "authors": [
      "Tomoki Shibata",
      "Alena Borisenko",
      "Anzu Hakone",
      "Tal August",
      "Leonidas Deligiannidis",
      "Chen-Hsiang Yu",
      "Matthew Russell",
      "Alex Olwal",
      "Robert J. K. Jacob"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents our efforts in redesigning the conventional on/off interruption management tactic (a.k.a. \"Do Not Disturb Mode\") for situations where interruptions are inevitable. We introduce an implicit dialogue injection system, in which the computer implicitly observes the user's state of busyness from passive measurement of the prefrontal cortex to determine how to interrupt the user. We use functional Near-Infrared Spectroscopy (fNIRS), a noninvasive brain-sensing technique. In this paper, we describe our system architecture and report results of our proof-of-concept study, in which we compared two contrasting interruption strategies; the computer either forcibly interrupts the user with a secondary task or requests the user's participation before presenting it. The latter yielded improved user experience (e.g. lower reported annoyance), in addition to showing a potential improvement in task performance (i.e. retaining context information) when the user was busier. We conclude that tailoring the presentation of interruptions based on real-time user state provides a step toward making computers more considerate of their users.",
    "keywords": [
      "implicit interactions",
      "implicit dialogue injection",
      "functional Near-Infrared Spectroscopy (fNIRS)",
      "Interruption",
      "Implicit User Interfaces",
      "HumanSketch"
    ],
    "doi": "10.1145/3311823.3311875",
    "url": "https://doi.org/10.1145/3311823.3311875",
    "citations": 5,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "27",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311872",
    "title": "Hearing Is Believing: Synthesizing Spatial Audio from Everyday Objects to Users",
    "authors": [
      "Jing Yang",
      "Yves Frank",
      "Gábor Sörös"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "The ubiquity of wearable audio devices and the importance of the auditory sense imply great potential for audio augmented reality. In this work, we propose a concept and a prototype of synthesizing spatial sounds from arbitrary real objects to users in everyday interactions, whereby all sounds are rendered directly by the user's own ear pods instead of loudspeakers on the objects. The proposed system tracks the user and the objects in real time, creates a simplified model of the environment, and generates realistic 3D audio effects. We thoroughly evaluate the usability and the usefulness of such a system based on a user study with 21 participants. We also investigate how an acoustic environment model improves the sense of engagement of the rendered 3D sounds.",
    "keywords": [
      "human-object interactions",
      "augmented reality",
      "Spatial audio"
    ],
    "doi": "10.1145/3311823.3311872",
    "url": "https://doi.org/10.1145/3311823.3311872",
    "citations": 16,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "28",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311873",
    "title": "MusiArm: Extending Prosthesis to Musical Expression",
    "authors": [
      "Kaito Hatakeyama",
      "MHD Yamen Saraiji",
      "Kouta Minamizawa"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "The emergence of prosthetic limbs where solely focused on substituting the missing limb with an artificial one, in order for the handicap people to manage their daily life independently. Past research on prosthetic hands has mainly focused on prosthesis' function and performance. Few proposals focused on the entertainment aspect of prosthetic hands. In this research, we considered the defective part as a potential margin for freely designing our bodies, and coming up with new use cases beyond the original function of the limb. Thus, we are not aiming to create anthropomorphic designs or functions of the limbs. By fusing the prosthetic hands and musical instruments, we propose a new prosthetic hand called \"MusiArm\" that extends the body part's function to become an instrument. MusiArm concept was developed through the dialogue between the handicapped people, engineers and prosthetists using the physical characteristics of the handicapped people as a \"new value\" that only the handicapped person can possess. We asked handicapped people who cannot play musical instruments, as well as people who do not usually play instruments, to use prototypes we made. As a result of the usability tests, using MusiArm, we made a part of the body function as a musical instrument, drawing out the unique expression methods of individuals, and enjoying the performance and clarify the possibility of showing interests.",
    "keywords": [
      "Prosthetic Arm",
      "Prosthesis",
      "Musical Instrument",
      "Human Augmentation",
      "Congenital Defect",
      "Co-creation"
    ],
    "doi": "10.1145/3311823.3311873",
    "url": "https://doi.org/10.1145/3311823.3311873",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "29",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311854",
    "title": "Automatic Eyeglasses Replacement for a 3D Virtual Try-on System",
    "authors": [
      "Takumi Kobayashi",
      "Yuta Sugiura",
      "Hideo Saito",
      "Yuji Uema"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents a 3D virtual eyeglasses try-on system for practical use. For fitting eyeglasses in a shop, consumers wish to look at themselves in a mirror while trying on various eyeglass styles. However, for people who need to wear eyeglasses for correcting problems with eyesight, it is impossible for them to clearly observe their face in the mirror without wearing eyeglasses. This makes fitting them for new eyeglasses difficult. This research proposes a virtual try-on system that can be used while wearing eyeglasses. We replace the user's eyeglasses in the input video with new eyeglasses virtually. Moreover, a fast and accurate face tracking tool enables our system to automatically display 3D virtual glasses following a user's head motion. Experimental results demonstrate that the proposed method can render virtual glasses naturally while the user is wearing real eyeglasses.",
    "keywords": [
      "virtual try-on",
      "mixed reality",
      "eyeglasses removal",
      "augmented reality"
    ],
    "doi": "10.1145/3311823.3311854",
    "url": "https://doi.org/10.1145/3311823.3311854",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "30",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311858",
    "title": "Effects of a Monocular Laser-Based Head-Mounted Display on Human Night Vision",
    "authors": [
      "Evangelos Niforatos",
      "Mélodie Vidal"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Head-mounted displays (HMDs) are expected to dominate the market of wearable electronics in the next 5 years. This foreseen proliferation of HMDs yields a plethora of design opportunities for revolutionizing everyday life via novel use cases, but also generates a considerable number of substantial safety implications. In this work, we systematically investigated the effect of a novel monocular laser-based HMD on the ability of our participants to see in low ambient light conditions in lab settings. We recruited a total of 19 participants in two studies and performed a series of established vision tests while using the newly available Focals by North HMD. We tested our participants' night vision after being exposed to different levels of laser luminous power and laser colors while using Focals, either with one or both eyes open. Our results showcase that the image perceived by the non-exposed eye compensates for the loss of contrast sensitivity observed in the image perceived by the laser-exposed eye. This indicates that monocular laser-based HMDs, such as Focals, permit dark adaptation to occur naturally for the non-exposed eye.",
    "keywords": [
      "Scotopic vision",
      "Laser light projection",
      "Human Factors",
      "Head-mounted displays"
    ],
    "doi": "10.1145/3311823.3311858",
    "url": "https://doi.org/10.1145/3311823.3311858",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "31",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311859",
    "title": "MagniFinger: Fingertip probe microscope with direct micro movements",
    "authors": [
      "Noriyasu Obushi",
      "Sohei Wakisaka",
      "Shunichi Kasahara",
      "Katie Seaborn",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "By adulthood, our fingers have developed a high level of dexterity: sensory and motor skills that developers have only just started to make use of in modern interfaces. Previous research has unveiled the possibilities of enhancing touch modalities by introducing visual feedback of the magnified touch image. Yet, most of the microscopes on the market require a complicated procedure to operate and this makes it difficult to move the felt/observed area. To address this, we introduce MagniFinger, a new finger-based microscope that allows users to magnify the contacting surface on their fingertips using two means of control: sliding and tilting. The tilting-based control enables a more precise movement under micro-environments. According to the results of our experiments, it shortens the time of reaching targets compared to the simple sliding-based control.",
    "keywords": [
      "microscope",
      "magnification",
      "finger-worn device"
    ],
    "doi": "10.1145/3311823.3311859",
    "url": "https://doi.org/10.1145/3311823.3311859",
    "citations": 6,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "32",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311860",
    "title": "Let Your World Open: CAVE-based Visualization Methods of Public Virtual Reality towards a Shareable VR Experience",
    "authors": [
      "Akira Ishii",
      "Masaya Tsuruta",
      "Ippei Suzuki",
      "Shuta Nakamae",
      "Junichi Suzuki",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Virtual reality (VR) games are currently becoming part of the public-space entertainment (e.g., VR amusement parks). Therefore, VR games should be attractive for players, as well as for bystanders. Current VR systems are still mostly focused on enhancing the experience of the head-mounted display (HMD) users; thus, bystanders without an HMD cannot enjoy the experience together with the HMD users. We propose the \"ReverseCAVE\": a proof-of-concept prototype for public VR visualization using CAVE-based projection with translucent screens for bystanders toward a shareable VR experience. The screens surround the HMD user and the VR environment is projected onto the screens. This enables the bystanders to see the HMD user and the VR environment simultaneously. We designed and implemented the ReverseCAVE, and evaluated it in terms of the degree of attention, attractiveness, enjoyment, and shareability, assuming that it is used in a public space. Thus, we can make the VR world more accessible and enhance the public VR experience of the bystanders via the ReverseCAVE.",
    "keywords": [
      "sharing experience",
      "mixed reality (MR)",
      "Environmental VR",
      "CAVE"
    ],
    "doi": "10.1145/3311823.3311860",
    "url": "https://doi.org/10.1145/3311823.3311860",
    "citations": 26,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "33",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311862",
    "title": "Double Shellf: What Psychological Effects can be Caused through Interaction with a Doppelganger?",
    "authors": [
      "Yuji Hatada",
      "Shigeo Yoshida",
      "Takuji Narumi",
      "Michitaka Hirose"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Advances in 3D capture technology have made it easier to generate a realistic avatar, which can represent a person in virtual environments. Because avatars can be easily duplicated in the virtual environments, there can be an unrealistic situation where a person sees her/his own doppelgangers. Doppelganger is a double of a person and sometimes portrayed as a sinister existence. To investigate how people feel and react when they face their doppelgangers, we developed \"Double Shellf\", a virtual reality experience in which people can interact with their virtual doppelgangers in various situations. In this paper, we introduce the design of Double Shellf and discuss the reactions of 86 users. The user study revealed that most people felt intense eeriness when they see their doppelgangers which acts autonomously and when they were touched by their doppelgangers. We also found that there is a gender difference in reactions to their doppelgangers. We explore the effective way of utilizing doppelgangers.",
    "keywords": [
      "Virtual Reality",
      "Social Psychology",
      "Doppelganger",
      "Avatar"
    ],
    "doi": "10.1145/3311823.3311862",
    "url": "https://doi.org/10.1145/3311823.3311862",
    "citations": 11,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "34",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311847",
    "title": "Augmenting Human With a Tail",
    "authors": [
      "Haoran Xie",
      "Kento Mitsuhashi",
      "Takuma Torii"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Human-augmentation devices have been extensively proposed and developed recently and are useful in improving our work efficiency and our quality of life. Inspired by animal tails, this study aims to propose a wearable and functional tail device that combines physical and emotional-augmentation modes. In the physical-augmentation mode, the proposed device can be transformed into a consolidated state to support a user's weight, similar to a kangaroo's tail. In the emotional-augmentation mode, the proposed device can help users express their emotions, which are realized by different tail-motion patterns. For our initial prototype, we developed technical features that can support the weight of an adult, and we performed a perceptional investigation of the relations between the tail movements and the corresponding perceptual impressions. Using the animal-tail analog, the proposed device may be able to help the human user in both physical and emotional ways.",
    "keywords": [
      "Tail Device",
      "Physical Capability",
      "Human Augmentation",
      "Emotional Expression"
    ],
    "doi": "10.1145/3311823.3311847",
    "url": "https://doi.org/10.1145/3311823.3311847",
    "citations": 37,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "35",
    "numpages": "7",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311848",
    "title": "Prosthetic Tail: Artificial Anthropomorphic Tail for Extending Innate Body Functions",
    "authors": [
      "Junichi Nabeshima",
      "MHD Yamen Saraiji",
      "Kouta Minamizawa"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "For most mammals and vertebrate animals, tail plays an important role for their body providing variant functions to expand their mobility, or as a limb that allows manipulation and gripping. In this paper, we propose an exploratory biomimicry-inspired anthropomorphic tail design to allow engineering and expanding human body functions. The proposed tail consists of adjacent joints with a spring-based structure to handle shearing and tangential forces, and allow managing the length and weight of the target tail. The internal structure of the tail is driven by four pneumatic artificial muscles providing the actuation mechanism for the tail tip. Here we describe the design and implementation process, and highlight potential applications for using such prosthetic tail.",
    "keywords": [
      "Embodied Robotics",
      "Biomimicry",
      "Biomechanics",
      "Artificial Tail"
    ],
    "doi": "10.1145/3311823.3311848",
    "url": "https://doi.org/10.1145/3311823.3311848",
    "citations": 13,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "36",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311850",
    "title": "Orochi: Investigating Requirements and Expectations for Multipurpose Daily Used Supernumerary Robotic Limbs",
    "authors": [
      "Mohammed Al-Sada",
      "Thomas Höglund",
      "Mohamed Khamis",
      "Jaryd Urbani",
      "Tatsuo Nakajima"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Supernumerary robotic limbs (SRLs) present many opportunities for daily use. However, their obtrusiveness and limitations in interaction genericity hinder their daily use. To address challenges of daily use, we extracted three design considerations from previous literature and embodied them in a wearable we call Orochi. The considerations include the following: 1) multipurpose use, 2) wearability by context, and 3) unobtrusiveness in public. We implemented Orochi as a snake-shaped robot with 25 DoFs and two end effectors, and demonstrated several novel interactions enabled by its limber design. Using Orochi, we conducted hands-on focus groups to explore how multipurpose SRLs are used daily and we conducted a survey to explore how they are perceived when used in public. Participants approved Orochi's design and proposed different use cases and postures in which it could be worn. Orochi's unobtrusive design was generally well received, yet novel interactions raise several challenges for social acceptance. We discuss the significance of our results by highlighting future research opportunities based on the design, implementation, and evaluation of Orochi.",
    "keywords": [
      "Wearable",
      "Unobtrusive",
      "Multipurpose",
      "Design",
      "Augmentation"
    ],
    "doi": "10.1145/3311823.3311850",
    "url": "https://doi.org/10.1145/3311823.3311850",
    "citations": 46,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "37",
    "numpages": "9",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/3311823.3311849",
    "title": "Naviarm: Augmenting the Learning of Motor Skills using a Backpack-type Robotic Arm System",
    "authors": [
      "Azumi Maekawa",
      "Shota Takahashi",
      "MHD Yamen Saraiji",
      "Sohei Wakisaka",
      "Hiroyasu Iwata",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "We present a wearable haptic assistance robotic system for augmented motor learning called Naviarm. This system comprises two robotic arms that are mounted on a user's body and are used to transfer one person's motion to another offline. Naviarm pre-records the arm motion trajectories of an expert via the mounted robotic arms and then plays back these recorded trajectories to share the expert's body motion with a beginner. The Naviarm system is an ungrounded system and provides mobility for the user to conduct a variety of motions. In this paper, we focus on the temporal aspect of motor skill and use a mime performance as a case study learning task. We verified the system effectiveness for motor learning using the conducted experiments. The results suggest that the proposed system has benefits for learning sequential skills.",
    "keywords": [
      "Wearable Device",
      "Robotics",
      "Motor Learning",
      "Haptics",
      "Augmented learning"
    ],
    "doi": "10.1145/3311823.3311849",
    "url": "https://doi.org/10.1145/3311823.3311849",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "38",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311855",
    "title": "BitoBody: Real-time human contact detection and dynamic projection system",
    "authors": [
      "Erwin Wu",
      "Mistki Piekenbrock",
      "Hideki Koike"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this research, we propose a novel human body contact detection and projection system with dynamic mesh collider. We use motion capture camera and generated human 3D models to detect the contact between user's bodies. Since it is difficult to update human mesh collider every frame, a special algorithm that divides body meshes into small pieces of polygons to do collision detection is developed and detected hit information will be dynamically projected according to its magnitude of damage. The maximum deviation of damage projection is about 7.9cm under a 240-fps optitrack motion capture system and 12.0cm under a 30-fps Kinect camera. The proposed system can be used in various sports where bodies come in contact and it allows the audience and players to understand the context easier.",
    "keywords": [
      "Real-time contact detection",
      "Human 3D model Generatrion",
      "Dynamic projection mapping"
    ],
    "doi": "10.1145/3311823.3311855",
    "url": "https://doi.org/10.1145/3311823.3311855",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "39",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311827",
    "title": "CompoundDome: A wearable dome device that enables interaction with the real world by controlling the transparency of the screen",
    "authors": [
      "Eriko Maruyama",
      "Jun Rekimoto"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "The head-mounted display (HMD) is widely used as a method to experience virtual space. However, HMD has problems in mounting, such as skin touching the equipment used by others, functional issues such as easy to induce VR sickness. In this research, we propose a wearable dome device named \"CompoundDome\", which enables interaction with the real world by projecting images on the dome. In our system, we used a 600 mm diameter dome, and a projector projects images to the dome to cover the wearer's field of view. With this configuration, the equipment does not touch the skin, and motion sickness can be reduced. HMD also lacks in providing face-to-face communication, because it hides user's face. In addition, the wearer can not see the outside when wearing the HMD. Hence, we applied screen paint to the transparent dome in a mesh form. With this configuration, users can see the image when the image is projected, and they can see the outside of the dome when the image is not projected. Furthermore, users and the surrounding people can make face to face communication by photographing the face with the camera installed in the dome and projecting the face in the virtual space. In this paper, we describe the composition of CompoundDome, in comparison with other virtual space presentation means, and various applications enabled by CompoundDome.",
    "keywords": [
      "human augmentation",
      "Mixed Reality",
      "Dome shape"
    ],
    "doi": "10.1145/3311823.3311827",
    "url": "https://doi.org/10.1145/3311823.3311827",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "40",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311853",
    "title": "Investigating Universal Appliance Control through Wearable Augmented Reality",
    "authors": [
      "Vincent Becker",
      "Felix Rauchenstein",
      "Gábor Sörös"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "The number of interconnected devices around us is constantly growing. However, it may become challenging to control all these devices when control interfaces are distributed over mechanical elements, apps, and configuration webpages. We investigate interaction methods for smart devices in augmented reality. The physical objects are augmented with interaction widgets, which are generated on demand and represent the connected devices along with their adjustable parameters. For example, a loudspeaker can be overlaid with a controller widget for its volume. We explore three ways of manipulating the virtual widgets: (a) in-air finger pinching and sliding, (b) whole arm gestures rotating and waving, (c) incorporating physical objects in the surrounding and mapping their movements to the interaction primitives. We compare these methods in a user study with 25 participants and find significant differences in the preference of the users, the speed of executing commands, and the granularity of the type of control.",
    "keywords": [
      "Wearable Computing",
      "Ubiquitous Computing",
      "Tangible User Interfaces",
      "Smart Object",
      "Augmented Reality"
    ],
    "doi": "10.1145/3311823.3311853",
    "url": "https://doi.org/10.1145/3311823.3311853",
    "citations": 12,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "41",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311874",
    "title": "CapMat: A Smart Foot Mat for User Authentication",
    "authors": [
      "Denys J. C. Matthies",
      "Don Samitha Elvitigala",
      "Sachith Muthukumarana",
      "Jochen Huber",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "We present CapMat, a smart foot mat that enables user identification, supporting applications such as multi-layer authentication. CapMat leverages a large form factor capacitive sensor to capture shoe sole images. These images vary based on shoe form factors, the individual wear, and the user's weight. In a preliminary evaluation, we distinguished 15 users with an accuracy of up to 100%.",
    "keywords": [
      "User Identification",
      "Smart Home",
      "Implicit Authentication",
      "Floor mat",
      "Capacitive Sensing"
    ],
    "doi": "10.1145/3311823.3311874",
    "url": "https://doi.org/10.1145/3311823.3311874",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "42",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311833",
    "title": "CricketCoach: Towards Creating a Better Awareness of Gripping Forces for Cricketers",
    "authors": [
      "Sachith Muthukumarana",
      "Denys J. C. Matthies",
      "Chamod Weerasinghe",
      "Don Samitha Elvitigala",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we demonstrate a smart system that creates awareness of the hand-grip force for cricket players. A custom Force-Sensitive Resistor (FSR) matrix is attached to the bat's handle to sense the gripping. Two wrist bands, incorporating vibration motors, provide feedback that helps nonexpert users to understand the relative forces exerted by each hand while performing a stroke. A preliminary user study was conducted to collect first insights.",
    "keywords": [],
    "doi": "10.1145/3311823.3311833",
    "url": "https://doi.org/10.1145/3311823.3311833",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "43",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311866",
    "title": "Second Language Vocabulary Learning While Walking",
    "authors": [
      "Shogo Fukushima",
      "Ari Hautasaari",
      "Takeo Hamada"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Second language (L2) learners often lack opportunities or motivation to dedicate their time to vocabulary learning over other daily activities. In this work, we introduce a mobile application that allows L2 learners to instead leverage their \"dead time\", such as when walking to and from school or work, to study new vocabulary items. The application combines audio learning and location-based contextually relevant L1-L2 word pairs to allow L2 learners to \"discover\" new foreign language words while walking. We report on the evaluation of the approach from three aspects: L2 vocabulary retention after 1 month, system usability and workload.",
    "keywords": [
      "walking",
      "incidental learning",
      "contextualized audio",
      "Second language vocabulary learning"
    ],
    "doi": "10.1145/3311823.3311866",
    "url": "https://doi.org/10.1145/3311823.3311866",
    "citations": 14,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "44",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311852",
    "title": "Build your Own! Open-Source VR Shoes for Unity3D",
    "authors": [
      "Jens Reinhardt",
      "Eike Lewandowski",
      "Katrin Wolf"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "Hand-held controllers enable all kinds of interaction in Virtual Reality (VR), such as object manipulation as well as for locomotion. VR shoes allow using the hand exclusively for naturally manual tasks, such as object manipulation, while locomotion could be realized through feet input -- just like in the physical world. While hand-held VR controllers became standard input devices for consumer VR products, VR shoes are only barely available, and also research on that input modality remains open questions. We contribute here with open-source VR shoes and describe how to build and implement them as Unity3D input device. We hope to support researchers in VR research and practitioners in VR product design to increase usability and natural interaction in VR.",
    "keywords": [
      "Virtual Reality",
      "Locomotion",
      "Foot Interaction"
    ],
    "doi": "10.1145/3311823.3311852",
    "url": "https://doi.org/10.1145/3311823.3311852",
    "citations": 5,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "45",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311867",
    "title": "Design of Enhanced Flashcards for Second Language Vocabulary Learning with Emotional Binaural Narration",
    "authors": [
      "Shogo Fukushima"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we report on the design of a flashcard application with which learners experience the meaning of written words with emotional binaural voice narrations to enhance second language vocabulary learning. Typically, voice used in English vocabulary learning is recorded by a native speaker with no accent, and it aims for accurate pronunciation and clarity. However, the voice can also be flat and monotonous, and it can be difficult for learners to retain the new vocabulary in the semantic memory. Enhancing textual flashcards with emotional narration in the learner's native language helps the retention of new second language vocabulary items in the episodic memory instead of the semantic memory. Further, greater emotionality in the narration reinforces the retention of episodic memory.",
    "keywords": [
      "vocabulary learning",
      "binaural recording",
      "Emotion"
    ],
    "doi": "10.1145/3311823.3311867",
    "url": "https://doi.org/10.1145/3311823.3311867",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "46",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311856",
    "title": "AR Pottery Wheel-Throwing by Attaching Omnidirectional Cameras to the Center of a User's Palms",
    "authors": [
      "Yusuke Maruyama",
      "Yasuyuki Kono"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "This research describes our system for AR pottery wheel-throwing employing an HMD and omnidirectional cameras each of which is attached to the center of a user's palm. The omnidirectional cameras enable the user's finger postures and the three-dimensional relative position and orientation between the user's hands and virtual clay model on the wheel to be estimated. Our system detects a marker on the desk and the wheel is set on its coordinate system along with the finger posture estimation in real time. The system then simulates the collision between the virtual clay model and the left/right hand model based on the above information. Pottery wheel-throwing is reproduced in Unity software environment by deforming the clay model by contact with hand models in this simulation.",
    "keywords": [
      "Omnidirectional Camera",
      "Marker-based AR",
      "Finger Posture"
    ],
    "doi": "10.1145/3311823.3311856",
    "url": "https://doi.org/10.1145/3311823.3311856",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "47",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3313868",
    "title": "Demonstrating Naviarm: Augmenting the Learning of Motor Skills using a Backpack-type Robotic Arm System",
    "authors": [
      "Azumi Maekawa",
      "Shota Takahashi",
      "MHD Yamen Saraiji",
      "Sohei Wakisaka",
      "Hiroyasu Iwata",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AH",
    "conferenceYear": "AH2019",
    "abstract": "We present a wearable haptic assistance robotic system for augmented motor learning called Naviarm. This system comprises two robotic arms that are mounted on a user's body and are used to transfer one person's motion to another offline. Naviarm prerecords the arm motion trajectories of an expert via the mounted robotic arms and then plays back these recorded trajectories to share the expert's body motion with a beginner. The Naviarm system is an ungrounded system and provides mobility for the user to conduct a variety of motions. In our demonstration, the user will experience the recording of arm movement with backpack-type robotic arm. Then, the recorded movement will replayed and the user can experience the haptic feedback.",
    "keywords": [
      "Wearable Device",
      "Robotics",
      "Motor Learning",
      "Haptics",
      "Augmented learning"
    ],
    "doi": "10.1145/3311823.3313868",
    "url": "https://doi.org/10.1145/3311823.3313868",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "48",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174911",
    "title": "Air Mounted Eyepiece: Optical See-Through HMD Design with Aerial Optical Functions",
    "authors": [
      "Kazuki Otao",
      "Yuta Itoh",
      "Kazuki Takazawa",
      "Hiroyuki Osone",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "We propose a novel method of implementing an optical see-through (OST) head-mounted display (HMD) with a wide viewing angle and high resolution for augmented reality, called an Air Mounted Eyepiece (AME). In past years, many optical elements, such as transmissive liquid-crystal display (LCD), half-mirror, and waveguide have been adopted for OST-HMD. To achieve the AME design, we employ an off-the-shelf HMD and Transmissive Mirror Device (TMD), which is used in aerial real-imaging systems, instead of conventional optical elements. In the proposed method, we present 'Virtual lens,\" which has the same function as the HMD lens in front of the eyes. By using TMD, it is possible to shorten the optical length between the virtual lens and the eye. Therefore, the aerial lens provides an immersive image with see-through capability. In this paper, we describe a detailed design method of TMD-based HMD, and compare it to previous half mirror-based HMD and convex mirror-based HMD. Then, we construct a fabricated prototype of the OST-HMD using TMD. We aim to contribute to the field of human-computer interaction and the research on eyepiece interfaces by discussing the advantages and the limitations through simulations and experiments.",
    "keywords": [
      "Optical See-Through Display",
      "Transmissive Mirror Device",
      "Near-Eye Display",
      "Augmented Reality"
    ],
    "doi": "10.1145/3174910.3174911",
    "url": "https://doi.org/10.1145/3174910.3174911",
    "citations": 21,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "1",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174918",
    "title": "Investigation of Tracer Particles Realizing 3-Dimensional Water Flow Measurement for Augmented Swimming Training",
    "authors": [
      "Shogo Yamashita",
      "Shunichi Suwa",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Previous studies have revealed that understanding the 3D movement of water contributes to improving propulsion in the water when swimming. Fluid measurements are made by scattering tracer particles into a liquid, and cameras track the movement of these particles to measure the fluid's flow. Strong lasers illuminate tracer particles to make them visible to cameras, but 3D water flow measurements in wide spaces like swimming pools have not yet been successful. This can be owed to the limitations of current optical systems impacting the measurable parameters of existing methods, such as the laser capacity and lens size. Moreover, visualized tracer particles buoyed in swimming pools can affect the swimmer's view and may be harmful to humans when swallowed. Therefore, we propose a 3D water flow tracing technology with tracer particles suitable for a swimming pool. We use an optical property called optical rotation to track the tracer particles. This method would be effective in extending the measurable area of water flow than previous methods because it does not require the use of optical systems, which are technically difficult to expand. In this study, we investigated the materials and processing methods for creating tracer particles for augmented swimming training.",
    "keywords": [
      "Underwater Spatial Interaction",
      "Swimming",
      "Underwater Virtual Reality",
      "Fluid Measurement",
      "Augmented Sports"
    ],
    "doi": "10.1145/3174910.3174918",
    "url": "https://doi.org/10.1145/3174910.3174918",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "2",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174930",
    "title": "Couples Designing Their Living Room Together: A Study with Collaborative Handheld Augmented Reality",
    "authors": [
      "Joon Gi Shin",
      "Gary Ng",
      "Daniel Saakes"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we investigate the use of Augmented Reality for users designing together. We present a design application that runs on multiple synchronized and spatially aware tablets to support couples making interior decisions in and for their future living room. Based on the prior art into collaborative design, we suggest a novel design interface that deals with situated design and supports virtual workspaces. We asked six couples to design together with our prototype and analyzed their design process, the roles they took, and how they communicated. The results suggest that the social practice of couples designing in and for their home differs from professional design teams and involves more than just positioning furniture in space. We use the design, the prototype and the study to discuss implications for spatial in-situ tools concerning intimacy, collaboration, and design process. The findings are useful for future applications that deal with collaborative applications for casual users.",
    "keywords": [
      "Multi User",
      "User Study",
      "Augmented Reality",
      "Interior Design"
    ],
    "doi": "10.1145/3174910.3174930",
    "url": "https://doi.org/10.1145/3174910.3174930",
    "citations": 16,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "3",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174920",
    "title": "Augmenting Memory Recall in Work Meetings: Establishing a Quantifiable Baseline",
    "authors": [
      "Evangelos Niforatos",
      "Matías Laporte",
      "Agon Bexheti",
      "Marc Langheinrich"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "The proliferation of ubiquitous technologies has allowed us to capture increasing amounts of our daily life in digital format (\"lifelogging\"). While much work in lifelogging has focused on augmenting human memory in clinical settings, e.g., for people with dementia, the ability to recall our past obviously has also importance in one's work life. Being able to better recall a work meeting could improve coordination and collaboration among peers, ultimately raising overall productivity. To better understand the potential of memory augmentation technology for improving work meetings, we conducted a multi-week study with seven groups. Using a within-subjects design, participants in the experimental condition were, prior to a meeting, briefly presented with an automatically created memory augmentation aid (slides), based on captured data from a prior meeting. Our results show that a 3-4 minute exposure to our simple image-keywords slide deck prior to a meeting, increased our participants' ability to recall their previous meeting by up to 15 %. Our findings serve as an initial baseline against which future memory augmentation systems can be compared.",
    "keywords": [
      "Human Memory Augmentation",
      "Work Meetings",
      "Lifelogging"
    ],
    "doi": "10.1145/3174910.3174920",
    "url": "https://doi.org/10.1145/3174910.3174920",
    "citations": 9,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "4",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174946",
    "title": "Virtual Participation in Ukiyo-e Appreciation Using Body Motion",
    "authors": [
      "Shota Kusajima",
      "Takuya Takahashi",
      "Yasuyuki Sumi"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "This paper presents a novel method of art appreciation by participating in artwork using virtual reality technology. We chose a famous ukiyo-e work titled \"many people are assembled to create one good person\" drawn by Kuniyoshi Utagawa in the Edo period of Japan. The painting provides an illusion that it seems a one man but shows many (over ten) persons assembled to create the one person. We developed a 3D CG game where its player can join to create the big person by virtually participating into the artwork as the part of the big person. The system enabled its players to enjoy the game with their own body movements, which extends their experience of art appreciation. This paper describes an attempt to enrich our game players' experience under very limited temporal and spatial constraints given at a contest exhibition.",
    "keywords": [
      "Virtual reality",
      "Bodily kinesthetic game",
      "Content design",
      "Art appreciation"
    ],
    "doi": "10.1145/3174910.3174946",
    "url": "https://doi.org/10.1145/3174910.3174946",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "5",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174919",
    "title": "AR Timewarping: A Temporal Synchronization Framework for Real-Time Sensor Fusion in Head-Mounted Displays",
    "authors": [
      "Peter Kim",
      "Jason Orlosky",
      "Kiyoshi Kiyokawa"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "A significant issue associated with the use of head-mounted displays for augmented reality is the presence of latency between the real world and the augmented images displayed to the headset, which, if uncompensated for, results in registration error that may limit the effectiveness of the augmented information as well as bring about user discomfort. In addition, further temporal discrepancies arise in the case when fusing information from multiple cameras of different capture frequencies to construct the augmented image as, in addition to temporal misalignment between the real world and the augmented image, there is desynchronization among the different sensors that may also lead to registration mismatch.In order to address these temporal inconsistencies, we present AR Timewarping, a novel temporal synchronization framework that is particularly adapted for video see-through (VST) head-mounted displays and consists of two main algorithms, one for head motion and one for scene motion, that together act to temporally warp and merge information from multiple sensors to significantly improve registration in the augmented image.System tests of our algorithms show that we can reduce registration error between two unsynchronized video streams by 87.04% and 81.64% for registration error arising from head motion and scene motion, respectively. Results from a user experiment show that subjects' abilities to track a moving object were significantly improved by our algorithms, with a 32.14% average reduction in angular tracking error, and furthermore subjects rated the combined algorithms better overall than the base case in terms of both image clarity and user comfort.",
    "keywords": [
      "Vision augmentation",
      "augmented reality",
      "delay compensation",
      "sensor fusion"
    ],
    "doi": "10.1145/3174910.3174919",
    "url": "https://doi.org/10.1145/3174910.3174919",
    "citations": 5,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "6",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174914",
    "title": "Telewheelchair: The Remote Controllable Electric Wheelchair System Combined Human and Machine Intelligence",
    "authors": [
      "Satoshi Hashizume",
      "Ippei Suzuki",
      "Kazuki Takazawa",
      "Ryuichiro Sasaki",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Wheelchairs are essential means of transport for the elderly people and the physically challenged. However, wheelchairs need to be accompanied by caregivers. As society ages and the number of care recipients increases, the burden on caregivers is expected to increase. In order to reduce the burden on caregivers, we present Telewheelchair, an electric wheelchair equipped with a remote control function and computational operation assistance function. The caregiver can remotely control the Telewheelchair by means of a head mounted display (HMD). In addition, the proposed system is equipped with a human detection system to stop the wheelchair automatically and avoid collisions. We conducted a user study on the wheelchair in four types of systems and investigated the time taken to achieve tasks. Telewheelchair will enhance geriatric mobility and improve society by combining human intelligence and machine intelligence.",
    "keywords": [
      "telepresence",
      "nursing",
      "virtual reality",
      "Wheelchair"
    ],
    "doi": "10.1145/3174910.3174914",
    "url": "https://doi.org/10.1145/3174910.3174914",
    "citations": 20,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "7",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174940",
    "title": "Command Selection in Gaze-Based See-through Virtual Image-Guided Environments",
    "authors": [
      "Hoorieh Afkari",
      "David Gil de Gómez Pérez",
      "Roman Bednarik"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Embedded close-to-the-eye gaze tracking permits new types of interaction in see-through augmented and virtual environments. It is however unclear how gaze-input can be used to select and confirm commands when the tracking technologies are located at close proximity to user's eyes, but cannot utilize fixed geometry as in screen-based environments. We conducted a study in a simulated image-guided medical environment where users employed gaze-input to control an on-screen display. The current hand-based interaction of such views is a frequent source of interruption and thus feasibility of alternative input modalities has to be evaluated. We created a three-stage gaze-based confirmation mechanism and evaluated its robustness and the limits of the target size. Two sizes of the target for command selection were evaluated, occupying 12 and 6 degrees of visual angle at the 30cm distance. The results show the time to perform an action using gaze input is shorter than in hand-based interaction with the real-world device, confirming that this input modality is feasible. The size of target has little effect on the interaction and the completion-error is low. The findings have implications on the design of future gaze-based input methods for these devices.",
    "keywords": [
      "Gaze interaction",
      "surgical image-guided techniques",
      "surgical microscope",
      "VR",
      "gaze-based interaction"
    ],
    "doi": "10.1145/3174910.3174940",
    "url": "https://doi.org/10.1145/3174910.3174940",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "8",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174935",
    "title": "HIVE Tracker: A Tiny, Low-Cost, and Scalable Device for Sub-Millimetric 3D Positioning",
    "authors": [
      "Darío R. Quiñones",
      "Gonçalo Lopes",
      "Danbee Kim",
      "Cédric Honnet",
      "David Moratal",
      "Adam Kampff"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Positional tracking systems could hugely benefit a number of niches, including performance art, athletics, neuroscience, and medicine. Commercial solutions can precisely track a human inside a room with sub-millimetric precision. However, these systems can track only a few objects at a time; are too expensive to be easily accessible; and their controllers or trackers are too large and inaccurate for research or clinical use. We present a light and small wireless device that piggybacks on current commercial solutions to provide affordable, scalable, and highly accurate positional tracking. This device can be used to track small and precise human movements, to easily embed custom objects inside of a VR system, or to track freely moving subjects for research purposes.",
    "keywords": [
      "Open Source",
      "Tracker",
      "Indoor",
      "Low cost",
      "Motion Capture",
      "Virtual Reality",
      "Neuroscience",
      "Wireless-Sensor"
    ],
    "doi": "10.1145/3174910.3174935",
    "url": "https://doi.org/10.1145/3174910.3174935",
    "citations": 10,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "9",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174938",
    "title": "A Motion Recognition Method Using Foot Pressure Sensors",
    "authors": [
      "Ayumi Ohnishi",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes a method for recognizing postures and gestures using foot pressure sensors, and we investigate optimal positions for pressure sensors on soles are the best for motion recognition. In experiments, the recognition accuracies of 22 kinds of daily postures and gestures were evaluated from foot-pressure sensor values. Furthermore, the optimum measurement points for high recognition accuracy were examined by evaluating combinations of two foot pressure measurement areas on a round-robin basis. As a result, when selecting the optimum two points for a user, the recognition accuracy was about 93.6% on average. Although individual differences were seen, the best combinations of areas for each subject were largely divided into two major patterns. When two points were chosen, combinations of the near thenar, which is located near the thumb ball, and near the heel or point of the outside of the middle of the foot were highly recognized. Of the best two points, one was commonly the near thenar for subjects. By taking three points of data and covering these two combinations, it will be possible to cope with individual differences. The recognition accuracy of the averaged combinations of the best two combinations for all subjects was classified with an accuracy of about 91.0% on average. On the basis of these results, two types of pressure sensing shoes were developed.",
    "keywords": [
      "Shoes device",
      "Gesture recognition",
      "Insole",
      "Pressure sensor",
      "Foot Pressure",
      "Posture recognition"
    ],
    "doi": "10.1145/3174910.3174938",
    "url": "https://doi.org/10.1145/3174910.3174938",
    "citations": 13,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174913",
    "title": "Design and Study of a Multi-Channel Electrical Muscle Stimulation Toolkit for Human Augmentation",
    "authors": [
      "Michinari Kono",
      "Yoshio Ishiguro",
      "Takashi Miyaki",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Electrical Muscle Stimulation (EMS) has attracted many users and researchers to apply the technique for various usages. However, EMS products for research purpose are limited and open source hardware still has limitations. We present a multi-channel EMS toolkit for researchers and designers to develop their original ideas. The toolkit was designed to have isolated multiple channels to be manipulated simultaneously, which allows simultaneous control of multiple body parts and multiple users. We organized a workshop with a theme of human augmentation, where EMS was not compelled to use. As a result, several groups found interest in using EMS for their development, and participants successfully used our toolkit for their presentation. We found that multi-channel EMS has a significant demand for human augmentation purposes.",
    "keywords": [
      "Human Augmentation",
      "Prototyping",
      "Wearable",
      "Toolkit",
      "Haptic Feedback",
      "Electrical Muscle Stimulation"
    ],
    "doi": "10.1145/3174910.3174913",
    "url": "https://doi.org/10.1145/3174910.3174913",
    "citations": 22,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "11",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174927",
    "title": "Sync Class: Visualization System for In-Class Student Synchronization",
    "authors": [
      "Katsuya Fujii",
      "Plivelic Marian",
      "Dav Clark",
      "Yoshi Okamoto",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "The engagement of students in a classroom is an important factor in education. Teachers are not only expected to possess strong teaching skills but also be simultaneously attentive to their students. Researchers have found that the most effective teaching happens when students in the room are in sync with one another.[1]However, assessing the classroom environment is not always easy for teachers especially when they teach multiple students at a time. The number of students per classroom can vary from a handful of student to over 50, this in turn can cause difficulty for teachers to provide an equal amount of attention per student, or even to be clearly attentive to the entire class environment.In this paper, we introduce an intelligent support system called \"Sync Class\" that helps teachers have access to a quantitative representation of how much students are in sync in a classroom. The system has a web camera placed in front of the classroom observing each student's face to detect students' engagement with the teacher. The system visualizes students' engagement in real time and alternatively, the system can also be used to analyze each student's attentiveness after class.We conducted several experiments to assess and evaluate the accuracy and capability of the system while also establishing usability for teachers. The teachers who tested our system, especially novice teachers, find it helpful to better understand student engagement whilst teaching. They are eager to utilize the system in future classes to respond to students, especially those who are disengaged, and better support their learning by effectively and promptly deploying interventions when needed.",
    "keywords": [
      "Education",
      "Human Computer Interaction",
      "Classroom",
      "User Interface"
    ],
    "doi": "10.1145/3174910.3174927",
    "url": "https://doi.org/10.1145/3174910.3174927",
    "citations": 23,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "12",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174926",
    "title": "AVATAREX: Telexistence System Based on Virtual Avatars",
    "authors": [
      "Timo Koskela",
      "Mounib Mazouzi",
      "Paula Alavesa",
      "Minna Pakanen",
      "Ilya Minyaev",
      "Eero Paavola",
      "Jere Tuliniemi"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "The telexistence technology can provide many kinds of benefits for the society. These include new ways of remote work, empowerment of handicapped and elderly people, and creation of new immersive and environmentally-friendly forms of tourism, travel, shopping, sports and leisure time activities. In this paper, we introduce AVATAREX, a telexistence system based on virtual avatars. AVATAREX provides means for connecting users that are simultaneously occupying the same space in the real world and its virtual replica. Using an indoor prototype implementation of AVATAREX and a simple collaborative game, we investigated how users experience co-presence in a telexistence system based on virtual avatars and measured the performance of AVATAREX on high-end smart glasses. Based on our findings, users wearing virtual reality gear reported a stronger sense of co-presence compared to users wearing augmented reality gear. Unexpectedly, users wearing smart glasses reported a lower sense of co-presence than users using a tablet for augmented reality experience.",
    "keywords": [
      "user study",
      "augmented reality",
      "mixed reality",
      "virtual reality"
    ],
    "doi": "10.1145/3174910.3174926",
    "url": "https://doi.org/10.1145/3174910.3174926",
    "citations": 12,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "13",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174937",
    "title": "V8 Storming: How Far Should Two Ideas Be?",
    "authors": [
      "Yui Kita",
      "Jun Rekimoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "All innovative ideas are said to be pairings of existing ideas. In collaborative ideation, various methods are used to suggest words for stimulating ideation based on the theory that suggesting strongly related words to the ideation topic will activate and enhance the ideation. However, the relationship between the words suggested by the system and the words that users associate remains unclear. Solving this issue will help selecting words suggestion in computer-supported ideation systems, in which the information flow is fluid and, therefore, users have strictly limited time for interaction. To address this issue, we studied the relationship between pairs of words suggested by computer and the quantity of associated ideas by users. We also conducted a user study on practical applications.",
    "keywords": [
      "Collaborative Ideation",
      "Human Computer Interaction",
      "Association"
    ],
    "doi": "10.1145/3174910.3174937",
    "url": "https://doi.org/10.1145/3174910.3174937",
    "citations": 9,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174922",
    "title": "Seeing is Smelling: Localizing Odor-Related Objects in Images",
    "authors": [
      "Sangyun Kim",
      "Junseok Park",
      "Junseong Bang",
      "Haeryong Lee"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Research on cross-modal associations between vision and olfaction has shown that odor perception can be strongly influenced by vision including odor-evoked objects. Recently, convolutional neural networks (CNNs), which is mainly applied to image recognition by learning image features, is driving the development of a visual recognition method with promising results. However, there is no attempts to recognition and localization of odor-related objects which allow us to facilitate detection of objects on scene when a congruent odor stimuli is released from scent device. The existing object localization methods require bounding-box annotation indicating the presence of object and location information in an image which is too costly because it is a time-consuming process. Image-level annotation, which indicates the presence of objects in an image, is easier to obtain than bounding-box annotation. In this work, we perform weakly supervised object localization using features of CNNs with only image-level annotation data to detect odor-related objects. We propose a method to classify and localize odor-related objects, and describe its implementation in detail. The experimental results indicate that its performance is comparable to previous CNNs for the classification of odor-related objects.",
    "keywords": [
      "Olfaction and vision cross-modal",
      "Convolutional neural networks",
      "Weakly supervised object localization",
      "Olfaction-enhanced multimedia"
    ],
    "doi": "10.1145/3174910.3174922",
    "url": "https://doi.org/10.1145/3174910.3174922",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "15",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174936",
    "title": "GlassPass: Tapping Gestures to Unlock Smart Glasses",
    "authors": [
      "MD. Rasel Islam",
      "Doyoung Lee",
      "Liza Suraiya Jahan",
      "Ian Oakley"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Wearable technologies such as smart-glasses can sense, store and display sensitive personal contents. In order to protect this data, users need to securely authenticate to their devices. However, current authentication techniques, such as passwords or PINs, are a poor fit for the limited input and output spaces available on wearables. This paper focuses on eyewear and addresses this problem with a novel authentication system that uses an alphabet of simple tapping patterns optimized for rapid and accurate input on the temples (or arms) of glasses. Furthermore, it explores how an eyewear display can support password memorization by privately presenting a visualization of entered symbols. A pair of empirical studies confirm that performance during input of both individual password symbols and full passwords is rapid and accurate. A follow-up session one week after the main study suggests using a private display to show entered password symbols effectively supports memorization.",
    "keywords": [
      "Authentication",
      "Security",
      "Smart glasses",
      "Usability",
      "Visual feedback",
      "Memorability",
      "PIN entry"
    ],
    "doi": "10.1145/3174910.3174936",
    "url": "https://doi.org/10.1145/3174910.3174936",
    "citations": 17,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "16",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174929",
    "title": "Open Palm Menu: A Virtual Menu Placed in Front of the Palm",
    "authors": [
      "Takumi Azai",
      "Mai Otsuki",
      "Fumihisa Shibata",
      "Asako Kimura"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Virtual and mixed realities make it possible to view and interact with virtual objects in a three-dimensional space. However, the location to display menus in three-dimensional space and the means of manipulating them are often problems. Existing studies developed methods of displaying a menu in the air or on the body. In this paper, a menu system is proposed that appears in front of the user's palm (of the non-dominant hand) when he/she opens that hand. The user employs the other hand (dominant hand) to then interact with the menu. Using the space around the body as opposed to projections onto actual limbs makes it possible to display more items in more varied layouts. Additionally, the user can control rendering of the menu by opening or closing the hand. Being adjacent to the palm of the open hand also enables the user to adjust the hand position to operate the menu more comfortably or to move the menu to an area where it is easier to view. In this study, we conducted an experiment to develop the menu design guidelines to ensure smooth menu operation, designed an optimal menu, and tested that menu by determining the ease of use in interacting with our modeling application.",
    "keywords": [
      "Head-mounted display",
      "Mixed reality",
      "Menu",
      "Virtual reality",
      "Gestural input"
    ],
    "doi": "10.1145/3174910.3174929",
    "url": "https://doi.org/10.1145/3174910.3174929",
    "citations": 30,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174944",
    "title": "Recognition and Feedback of Vowel Utterance with a Good Mouth Shape Based on Sensing Platysma Muscle Bulging",
    "authors": [
      "Yukihiro Nishimura",
      "Tomoko Hashida"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In public speaking, speakers are evaluated on verbal delivery and nonverbal delivery, and in particular, the mouth shape has an important role to support both of these. The mouth shape is mainly set during vowel utterance. We define the mouth shape, which can prompt the pronunciation of the speaker clearly and enrich the facial expression, as a good mouth shape in this research. The authors assume that a good mouth shape can be inferred from the bulging of the platysma muscle in the neck. We aim to support vowel utterances with a good mouth shape, and propose a system to recognize them. Specifically, we measure the uplift of the platysma muscle with photoreflectors and apply a machine learning method to implement a system to judge whether vowel utterances are being performed with a good shape. We conduct an accuracy measurement experiment of the proposed system and report the result. Finally, we describe the application that provides feedback of vowel utterances with a good mouth shape.",
    "keywords": [
      "Public Speech",
      "Machine Learning",
      "Mouth Shape",
      "Presentation Training"
    ],
    "doi": "10.1145/3174910.3174944",
    "url": "https://doi.org/10.1145/3174910.3174944",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "18",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174948",
    "title": "Wanding Through Space: Interactive Calibration for Electric Muscle Stimulation",
    "authors": [
      "Henning Pohl",
      "Kasper Hornbæk",
      "Jarrod Knibbe"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Electric Muscle Stimulation (EMS) has emerged as an interaction paradigm for HCI. It has been used to confer object affordance, provide walking directions, and assist with sketching. However, the electrical signals used for EMS are multi-dimensional and require expert calibration before use. To date, this calibration has occurred as a collaboration between the experimenter, or interaction designer, and the user/participant. However, this is time-consuming, results in sampling only a limited space of possible signal configurations, and removes control from the participant. We present a calibration and signal exploration technique that both enables the user to control their own stimulation and thus comfort, and supports exploration of the continuous space of stimulation signals.",
    "keywords": [
      "haptic feedback",
      "functional electrical stimulation",
      "EMS",
      "calibration",
      "Electric muscle stimulation"
    ],
    "doi": "10.1145/3174910.3174948",
    "url": "https://doi.org/10.1145/3174910.3174948",
    "citations": 14,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "19",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174916",
    "title": "GoalBaural: A Training Application for Goalball-Related Aural Sense",
    "authors": [
      "Takahiro Miura",
      "Shimpei Soga",
      "Masaki Matsuo",
      "Masatsugu Sakajiri",
      "Junji Onishi",
      "Tsukasa Ono"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Goalball, one of the official Paralympic events, is popular with visually impaired people all over the world. The purpose of goalball is to throw the specialized ball, with bells inside it, to the goal line of the opponents as many times as possible while defenders try to block the thrown ball with their bodies. Since goalball players cannot rely on visual information, they need to grasp the game situation using their auditory sense. However, it is hard, especially for beginners, to perceive the direction and distance of the thrown ball. In addition, they generally tend to be afraid of the approaching ball because, without visual information, they could be hit by a high-speed ball. In this paper, our goal is to develop an application called GoalBaural (Goalball + aural) that enables goalball players to improve the recognizability of the direction and distance of a thrown ball without going onto the court and playing goalball. The evaluation result indicated that our application would be efficient in improving the speed and the accuracy of locating the balls.",
    "keywords": [
      "Goalball",
      "visually impaired people",
      "acoustical virtual reality"
    ],
    "doi": "10.1145/3174910.3174916",
    "url": "https://doi.org/10.1145/3174910.3174916",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "20",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174934",
    "title": "An Interactive 4D Vision Augmentation of Rapid Motion",
    "authors": [
      "Tao Tao",
      "Photchara Ratsamee",
      "Yuki Uranishi",
      "Kiyoshi Kiyokawa",
      "Tomohiro Mashita",
      "Haruo Takemura"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "We propose an interactive 4D visualization (3D space with an additional time dimension) for the purpose of understanding rapid motion in dynamic 3D information. With an interactive 4D visualization, the user can not only observe dynamic 3D motion from different viewpoints, but also freely adapt the speed of visualization on each viewpoint. We developed a system by reconstructing 3D data from an RGBD camera into a VR environment so that the user can visualize the 3D information via an HMD. The experiment results show that our proposed system outperforms conventional 2D and 3D visualizations in terms of both the user's recognition accuracy and view counts when observing rapid motion.",
    "keywords": [
      "Interactive 4D Visualization",
      "3D Motion Data",
      "Slow Motion",
      "Augmented Reality"
    ],
    "doi": "10.1145/3174910.3174934",
    "url": "https://doi.org/10.1145/3174910.3174934",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "21",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174923",
    "title": "A Detachable Exoskeleton Interface That Duplicates the User's Hand Posture and Motions",
    "authors": [
      "Genki Toyama",
      "Tomoko Hashida"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "We propose a technology that duplicates the posture and motions of a person's fingers and separates it from the original fingers, thereby enabling it to be used as a tool. In this study, we present a detachable exoskeleton interface that can record and replay the posture and motions of fingers using a micro servo motor and a telescopic mechanism. The micro servo motor is improved so that both the input and output of angles can be performed, and the telescopic mechanism has a variable length between joints. Wearing this system and recording the posture and motions of the fingers enabled us to test the mechanism for arranging and reproducing them in the real world independently of the user. In this paper, we describe the system implementation, an experiment to check its accuracy, two types of application, and a user study to check its usefulness.",
    "keywords": [
      "telescopic mechanism",
      "Exoskeleton",
      "hand posture and motions",
      "micro servo motor"
    ],
    "doi": "10.1145/3174910.3174923",
    "url": "https://doi.org/10.1145/3174910.3174923",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "22",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174924",
    "title": "FaceRubbing: Input Technique by Rubbing Face Using Optical Sensors on Smart Eyewear for Facial Expression Recognition",
    "authors": [
      "Katsutoshi Masai",
      "Yuta Sugiura",
      "Maki Sugimoto"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "With the emergence of the wearable devices, the method to make use of the limited input space is required. This paper presents an input technique to a computer by rubbing face using optical sensors on smart eyewear. Since rubbing gesture occurs in daily life, our system enables a subtle interaction between the user and a computer. We used the smart eyewear based on the work by [5]. Although the device is developed for facial expression recognition, our method can recognize rubbing gesture independent from facial expression recognition.The embedded optical sensors measure the skin deformation caused by rubbing on the face. We detect the gestures using principal component analysis (PCA) and peak detection. we classify the area of the gesture with a random forest classifier. The accuracy of detecting rubbing gesture is 97.5%. The classification accuracy of 10 gesture area is 88.7% with user-independent training. The system can open up a new interaction method for smart glasses.",
    "keywords": [
      "Wearable Computing",
      "Eyewear Computing",
      "Input Technique"
    ],
    "doi": "10.1145/3174910.3174924",
    "url": "https://doi.org/10.1145/3174910.3174924",
    "citations": 19,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "23",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174933",
    "title": "A Soft Exoskeleton Suit to Reduce Muscle Fatigue with Pneumatic Artificial Muscles",
    "authors": [
      "Kosuke Tsuneyasu",
      "Ayumu Ohno",
      "Yoshiyuki Fukuda",
      "Kazunori Ogawa",
      "Toshio Tsuji",
      "Yuichi Kurita"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "A major issue at work sites corresponds to aging workers, and the associated back pain. In this research, we develop an assistive suit with light-weight and flexible pneumatic rubber artificial muscles to reduce muscle load. Two assist forces are designed to control the artificial muscles with PWM-control based on: 1) flexion angle, and 2) estimated torque of the hip joint. The experimental results show that the proposed suit can reduce muscle activity during bending and stretching motions.",
    "keywords": [
      "artificial muscle",
      "pneumatic artificial muscle",
      "assistive suit"
    ],
    "doi": "10.1145/3174910.3174933",
    "url": "https://doi.org/10.1145/3174910.3174933",
    "citations": 12,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "24",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174921",
    "title": "Object-Wise 3D Gaze Mapping in Physical Workspace",
    "authors": [
      "Kakeru Hagihara",
      "Keiichiro Taniguchi",
      "Irshad Abibouraguimane",
      "Yuta Itoh",
      "Keita Higuchi",
      "Jiu Otsuka",
      "Maki Sugimoto",
      "Yoichi Sato"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Understanding the intention of other people is a fundamental social skill in human communication. Eye behavior is an important, yet implicit communication cue. In this work, we focus on enabling people to see the users' gaze associated with objects in the 3D space, namely, we present users the history of gaze linked to real 3D objects. Our 3D gaze visualization system automatically segments objects in the workspace and projects user's gaze trajectory onto the objects in 3D for visualizing user's intention. By combining automated object segmentation and head tracking via the first-person video from a wearable eye tracker, our system can visualize user's gaze behavior more intuitively and efficiently compared to 2D based methods and 3D methods with manual annotation. We performed an evaluation of the system to measure the accuracy of object-wise gaze mapping. In the evaluation, the system achieved 94% accuracy of gaze mapping onto 40, 30, 20, 10-centimeter cubes. We also conducted a case study of through a case study where the user looks at food products, we showed that our system was able to predict products that the user is interested in.",
    "keywords": [
      "gaze mapping",
      "3D segmentation",
      "3D gaze"
    ],
    "doi": "10.1145/3174910.3174921",
    "url": "https://doi.org/10.1145/3174910.3174921",
    "citations": 8,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "25",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174928",
    "title": "Enhanced Pressure-Based Multimodal Immersive Experiences",
    "authors": [
      "Taeyong Kim",
      "Jeremy R. Cooperstock"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Haptic feedback to the feet has been explored in the context of ground surface simulation, but existing solutions have generally not taken into account the effects of variable foot pressure during naturalistic stepping movements. We present here our approach to simulating the surface of a frozen pond, including ice cracking under increased foot pressure. This serves as a compelling example of a wearable mobile augmented foot-based surface simulator, whose response varies as a function of applied foot pressure. To enhance the illusion, we render multi-sensory feedback, comprising audio, visual, and haptic effects. We describe the hardware employed, the algorithm used to determine the distribution of foot pressure, and suggest potential possibilities for such a foot-pressure-based augmented reality system.",
    "keywords": [
      "Haptic Effect",
      "Immersive Environment",
      "Surface Textures",
      "Wearable Computing",
      "Augmented Human",
      "Foot-based Interface"
    ],
    "doi": "10.1145/3174910.3174928",
    "url": "https://doi.org/10.1145/3174910.3174928",
    "citations": 6,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "26",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174912",
    "title": "VR Planning Toolkit to Simulate Physical and Virtual Configurations: A Case Study of an Indoor VR Roller Coaster Augmenting Experience",
    "authors": [
      "Dongsik Jo",
      "Yongwan Kim",
      "Woojin Jeon",
      "Yongsun Kim",
      "Hongki Kim",
      "Ki-Hong Kim",
      "Seungho Kwak"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes a novel VR planning toolkit to simulate physical configurations of the actual space such as physical sensors of user's surrounding space (e.g. tracking sensors, IoT devices, muscle interfaces, exoskeletons) in advance, and simultaneously create a VR content mixed for the configured environment. This toolkit provides guidelines for configurations of real and virtual spaces at the same time, and allows the user to bridging two spaces that need to be simulated. As a case study of our proposed VR planning toolkit, we have developed a prototype to test indoor VR roller coaster, and applied it as a usage scenario that can be experienced.",
    "keywords": [
      "planning",
      "simulation",
      "Virtual reality",
      "coaster",
      "authoring"
    ],
    "doi": "10.1145/3174910.3174912",
    "url": "https://doi.org/10.1145/3174910.3174912",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "27",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174943",
    "title": "Augmenting Smart Object Interactions with Smart Audio",
    "authors": [
      "Jing Yang",
      "Gábor Sörös"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "The auditory output channel is rather under-utilized in smart object to human communication. One reason is that in a smart environment, multiple overlapping audio sources can be disturbing to people. We propose a wearable audio augmentation system which allows people to effortlessly select and switch between sound sources given their interest. Our system leverages visual contact via the head pose as a measure of interest towards a smart object. We demonstrate a prototype implementation in three application scenarios and a preliminary user evaluation.",
    "keywords": [
      "Smart objects",
      "Audio augmentation",
      "Human-object communication"
    ],
    "doi": "10.1145/3174910.3174943",
    "url": "https://doi.org/10.1145/3174910.3174943",
    "citations": 1,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "28",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174939",
    "title": "Towards Enhancing Emotional Responses to Media Using Auto-Calibrating Electric Muscle Stimulation (EMS)",
    "authors": [
      "Takashi Goto",
      "Benjamin Tag",
      "Kai Kunze",
      "Tilman Dingler"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "We evaluate the use of Electric Muscle Stimulation (EMS) as a method of amplifying emotional responses to multimedia content. This paper presents an auto-calibration method to stimulate two facial expressions using EMS. We focus on two expressions: frown and smile. We attempted control of facial muscles with facial feedback for automatically calibrating these facial expressions: our computer vision system detects the facial expression and auto-calibrates the EMS parameters (intensity and duration) based on the user's current facial expression. We present results from a pilot study with four participants evaluating the auto-calibration system and collecting initial feedback on the use of EMS to augment, for example, media experiences: while watching movies we can enhance the emotional response of the users during happy and sad scenes by stimulating corresponding face muscles.",
    "keywords": [
      "facial feedback",
      "Affective computing",
      "Electrical Muscle Stimulation (EMS)",
      "emotion"
    ],
    "doi": "10.1145/3174910.3174939",
    "url": "https://doi.org/10.1145/3174910.3174939",
    "citations": 11,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "29",
    "numpages": "2",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3174910.3174932",
    "title": "Reinforced Suit Using Low Pressure Driven Artificial Muscles For Baseball Bat Swing",
    "authors": [
      "Watura Sakoda",
      "Antonio Vega Ramirez",
      "Kazunori Ogawa",
      "Toshio Tsuji",
      "Yuichi Kurita"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "This paper proposes an assistive device to augment the sports activity. This experiment presents a bat swing augmentation suit for baseball that can provide automatically the assistive timing required to improve the performance of the user, this action is determined by the system. To generate the operation at the appropriate timing, an acceleration sensor, electric valves, and a microcomputer are attached on the suit. We conduct the experiment to confirm the performance of the suit by measuring the swinging speed. The experimental results showed that the device can improve the swinging speed of experienced subjects about 3 km/h.",
    "keywords": [
      "Pneumatic Artificial Muscle",
      "Baseball",
      "Smart Suit",
      "Powered Suit",
      "Human Extension"
    ],
    "doi": "10.1145/3174910.3174932",
    "url": "https://doi.org/10.1145/3174910.3174932",
    "citations": 7,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "30",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174931",
    "title": "Visual Field Visualizer: Easier &amp; Scalable Way to Be Aware of the Visual Field",
    "authors": [
      "Ngoc Thi Nguyen",
      "Suranga Nanayakkara",
      "Hyowon Lee"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Current practices in visual field tests require skilled eye care professionals and expensive machine setup in a controlled environment [1], thus, limit their availability within eye care clinics and hospitals. With a low-cost Virtual Reality (VR) device and its immersive capability, we developed Visual Field Visualizer (VFV), a prototype VR system that enables easy and scalable visual field screening. This paper aims to discuss our iterative design approach on user interfaces and usability improvement of visual field tests. Two pilot studies, conducted with 39 participants, received high users' acceptance.",
    "keywords": [
      "Visual field",
      "Eye Screening",
      "Augmented Awareness",
      "VR Glasses",
      "Virtual Reality",
      "Automated Perimetry"
    ],
    "doi": "10.1145/3174910.3174931",
    "url": "https://doi.org/10.1145/3174910.3174931",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "31",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174915",
    "title": "Exo-Balancer: Design Method of Personalized Stabilizers for Shooting Actions",
    "authors": [
      "Ryota Kawamura",
      "Kazuki Takazawa",
      "Riku Iwasaki",
      "Kenta Yamamoto",
      "Yoichi Ochiai"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In photography and videography, it is a huge challenge to align sight towards a moving target continuously and steadily often requiring considerable practice and experience. Blurry photos are often taken by camera users who lack requisite skills. To address this problem, stabilizers have been designed. Conventional stabilizers introduce a steep learning curve, because they are designed to be mass-produced and not tailored to the individual. Therefore, we present a design method of personalized stabilizers for shooting actions. Our system requires users to input their body data. Then, the system proposes the suitable position of the camera to be harnessed to the user considering the moments of force of both the user and camera.",
    "keywords": [],
    "doi": "10.1145/3174910.3174915",
    "url": "https://doi.org/10.1145/3174910.3174915",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "32",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174941",
    "title": "Restorative Effects of Exercise in Virtual Environments",
    "authors": [
      "Taekyu Kim",
      "Sangwon Lee"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Development of virtual reality devices cannot consider effects of exercise experiences by virtual environments. In this paper, we compared exercise experience by 3 environments (Indoor, Virtual urban, Virtual nature), focusing on restorative effects. We conducted the experiment that rode a bicycle while watching 360 degrees video in each environment. And then, participants conducted the survey about calmness and measured their inter-beat interval from heart-rate. The result showed that calmness is high in virtual nature environments and correlates with inter-beat interval. Our results demonstrate that restorative effects with exercise experiences can be appeared through embodiment with virtual nature. This paper suggests the importance of considering virtual environments and a possibility of application of natural environments to mixed or augmented reality.",
    "keywords": [
      "Exercise Experience",
      "Virtual Reality Exercise",
      "Virtual Environments",
      "Restorative Effects"
    ],
    "doi": "10.1145/3174910.3174941",
    "url": "https://doi.org/10.1145/3174910.3174941",
    "citations": 6,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "33",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174947",
    "title": "TouchSense: Classifying and Measuring the Force of Finger Touches with an Electromyography Armband",
    "authors": [
      "Vincent Becker",
      "Pietro Oldrati",
      "Liliana Barrios",
      "Gábor Sörös"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "We present TouchSense, a system to classify and to compute the force of finger touches using an inexpensive, off-the-shelf electromyography (EMG) armband. From EMG input only, we classify the finger touches and estimate the force applied when pressing an object or surface with the thumb, forefinger, or middle finger. We propose a novel neural network architecture for finger classification using EMG data. Our system runs in real time and only utilizes the Thalmic Labs Myo EMG armband and an Android smartphone, thereby being wearable and mobile. We showcase one application for our system, which controls the brightness of a lamp.",
    "keywords": [
      "Interaction",
      "EMG",
      "Wearable Computing",
      "Touch-based interfaces",
      "CNN"
    ],
    "doi": "10.1145/3174910.3174947",
    "url": "https://doi.org/10.1145/3174910.3174947",
    "citations": 15,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "34",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174951",
    "title": "Paralogue: A Remote Conversation System Using a Hand Avatar Which Postures Are Controlled with Electrical Muscle Stimulation",
    "authors": [
      "Shin Hanagata",
      "Yasuaki Kakehi"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we propose a new method of communication with a remote partner. The system, which we call \"Paralogue\" (parasitic + dialogue), utilizes the user's arm and a hand as an avatar that represents the remote conversation partner. This method is realized by controlling the movement of the user's arm with electrical muscle stimulations(EMS) and making the user's arm behave as if the remote conversation partner were speaking to the user. Therefore, it is possible to add the physical presence of the remote conversation partner to the situation. This can also be a telepresence system using real human arms. However, unlike many telepresence systems, external actuators and large-sized devices are not required for this system and can be completed by only utilizing physical interaction. In this paper, we present its design and implementation.",
    "keywords": [
      "Electrical Muscle Stimulation",
      "Hand Gesture",
      "Telepresence"
    ],
    "doi": "10.1145/3174910.3174951",
    "url": "https://doi.org/10.1145/3174910.3174951",
    "citations": 13,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "35",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174957",
    "title": "MOYA: Interactive AI Toy for Children to Develop Their Language Skills",
    "authors": [
      "Ji Yoon Ahn",
      "Dong Wan Kim",
      "Yong Hyeon Lee",
      "Woori Kim",
      "Jeong Kuk Hong",
      "Yeonbo Shim",
      "Jin Ho Kim",
      "Juhyun Eune",
      "Seong-Woo Kim"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "The main concept of MOYA, the interactive AI toy, is to provide an answer to the children's curiosity about an object's name. MOYA is developed on the basis of four functionalities: hand motion tracking, image classification, robot motion control and graphic user interface. Raspberry PI, Ubuntu Mate OS, Arduino, OpenCV, TensorFlow, RealSense SDK and ROS are integrated for MOYA to accomplish some interactive tasks.",
    "keywords": [
      "Artificial Intelligence",
      "Motion control",
      "Hand motion tracking",
      "Interactive robot",
      "Deep Learning",
      "Image Classification Algorithm"
    ],
    "doi": "10.1145/3174910.3174957",
    "url": "https://doi.org/10.1145/3174910.3174957",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "36",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174942",
    "title": "E-Mmersive Book: The AR Book That Assists the Syntopical Reading",
    "authors": [
      "Shinhyo Kim",
      "Jihyun Park",
      "Jusub Kim"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Recently, researchers have actively explored ways to enhance reading experiences by utilizing Augmented Reality (AR) technologies. Augmenting a 3D virtual model on top of the real story book or science textbook is the typical way of using AR to enhance user experiences of reading activities. We present a new AR-based book system, e-mmersive Book, that assists the syntopical reading, where users read more than one book in an inspectional way to find answers to particular questions. Our new AR-based book system provides new possibilities to enhance reading experiences for students and researchers.",
    "keywords": [
      "Reading experience",
      "Augmented Reality",
      "AR books"
    ],
    "doi": "10.1145/3174910.3174942",
    "url": "https://doi.org/10.1145/3174910.3174942",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "37",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174955",
    "title": "Unconstrained Neck: Omnidirectional Observation from an Extra Robotic Neck",
    "authors": [
      "Lichao Shen",
      "Mhd Yamen Saraji",
      "Kai Kunze",
      "Kouta Minamizawa"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Humans are born with physiological limitations in terms of the sensory and motor abilities. Due to the narrow range of motion of the neck and the small visual field of the eyes, the human visual sense is limited in terms of the spatial range. We address this visual limitation by proposing a programmable neck that can leverage the range of motion limits. Unconstrained Neck, a head-mounted robotic neck, is a substitution neck system which provides a wider range of motion enabling humans to overcome the physical constraints of the neck. Using this robotic neck, it is possible to control the visual/motor gain which allows the user to thus control the range and speed of his effective neck motion or visual motion.",
    "keywords": [
      "robotic neck",
      "vision expansion",
      "biomimetics",
      "human augmentation"
    ],
    "doi": "10.1145/3174910.3174955",
    "url": "https://doi.org/10.1145/3174910.3174955",
    "citations": 4,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "38",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174952",
    "title": "Distributed Metaverse: Creating Decentralized Blockchain-Based Model for Peer-to-Peer Sharing of Virtual Spaces for Mixed Reality Applications",
    "authors": [
      "Bektur Ryskeldiev",
      "Yoichi Ochiai",
      "Michael Cohen",
      "Jens Herder"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Mixed reality telepresence is becoming an increasingly popular form of interaction in social and collaborative applications. We are interested in how created virtual spaces can be archived, mapped, shared, and reused among different applications. Therefore, we propose a decentralized blockchain-based peer-to-peer model of distribution, with virtual spaces represented as blocks. We demonstrate the integration of our system in a collaborative mixed reality application and discuss the benefits and limitations of our approach.",
    "keywords": [
      "Photospherical Imagery",
      "Blockchain",
      "Mixed Reality",
      "Telepresence",
      "Groupware",
      "Social Media",
      "Spatial Media",
      "Mobile Computing"
    ],
    "doi": "10.1145/3174910.3174952",
    "url": "https://doi.org/10.1145/3174910.3174952",
    "citations": 77,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "39",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174954",
    "title": "Introducing Smart Pillow Using Actuator Mechanism, Pressure Sensors, and Deep Learning-Based ASR",
    "authors": [
      "Seung Hee Yang",
      "Sangwoo Park",
      "Taemyung Yang",
      "Ilhyung Jin",
      "Wooil Kim",
      "Chingwei Liu",
      "Seong-Woo Kim",
      "Juhyun Eune"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Sleeping is one of the most essential activities in human lives. It plays a vital role in good health and well-being of human lives. However, WHO (World Health Organization)'s research shows that approximately 27 percent of worldwide population are suffering from sleeping problems, which may affect everyday activities. Amongst various sleeping aid products, pillows can provide the solution to alleviate sleep disorders, neck pain, and chronic fatigue. The adjustable or smart pillows in today's market have some limitations. For instance, there are only few fixed forms and there is only one way interaction from human to the smart pillow. We further improve the existing smart pillow technology in terms of higher transformability and better human-pillow interactions which is bidirectional, thanks to the actuator mechanism, pressure sensors and ASR system.",
    "keywords": [
      "Actuator mechanism",
      "Pressure sensor",
      "Smart Pillow",
      "Automatic speech recognition",
      "Sleeping quality",
      "Augmented Human"
    ],
    "doi": "10.1145/3174910.3174954",
    "url": "https://doi.org/10.1145/3174910.3174954",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "40",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174945",
    "title": "Augmented Memory: Site-Specific Social Media with AR",
    "authors": [
      "Seonghun Park",
      "Jusub Kim"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In this paper, we propose a new social media application which can allow one to save site-specific memories in virtual space and share them with other users. This application allows users to leave traces in visiting places avoiding damaging the physical places while satisfying the human desire to leave and share their traces. The AR-based site-specific social media application provides new opportunities for making social relationship.",
    "keywords": [
      "Augmented Reality",
      "Social Media",
      "Mixed Reality"
    ],
    "doi": "10.1145/3174910.3174945",
    "url": "https://doi.org/10.1145/3174910.3174945",
    "citations": 1,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "41",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174949",
    "title": "SmartFiber: Reconfigurable Shape Changing Interface",
    "authors": [
      "Masaru Ohkubo",
      "Takuya Nojima"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "As a way to augment shape and movement of a tangible object, we propose an actuator interface called \"SmartFiber\". Different from our previous work, SmartHair[7], this interface integrates an actuation and information network. The SmartFiber is composed of a silicon tube and two kinds of wires: (1) a pair of shape memory alloy wires and (2) copper wires. These copper wires contain a signal bus, which comprises an information network of shape changing interfaces. Within the network, actuators bend, shrink and rotate according to a control signal on the bus. By connecting to a power supply and sensor-integrated connectors, the user can compose a physically augmented object that has additional shape and movement. This system contributes to composing an augmented interface, which is a material-controllable extension of an object. The chain and mesh of the interface enable reconfiguration of the tangible object's functions by attaching new shapes and movements above its surface. In this paper we present a summary of the concept and the current prototype.",
    "keywords": [
      "Tangible",
      "Augmentation",
      "SmartHair",
      "Hairlytop Interface"
    ],
    "doi": "10.1145/3174910.3174949",
    "url": "https://doi.org/10.1145/3174910.3174949",
    "citations": 2,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "42",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3174910.3174953",
    "title": "Automated Data Gathering and Training Tool for Personalized \"Itchy Nose\"",
    "authors": [
      "Juyoung Lee",
      "Hui-Shyong Yeo",
      "Thad Starner",
      "Aaron Quigley",
      "Kai Kunze",
      "Woontack Woo"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "In \"Itchy Nose\" we proposed a sensing technique for detecting finger movements on the nose for supporting subtle and discreet interaction. It uses the electrooculography sensors embedded in the frame of a pair of eyeglasses for data gathering and uses machine-learning technique to classify different gestures. Here we further propose an automated training and visualization tool for its classifier. This tool guides the user to make the gesture in proper timing and records the sensor data. It automatically picks the ground truth and trains a machine-learning classifier with it. With this tool, we can quickly create trained classifier that is personalized for the user and test various gestures.",
    "keywords": [
      "Training tool",
      "online classification",
      "smart eyeglasses",
      "smart eyewear",
      "subtle interaction",
      "EOG",
      "Nose gesture",
      "wearable computer"
    ],
    "doi": "10.1145/3174910.3174953",
    "url": "https://doi.org/10.1145/3174910.3174953",
    "citations": 3,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "43",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3174910.3174956",
    "title": "ToolShaker: Presentation Technique for \"as-is\" Display of Daily Commodities",
    "authors": [
      "Hayato Dogai",
      "Maho Oki",
      "Koji Tsukada"
    ],
    "year": 2018,
    "conference": "AHS",
    "conferenceYear": "AH '18",
    "abstract": "Recently, various research projects have proposed presentation techniques for displaying products in our day-to-day environments. These techniques involve embedding sensors and actuators in the commonly used commodities. However, such methods have difficulty in maintain the usability of these everyday objects because of their effect on the product size and weight. In this study, we propose a presentation technique, namely, the TookShaker, which can physically control the daily used items \"as-is\" without attaching specific devices. We develop a prototype to control ferromagnetic objects (e.g., tools) placed on a wall surface by using electromagnets.",
    "keywords": [
      "daily commodities",
      "Display technique",
      "and ambient display",
      "electromagnet"
    ],
    "doi": "10.1145/3174910.3174956",
    "url": "https://doi.org/10.1145/3174910.3174956",
    "citations": 0,
    "booktitle": "Proceedings of the 9th Augmented Human International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Seoul, Republic of Korea",
    "articleno": "44",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311831",
    "title": "TongueBoard: An Oral Interface for Subtle Input",
    "authors": [
      "Richard Li",
      "Jason Wu",
      "Thad Starner"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "We present TongueBoard, a retainer form-factor device for recognizing non-vocalized speech. TongueBoard enables absolute position tracking of the tongue by placing capacitive touch sensors on the roof of the mouth. We collect a dataset of 21 common words from four user study participants (two native American English speakers and two non-native speakers with severe hearing loss). We train a classifier that is able to recognize the words with 91.01% accuracy for the native speakers and 77.76% accuracy for the non-native speakers in a user dependent, offline setting. The native English speakers then participate in a user study involving operating a calculator application with 15 non-vocalized words and two tongue gestures at a desktop and with a mobile phone while walking. TongueBoard consistently maintains an information transfer rate of 3.78 bits per decision (number of choices = 17, accuracy = 97.1%) and 2.18 bits per second across stationary and mobile contexts, which is comparable to our control conditions of mouse (desktop) and touchpad (mobile) input.",
    "keywords": [
      "wearable devices",
      "input interaction",
      "silent speech interface",
      "oral sensing",
      "subtle gestures"
    ],
    "doi": "10.1145/3311823.3311831",
    "url": "https://doi.org/10.1145/3311823.3311831",
    "citations": 75,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "1",
    "numpages": "9",
    "influentialCitations": 5
  },
  {
    "id": "10.1145/3311823.3311824",
    "title": "Estimation of Fingertip Contact Force by Measuring Skin Deformation and Posture with Photo-Reflective Sensors",
    "authors": [
      "Ayane Saito",
      "Wakaba Kuno",
      "Wataru Kawai",
      "Natsuki Miyata",
      "Yuta Sugiura"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "A wearable device for measuring skin deformation of the fingertip---to obtain contact force when the finger touches an object---was prototyped and experimentally evaluated. The device is attached to the fingertip and uses multiple photo-reflective sensors (PRSs) to measures the distance from the PRSs to the side surface of the fingertip. The sensors do not touch the contact surface between the fingertip and the object; as a result, the contact force is obtained without changing the user's tactile sensation. In addition, the accuracy of estimated contact force was improved by determining the posture of the fingertip by measuring the distance between the fingertip and the contact surface. Based on the prototyped device, a system for estimating three-dimensional contact force on the fingertip was implemented.",
    "keywords": [
      "photo-reflective sensor",
      "wearable device",
      "force sensing"
    ],
    "doi": "10.1145/3311823.3311824",
    "url": "https://doi.org/10.1145/3311823.3311824",
    "citations": 14,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "2",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311832",
    "title": "2bit-TactileHand: Evaluating Tactons for On-Body Vibrotactile Displays on the Hand and Wrist",
    "authors": [
      "Don Samitha Elvitigala",
      "Denys J. C. Matthies",
      "Vipula Dissanayaka",
      "Chamod Weerasinghe",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Visual interfaces can provide a great density of information. However, the required focused visual attention results in a high cognitive effort. This cognitive load significantly increases when multiple tasks are performed that also require visual attention. In this paper, we evaluate the perceptual abilities of 2bit tactons on the wrist and the hand as a type of complementary feedback. Based on our evaluation, 2bit tactons are reasonably high perceivable (≈ 92%) at the hand distributed among several fingers. Additionally, the data concluded that vibrotactile feedback on hand is significantly more accurate than the wrist, which coincides with the subjects' preference. TactileHand's feasibility was demonstrated in three pilot studies, encoding ambient, explicit and implicit information into 2bit tactons in different scenarios.",
    "keywords": [
      "Low-density Information",
      "Information Presentation Interface",
      "Vibrotactile Display",
      "Tactons",
      "On-body Feedback"
    ],
    "doi": "10.1145/3311823.3311832",
    "url": "https://doi.org/10.1145/3311823.3311832",
    "citations": 16,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "3",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311834",
    "title": "StockSense: A Wrist-Worn Vibrotactile Display for Tracking Volatile Markets",
    "authors": [
      "Erik Pescara",
      "Ilya Fillipov",
      "Michael Beigl"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents StockSense, a prototype of a haptic wristband for tracking volatile markets. StockSense informs the user about current price changes via structured vibrotactile feedback. This system can be used for any volatile market: cryptocurrency market, stock market, etc. A preliminary study was carried out with 5 participants to investigate whether vibrotactile displays of smartphones could be integrated into the system for additional information transmission. Afterwards the system was evaluated in a study with 20 participants. The participants became familiar with the system within a short period of time and were able to recognize the displayed information on the vibrotactile display reliably. The participants rated the system as positive. Overall, the participants appreciated the functionality of the system and considered it to be suitable for this field of application.",
    "keywords": [
      "Volatile Markets",
      "Tactile Interfaces",
      "Haptics"
    ],
    "doi": "10.1145/3311823.3311834",
    "url": "https://doi.org/10.1145/3311823.3311834",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "4",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311837",
    "title": "Evaluation of a Device Reproducing the Pseudo-Force Sensation Caused by a Clothespin",
    "authors": [
      "Masahiro Miyakami",
      "Takuto Nakamura",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "A pseudo-force sensation can be elicited by pinching a finger with a clothespin. When the clothespin is used to pinch the finger from the palm side, a pseudo-force is felt in the direction towards the palm side, and when it is used to pinch the finger from the back side of the hand, the pseudo-force is felt in the extension direction. Here, as a first step to utilizing this phenomenon in human-machine interfaces, we developed a device that reproduces the clothespin phenomenon and confirmed the occurrence rate of the pseudo-force sensation.",
    "keywords": [
      "Clothespin",
      "Virtual reality",
      "Human interface",
      "Pseudo force"
    ],
    "doi": "10.1145/3311823.3311837",
    "url": "https://doi.org/10.1145/3311823.3311837",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "5",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311838",
    "title": "Grip Force Modulation by Finger Posture",
    "authors": [
      "Hideyuki Asazu",
      "Masahiro Miyakami",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Modulation of grip force has several applications such as sports. We speculated that as mupltiple fingers mechanically interact with each other, it might be possible to adjust grip strength by controlling the movement of single finger. We also speculated that changing grip strength may affect the weight perception of an object on grasping. In this paper, we investigated whether grip strength could be modulated by a specific finger posture, and found that it can be reduced by stretching some fingers, specifically middle finger and ring finger. Preliminary result of weight perception modulation is also reported.",
    "keywords": [
      "Grip force",
      "Finger posture",
      "Weight perception",
      "Virtual reality"
    ],
    "doi": "10.1145/3311823.3311838",
    "url": "https://doi.org/10.1145/3311823.3311838",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "6",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311835",
    "title": "Guided Walking to Direct Pedestrians toward the Same Destination",
    "authors": [
      "Nobuhito Sakamoto",
      "Masahiro Furukawa",
      "Masataka Kurokawa",
      "Taro Maeda"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we propose a floor covering-type walking guidance sheet to direct pedestrians without requiring attachment/detachment. Polarity is reversed with respect to the direction of walking in the guidance sheet such that a pedestrian travelling in any direction can be guided toward a given point. In experiments, our system successfully guided a pedestrian along the same direction regardless of the direction of travel using the walking guidance sheet. The induction effect of the proposed method was also evaluated.",
    "keywords": [
      "walking guidance",
      "pedestrian",
      "visual stimulus",
      "vection"
    ],
    "doi": "10.1145/3311823.3311835",
    "url": "https://doi.org/10.1145/3311823.3311835",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "7",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311857",
    "title": "Detection Threshold of the Height Difference between a Visual and Physical Step",
    "authors": [
      "Masato Kobayashi",
      "Yuki Kon",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In recent years, virtual reality (VR) applications that accompany real-space walking have become popular. In these applications, the expression of steps, such as a stairway, is a technical challenge. Preparing a real step with the same scale as that of the step in the VR space is one alternative; however, it is costly and impractical. We propose using a real step, but one physical step for the expression of various steps, by manipulating the viewpoint and foot position when ascending and descending real steps. The hypothesis is that the height of a step can be complemented to some extent visually, even if the heights of the real step and that in the VR space are different. In this paper, we first propose a viewpoint and foot position manipulation algorithm. T hen we measure the detection threshold of the height difference between the visual and physical step when ascending and descending the physical step using our manipulation algorithm. As a result, we found that the difference can be detected if there is a difference of approximately 1.0 cm between the VR space and the real space, irrespective of the height of the physical step.",
    "keywords": [
      "Virtual reality",
      "Height difference",
      "Viewpoint manipulation",
      "Redirected walking"
    ],
    "doi": "10.1145/3311823.3311857",
    "url": "https://doi.org/10.1145/3311823.3311857",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "8",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311836",
    "title": "Enhancement of Subjective Mechanical Tactile Intensity via Electrical Stimulation",
    "authors": [
      "Ryo Mizuhara",
      "Akifumi Takahashi",
      "Hiroyuki Kajimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Naturalistic tactile sensations can be elicited by mechanical stimuli because mechanical stimulation reproduces a natural physical phenomenon. However, a mechanical stimulation that is too strong may cause injury. Although electrical stimulation can elicit strong tactile sensations without damaging the skin, electrical stimulation is inferior in terms of naturalness. Here, we propose and validate a haptic method for presenting naturalistic and intense sensations by combining electrical and mechanical stimulation. Prior to the main experiment, we measured the appropriate temporal gap between the two stimuli such that they are perceived as simultaneous, since nerve activity directly elicited by electrical stimulation is generally considered to be perceived faster than mechanical stimulation. We confirmed that enhancement of subjective strength took place when two stimuli were given simultaneously. The main experiment with simultaneous electrical and mechanical stimulation confirmed that addition of electrical stimulation enhances the sensation of mechanical stimulation, and participants' comments implied that electrical stimulation was interpreted as part of the mechanical stimulation.",
    "keywords": [
      "Electrical stimulation",
      "Virtual Reality",
      "Mechanical stimulation",
      "Tactile"
    ],
    "doi": "10.1145/3311823.3311836",
    "url": "https://doi.org/10.1145/3311823.3311836",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "9",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311842",
    "title": "Glove-Through Tactile Information Transmission System",
    "authors": [
      "Hideki Kawai",
      "Hidenori Itoh",
      "Takuya Nakano",
      "Hiroyuki Kajimoto",
      "Yasuyuki Yanagida"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "By covering the skin with clothes such as gloves, humans have managed to survive in severe environments and undertake dangerous work. However, when we wear covering materials, tactile information is lost; this may decrease working efficiency and degrade the performance of object perception at the moment of contact. In order to address this issue, the concept of a \"haptic-through\" system that transmits tactile information through a covering material has been proposed. However, the effectiveness of this concept has been verified only by vertically pushing a surface of an object with one finger; there have been no systems proposed that can be used in actual complex finger motion. We constructed a glove-style haptic-through system that can be used in practical finger motion. By using this system, we conducted an experiment to investigate the discrimination threshold of the angle of a rod-shaped object. As a result, the angle discrimination threshold through a glove, with the support of our system, was equivalent to that obtained with bare hands.",
    "keywords": [
      "Tactile Display",
      "Glove-Through",
      "Tactile Sensor",
      "Haptic-Through"
    ],
    "doi": "10.1145/3311823.3311842",
    "url": "https://doi.org/10.1145/3311823.3311842",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "10",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311839",
    "title": "FSense: Unlocking the Dimension of Force for Gestural Interactions Using Smartwatch PPG Sensor",
    "authors": [
      "Thisum Buddhika",
      "Haimo Zhang",
      "Samantha W. T. Chan",
      "Vipula Dissanayake",
      "Suranga Nanayakkara",
      "Roger Zimmermann"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "While most existing gestural interfaces focus on the static posture or the dynamic action of the hand, few have investigated the feasibility of using the forces that are exerted while performing gestures. Using the photoplethysmogram (PPG) sensor of off-the-shelf smartwatches, we show that, it is possible to recognize the force of a gesture as an independent channel of input. Based on a user study with 12 participants, we found that users were able to reliably produce two levels of force across several types of common gestures. We demonstrate a few interaction scenarios where the force is either used as a standalone input or to complement existing input modalities.",
    "keywords": [
      "Mobile Sensors",
      "Smartwatch",
      "Gesture Interaction",
      "PPG Sensor",
      "Wearable Computing"
    ],
    "doi": "10.1145/3311823.3311839",
    "url": "https://doi.org/10.1145/3311823.3311839",
    "citations": 25,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "11",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311840",
    "title": "Haptic Collar: Vibrotactile Feedback around the Neck for Guidance Applications",
    "authors": [
      "Stefanie Schaack",
      "George Chernyshov",
      "Kirill Ragozin",
      "Benjamin Tag",
      "Roshan Peiris",
      "Kai Kunze"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we present a Haptic Collar prototype, a neck worn band with vibrotactile actuators for eyes-free haptic navigation. We evaluate the system for guidance applications on over 11 users, analyzing 4 different tactile patterns regarding comfort and ease of understanding as well as the number of actuators to encode 8 directions (4, 6 and 8). Overall, users can recognize the directional signs well (up to 95 % recognition rates for over 528 triggers). We also present a use case applying our prototype for a haptic navigation walk.",
    "keywords": [
      "Haptic Feedback",
      "Haptics",
      "Collar",
      "Vibrotactile Actuators",
      "Navigation",
      "Neck",
      "Sensitivity"
    ],
    "doi": "10.1145/3311823.3311840",
    "url": "https://doi.org/10.1145/3311823.3311840",
    "citations": 28,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "12",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311841",
    "title": "OSense: Object-Activity Identification Based on Gasping Posture and Motion",
    "authors": [
      "Thisum Buddhika",
      "Haimo Zhang",
      "Chamod Weerasinghe",
      "Suranga Nanayakkara",
      "Roger Zimmermann"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Observing that, how we grasp objects is highly correlated with geometric shapes and interactions, we propose the use of hand postures and motions as an indirect source of inputs for object-activity recognition. This paradigm treats the human hand as an always-available sensor, and transforms all sensing problems to the data analysis for the \"sensor hand\". We envision this paradigm to be generalizable for all objects regardless of whether they are acoustically or electromagnetically active, and that it detects different motions while holding the same object. Our proof-of-concept setup consists of six IMU sensors mounted on the fingers and back of the hand. Our experiments show that when the posture is combined with the motion, the personalized object-activity detection accuracy increases from 80% to 87%.",
    "keywords": [
      "Wearable Computing",
      "Object Identification",
      "Activity Recognition"
    ],
    "doi": "10.1145/3311823.3311841",
    "url": "https://doi.org/10.1145/3311823.3311841",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "13",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311826",
    "title": "TherModule: Wearable and Modular Thermal Feedback System Based on a Wireless Platform",
    "authors": [
      "Tomosuke Maeda",
      "Tetsuo Kurahashi"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Humans have specific sensory organs and they can feel tactile sensation on the whole body. However, many haptic devices have limitations due to the location of the body part and might not provide natural haptic feedback. Thus, we propose a novel interface, TherModule, which is a wearable and modular thermal feedback system for embodied interactions based on a wireless platform. TherModule can be worn on multiple body parts such as the wrist, forearm, ankle, and neck. In this paper, we describe the system concept, module implementation, and applications. To demonstrate and explore the embodied interaction with thermal feedback, we implemented prototype applications, such as movie experiences, projector-based augmented reality, navigation, and notification based on a wireless platform, with TherModule on multiple parts of the body. The result of an experiment on movie experience showed that participants felt more interactions between temperature and visual stimulus.",
    "keywords": [
      "Embodied interaction",
      "Haptics",
      "Wearable"
    ],
    "doi": "10.1145/3311823.3311826",
    "url": "https://doi.org/10.1145/3311823.3311826",
    "citations": 35,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "14",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311843",
    "title": "Augmented Recreational Volleyball Court: Supporting the Beginners' Landing Position Prediction Skill by Providing Peripheral Visual Feedback",
    "authors": [
      "Koya Sato",
      "Yuji Sano",
      "Mai Otsuki",
      "Mizuki Oka",
      "Kazuhiko Kato"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Volleyball is widely popular as a way to share a sense of unity and achievement with others. However, errors detract beginners from enjoying the game. To overcome this issue, we developed a system that supports the beginners' skill to predict the ball landing position by indicating the predicted ball landing position on the floor as a visual feedback. In volleyball, it is necessary to pay attention to the ball that has been launched in air, and visual feedback on the floor surface must be perceived through peripheral vision. The effect of such visual feedback in supporting beginners' prediction skill was not clear. Therefore, we evaluated the effectiveness of the proposed system via a simulated serve-reception experiment. As a result, we confirmed that the proposed system improved the prediction skill in terms of the prediction speed and accuracy in the left-right direction, and that beginners felt an improvement in the prediction accuracy and ease of ball manipulation, thereby increasing the enjoyment. These results also indicate that it is possible to utilize peripheral vision supports in other disciplines in which there is a distance between the object of attention and the sports field on which visual feedback can be presented.",
    "keywords": [
      "peripheral vision",
      "floor projection",
      "Augmented reality",
      "augmented sports",
      "recreational volleyball",
      "visual feedback"
    ],
    "doi": "10.1145/3311823.3311843",
    "url": "https://doi.org/10.1145/3311823.3311843",
    "citations": 17,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "15",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311844",
    "title": "Prediction of Volleyball Trajectory Using Skeletal Motions of Setter Player",
    "authors": [
      "Shuya Suda",
      "Yasutoshi Makino",
      "Hiroyuki Shinoda"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we present a method that predicts the ball trajectory of a volleyball toss 0.3 s before the actual toss by observing the motion of the setter player. We input 3D data of body joints obtained using Kinect into a simple neural network, and 2D data estimated using OpenPose is used for comparison. We created simple neural networks for the two players and tested them. The trajectory of a volleyball toss is properly predicted by the proposed method and the error of the toss trajectory was approximately equal to the size of the ball. This technology can provide a new spectating experience in sports by superimposing the predicted images onto a live broadcast. We also show that this method can be used to identify the important body parts that contribute to the toss prediction. A professional volleyball analyst stated that this technology can be used for analyzing the peculiarities of opponent players.",
    "keywords": [
      "Volleyball",
      "Ball trajectory prediction",
      "Machine learning"
    ],
    "doi": "10.1145/3311823.3311844",
    "url": "https://doi.org/10.1145/3311823.3311844",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "16",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311845",
    "title": "Identifying Muscle Fatigue and Hyperthermia in Sports Activities Using Thermal Imaging and Facial Recognition Software",
    "authors": [
      "Christopher G. Harris"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Hyperthermia and muscle fatigue during sports activities are a challenge to detect because body temperatures cannot be unobtrusively evaluated from the field of play. Recently, inexpensive portable thermal imaging devices have become available, allowing accurate monitoring of players from a distance. In this paper, we examine the accuracy of thermal imaging devices by distance and monitoring angle. Using thermal imaging, object recognition, and facial recognition techniques, we develop a visualization tool to display core body temperature information for each player. This augmented display allows us to detect potential heat-related player injuries on a sports field in near real time.",
    "keywords": [
      "Thermal imaging",
      "sports medicine",
      "augmented health",
      "hyperthermia",
      "visualization",
      "muscle fatigue"
    ],
    "doi": "10.1145/3311823.3311845",
    "url": "https://doi.org/10.1145/3311823.3311845",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311861",
    "title": "Virtual Super-Leaping: Immersive Extreme Jumping in VR",
    "authors": [
      "Tomoya Sasaki",
      "Kao-Hua Liu",
      "Taiki Hasegawa",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "People sometimes imagine and yearn for a \"Super Power,\" an ability they do not have naturally. In this paper, we propose Virtual Super-Leaping (VSL) as an immersive virtual experience that provides the feeling of extreme jumping in the sky. First, we define the necessary feedback elements and classify the action sequence of Super-Leaping, including the design of the multimodal feedback for each action state. Then, we describe the design of the VSL system, which has two components: (i) visual a head-mounted display-based feedback, and (ii) a VSL-enabling haptic device, which provides both kinesthesia and airflow using multiple synchronized propeller units. We end by reporting on our technical evaluation and public demonstrations. This work contributes to the enhancement of immersive virtual experiences and development of devices for human augmentation.",
    "keywords": [
      "Kinesthesia",
      "Illusion of Self-motion",
      "Haptic Device",
      "Airflow"
    ],
    "doi": "10.1145/3311823.3311861",
    "url": "https://doi.org/10.1145/3311823.3311861",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "18",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311846",
    "title": "Social Activity Measurement by Counting Faces Captured in First-Person View Lifelogging Video",
    "authors": [
      "Akane Okuno",
      "Yasuyuki Sumi"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "This paper proposes a method to measure the daily face-to-face social activity of a camera wearer by detecting faces captured in first-person view lifelogging videos. This study was inspired by pedometers used to estimate the amount of physical activity by counting the number of steps detected by accelerometers, which is effective for reflecting individual health and facilitating behavior change. We investigated whether we can estimate the amount of social activity by counting the number of faces captured in the first-person view videos like a pedometer. Our system counts not only the number of faces but also weighs in the numbers according to the size of the face (corresponding to a face's closeness) and the amount of time it was shown in the video. By doing so, we confirmed that we can measure the amount of social activity based on the quality of each interaction. For example, if we simply count the number of faces, we overestimate social activities while passing through a crowd of people. Our system, on the other hand, gives a higher score to a social actitivity even when speaking with a single person for a long time, which was also positively evaluated by experiment participants who viewed the lifelogging videos. Through evaluation experiments, many evaluators evaluated the social activity high when the camera wearer speaks. An interesting feature of the proposed system is that it can correctly evaluate such scenes higher as the camera wearer actively engages in conversations with others, even though the system does not measure the camera wearer's utterances. This is because the conversation partners tend to turn their faces towards to the camera wearer, and that increases the number of detected faces as a result. However, the present system fails to correctly estimate the depth of social activity compared to what the camera wearer recalls especially when the conversation partners are standing out of the camera's field of view. The paper briefly descibes how the results can be improved by widening the camera's field of view.",
    "keywords": [
      "first-person view video",
      "social health",
      "quantified self",
      "lifelogging",
      "Social activity measurement",
      "face detection"
    ],
    "doi": "10.1145/3311823.3311846",
    "url": "https://doi.org/10.1145/3311823.3311846",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "19",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311871",
    "title": "Augmented Taste of Wine by Artificial Climate Room: Influence of Temperature and Humidity on Taste Evaluation",
    "authors": [
      "Toshiharu Igarashi",
      "Tatsuya Minagawa",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In previous research, there is a augmenting device limited taste influences due to limited contact with utensils. However, in the situation such as enjoying wine while talking with other people and matching cheese with wine, the solution that limits human behaviors must not have been acceptable. So, we focused on changing the temperature and humidity when drinking wine.To study the influence of temperature and humidity on the ingredients and subjective taste of wine, we conducted wine tasting experiments with 16 subjects using an artificial climate room. For the environmental settings, three conditions, i.e., a room temperature of 14°C and humidity of 35%, 17°C and 40% humidity, and 26°C and 40% humidity, were evaluated. In one of the two wines used in the experiment, significant differences in [Color intensity], [Smell development] and [Body] were detected among conditions (p &lt; 0.05). We further investigated changes in the components of the two wines at different temperature conditions (14°C, 17°C, 23°C, and 26°C). Malic acid, protocatechuic acid, gallic acid, and epicatechin were related to temperature in the former wine only.In conclusion, we confirmed that we can change the taste evaluation of wine by adjusting temperature and humidity using the artificial climate room, without attaching the device to human beings themselves. This suggests the possibility to serve wine in a more optimal environment if we can identify the type of wine and person's preference.",
    "keywords": [
      "Artificial climate room",
      "Augmented taste",
      "Human-centered computing",
      "Empirical studies in HCI",
      "Taste of wine"
    ],
    "doi": "10.1145/3311823.3311871",
    "url": "https://doi.org/10.1145/3311823.3311871",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "20",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311863",
    "title": "GANs-Based Clothes Design: Pattern Maker Is All You Need to Design Clothing",
    "authors": [
      "Natsumi Kato",
      "Hiroyuki Osone",
      "Kotaro Oomori",
      "Chun Wei Ooi",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Machine learning have been recently applied to multiple areas, including fashion. Fashion design by generated images makes it possible to inherit design without fashion designer and get inspiration, however, little research has been done on usage of machine learning for creation of designer clothing. The state-of-the-art works aim for high-definition output images. However in fashion design image generation, it has not been thoroughly investigated to what extent the quality of the generated image should be provided to the pattern makers that draw the costume pattern from the design images. Therefore, in this paper we propose a method of generation of clothing images for pattern makers using Progressive Growing of GANs (P-GANs) and conduct a user study to investigate whether the different image quality factors such as epoch and resolution affect the participants' confidence score. We discuss the results and possible applications of the developed method.",
    "keywords": [
      "Generative Adversarial Networks",
      "pattern",
      "fashion",
      "deep neural network",
      "pattern maker",
      "clothes"
    ],
    "doi": "10.1145/3311823.3311863",
    "url": "https://doi.org/10.1145/3311823.3311863",
    "citations": 35,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "21",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311864",
    "title": "Brain Computer Interface for Neuro-Rehabilitation With Deep Learning Classification and Virtual Reality Feedback",
    "authors": [
      "Tamás Karácsony",
      "John Paulin Hansen",
      "Helle Klingenberg Iversen",
      "Sadasivan Puthusserypady"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Though Motor Imagery (MI) stroke rehabilitation effectively promotes neural reorganization, current therapeutic methods are immeasurable and their repetitiveness can be demotivating. In this work, a real-time electroencephalogram (EEG) based MI-BCI (Brain Computer Interface) system with a virtual reality (VR) game as a motivational feedback has been developed for stroke rehabilitation. If the subject successfully hits one of the targets, it explodes and thus providing feedback on a successfully imagined and virtually executed movement of hands or feet. Novel classification algorithms with deep learning (DL) and convolutional neural network (CNN) architecture with a unique trial onset detection technique was used. Our classifiers performed better than the previous architectures on datasets from PhysioNet offline database. It provided fine classification in the real-time game setting using a 0.5 second 16 channel input for the CNN architectures. Ten participants reported the training to be interesting, fun and immersive. \"It is a bit weird, because it feels like it would be my hands\", was one of the comments from a test person. The VR system induced a slight discomfort and a moderate effort for MI activations was reported. We conclude that MI-BCI-VR systems with classifiers based on DL for real-time game applications should be considered for motivating MI stroke rehabilitation.",
    "keywords": [
      "CNN",
      "Online EEG classification",
      "Virtual Reality",
      "Brain Computer Interface",
      "Deep learning",
      "Motor Imagery"
    ],
    "doi": "10.1145/3311823.3311864",
    "url": "https://doi.org/10.1145/3311823.3311864",
    "citations": 62,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "22",
    "numpages": "8",
    "influentialCitations": 8
  },
  {
    "id": "10.1145/3311823.3311865",
    "title": "SubMe: An Interactive Subtitle System with English Skill Estimation Using Eye Tracking",
    "authors": [
      "Katsuya Fujii",
      "Jun Rekimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Owing to the improvement in accuracy of eye tracking devices, eye gaze movements occurring while conducting tasks are now a part of physical activities that can be monitored just like other life-logging data. Analyzing eye gaze movement data to predict reading comprehension has been widely explored and researchers have proven the potential of utilizing computers to estimate the skills and expertise level of users in various categories, including language skills. However, though many researchers have worked specifically on written texts to improve the reading skills of users, little research has been conducted to analyze eye gaze movements in correlation to watching movies, a medium which is known to be a popular and successful method of studying English as it includes reading, listening, and even speaking, the later of which is attributed to language shadowing. In this research, we focus on movies with subtitles due to the fact that they are very useful in order to grasp what is occurring on screen, and therefore, overall understanding of the content. We realized that the viewers' eye gaze movements are distinct depending on their English level. After retrieving the viewers' eye gaze movement data, we implemented a machine learning algorithm to detect their English levels and created a smart subtitle system called SubMe. The goal of this research is to estimate English levels through tracking eye movement. This was conducted by allowing the users to view a movie with subtitles. Our aim is create a system that can give the user certain feedback that can help improve their English studying methods.",
    "keywords": [
      "Human Computer Interaction",
      "User Interface",
      "Learning"
    ],
    "doi": "10.1145/3311823.3311865",
    "url": "https://doi.org/10.1145/3311823.3311865",
    "citations": 15,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "23",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311868",
    "title": "Sentiment Pen: Recognizing Emotional Context Based on Handwriting Features",
    "authors": [
      "Jiawen Han",
      "George Chernyshov",
      "Dingding Zheng",
      "Peizhong Gao",
      "Takuji Narumi",
      "Katrin Wolf",
      "Kai Kunze"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we discuss the assessment of the emotional state of the user from digitized handwriting for implicit human-computer interaction. The proposed concept exemplifies how a digital system could recognize the emotional context of the interaction. We discuss our approach to emotion recognition and the underlying neurophysiological mechanisms. To verify the viability of our approach, we have conducted a series of tests where participants were asked to perform simple writing tasks after being exposed to a series of emotionally-stimulating video clips from EMDB[6], one set of four clips per each quadrant on the circumplex model of emotion[28]. The user-independent Support Vector Classifier (SVC) built using the recorded data shows up to 66% accuracy for certain types of writing tasks for 1 in 4 classification (1. High Valence, High Arousal; 2. High Valence, Low Arousal; 3. Low Valence, High Arousal; 4. Low Valence, Low Arousal). In the same conditions, a user-dependent classifier reaches an average of 70% accuracy across all 12 study participants. While future work is required to improve the classification rate, this work should be seen as proof-of-concept for emotion assessment of users while handwriting aiming to motivate research on implicit interaction while writing to enable emotion-sensitivity in mobile and ubiquitous computing.",
    "keywords": [
      "Emotional Recognition",
      "Affective Computing",
      "Handwriting Analysis"
    ],
    "doi": "10.1145/3311823.3311868",
    "url": "https://doi.org/10.1145/3311823.3311868",
    "citations": 10,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "24",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311869",
    "title": "Automatic Smile and Frown Recognition with Kinetic Earables",
    "authors": [
      "Seungchul Lee",
      "Chulhong Min",
      "Alessandro Montanari",
      "Akhil Mathur",
      "Youngjae Chang",
      "Junehwa Song",
      "Fahim Kawsar"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we introduce inertial signals obtained from an earable placed in the ear canal as a new compelling sensing modality for recognising two key facial expressions: smile and frown. Borrowing principles from Facial Action Coding Systems, we first demonstrate that an inertial measurement unit of an earable can capture facial muscle deformation activated by a set of temporal micro-expressions. Building on these observations, we then present three different learning schemes - shallow models with statistical features, hidden Markov model, and deep neural networks to automatically recognise smile and frown expressions from inertial signals. The experimental results show that in controlled non-conversational settings, we can identify smile and frown with high accuracy (F1 score: 0.85).",
    "keywords": [
      "earable",
      "smile and frown recognition",
      "kinetic modeling",
      "FACS"
    ],
    "doi": "10.1145/3311823.3311869",
    "url": "https://doi.org/10.1145/3311823.3311869",
    "citations": 27,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "25",
    "numpages": "4",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311870",
    "title": "Prospero: A Personal Wearable Memory Coach",
    "authors": [
      "Samantha W. T. Chan",
      "Haimo Zhang",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Prospective memory, which involves remembering to perform intended actions, is essential for independent daily living especially as we grow older. Yet, majority of everyday memory failures are due to prospective memory lapses. Memory strategy training can help to tackle such lapses. We present Prospero, a wearable virtual memory coach that guides users to learn and apply a memory technique through conversation in natural language. Using physiological signals, Prospero proactively initiates practice of the technique during opportune times where user attention and cognitive load have more bandwidth. This could be a step towards creating more natural and effective digital memory training that could eventually reduce memory decline. In this paper, we contribute with details of its implementation and conversation design.",
    "keywords": [
      "Memory Training",
      "Memory Coach",
      "Conversational Agent"
    ],
    "doi": "10.1145/3311823.3311870",
    "url": "https://doi.org/10.1145/3311823.3311870",
    "citations": 8,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "26",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311875",
    "title": "An Implicit Dialogue Injection System for Interruption Management",
    "authors": [
      "Tomoki Shibata",
      "Alena Borisenko",
      "Anzu Hakone",
      "Tal August",
      "Leonidas Deligiannidis",
      "Chen-Hsiang Yu",
      "Matthew Russell",
      "Alex Olwal",
      "Robert J. K. Jacob"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents our efforts in redesigning the conventional on/off interruption management tactic (a.k.a. \"Do Not Disturb Mode\") for situations where interruptions are inevitable. We introduce an implicit dialogue injection system, in which the computer implicitly observes the user's state of busyness from passive measurement of the prefrontal cortex to determine how to interrupt the user. We use functional Near-Infrared Spectroscopy (fNIRS), a noninvasive brain-sensing technique. In this paper, we describe our system architecture and report results of our proof-of-concept study, in which we compared two contrasting interruption strategies; the computer either forcibly interrupts the user with a secondary task or requests the user's participation before presenting it. The latter yielded improved user experience (e.g. lower reported annoyance), in addition to showing a potential improvement in task performance (i.e. retaining context information) when the user was busier. We conclude that tailoring the presentation of interruptions based on real-time user state provides a step toward making computers more considerate of their users.",
    "keywords": [
      "implicit dialogue injection",
      "Implicit User Interfaces",
      "Interruption",
      "functional Near-Infrared Spectroscopy (fNIRS)",
      "implicit interactions",
      "HumanSketch"
    ],
    "doi": "10.1145/3311823.3311875",
    "url": "https://doi.org/10.1145/3311823.3311875",
    "citations": 5,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "27",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311872",
    "title": "Hearing Is Believing: Synthesizing Spatial Audio from Everyday Objects to Users",
    "authors": [
      "Jing Yang",
      "Yves Frank",
      "Gábor Sörös"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "The ubiquity of wearable audio devices and the importance of the auditory sense imply great potential for audio augmented reality. In this work, we propose a concept and a prototype of synthesizing spatial sounds from arbitrary real objects to users in everyday interactions, whereby all sounds are rendered directly by the user's own ear pods instead of loudspeakers on the objects. The proposed system tracks the user and the objects in real time, creates a simplified model of the environment, and generates realistic 3D audio effects. We thoroughly evaluate the usability and the usefulness of such a system based on a user study with 21 participants. We also investigate how an acoustic environment model improves the sense of engagement of the rendered 3D sounds.",
    "keywords": [
      "Spatial audio",
      "human-object interactions",
      "augmented reality"
    ],
    "doi": "10.1145/3311823.3311872",
    "url": "https://doi.org/10.1145/3311823.3311872",
    "citations": 16,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "28",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311873",
    "title": "MusiArm: Extending Prosthesis to Musical Expression",
    "authors": [
      "Kaito Hatakeyama",
      "MHD Yamen Saraiji",
      "Kouta Minamizawa"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "The emergence of prosthetic limbs where solely focused on substituting the missing limb with an artificial one, in order for the handicap people to manage their daily life independently. Past research on prosthetic hands has mainly focused on prosthesis' function and performance. Few proposals focused on the entertainment aspect of prosthetic hands. In this research, we considered the defective part as a potential margin for freely designing our bodies, and coming up with new use cases beyond the original function of the limb. Thus, we are not aiming to create anthropomorphic designs or functions of the limbs. By fusing the prosthetic hands and musical instruments, we propose a new prosthetic hand called \"MusiArm\" that extends the body part's function to become an instrument. MusiArm concept was developed through the dialogue between the handicapped people, engineers and prosthetists using the physical characteristics of the handicapped people as a \"new value\" that only the handicapped person can possess. We asked handicapped people who cannot play musical instruments, as well as people who do not usually play instruments, to use prototypes we made. As a result of the usability tests, using MusiArm, we made a part of the body function as a musical instrument, drawing out the unique expression methods of individuals, and enjoying the performance and clarify the possibility of showing interests.",
    "keywords": [
      "Prosthesis",
      "Musical Instrument",
      "Congenital Defect",
      "Co-creation",
      "Human Augmentation",
      "Prosthetic Arm"
    ],
    "doi": "10.1145/3311823.3311873",
    "url": "https://doi.org/10.1145/3311823.3311873",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "29",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311854",
    "title": "Automatic Eyeglasses Replacement for a 3D Virtual Try-on System",
    "authors": [
      "Takumi Kobayashi",
      "Yuta Sugiura",
      "Hideo Saito",
      "Yuji Uema"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "This paper presents a 3D virtual eyeglasses try-on system for practical use. For fitting eyeglasses in a shop, consumers wish to look at themselves in a mirror while trying on various eyeglass styles. However, for people who need to wear eyeglasses for correcting problems with eyesight, it is impossible for them to clearly observe their face in the mirror without wearing eyeglasses. This makes fitting them for new eyeglasses difficult. This research proposes a virtual try-on system that can be used while wearing eyeglasses. We replace the user's eyeglasses in the input video with new eyeglasses virtually. Moreover, a fast and accurate face tracking tool enables our system to automatically display 3D virtual glasses following a user's head motion. Experimental results demonstrate that the proposed method can render virtual glasses naturally while the user is wearing real eyeglasses.",
    "keywords": [
      "augmented reality",
      "virtual try-on",
      "eyeglasses removal",
      "mixed reality"
    ],
    "doi": "10.1145/3311823.3311854",
    "url": "https://doi.org/10.1145/3311823.3311854",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "30",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311858",
    "title": "Effects of a Monocular Laser-Based Head-Mounted Display on Human Night Vision",
    "authors": [
      "Evangelos Niforatos",
      "Mélodie Vidal"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Head-mounted displays (HMDs) are expected to dominate the market of wearable electronics in the next 5 years. This foreseen proliferation of HMDs yields a plethora of design opportunities for revolutionizing everyday life via novel use cases, but also generates a considerable number of substantial safety implications. In this work, we systematically investigated the effect of a novel monocular laser-based HMD on the ability of our participants to see in low ambient light conditions in lab settings. We recruited a total of 19 participants in two studies and performed a series of established vision tests while using the newly available Focals by North HMD. We tested our participants' night vision after being exposed to different levels of laser luminous power and laser colors while using Focals, either with one or both eyes open. Our results showcase that the image perceived by the non-exposed eye compensates for the loss of contrast sensitivity observed in the image perceived by the laser-exposed eye. This indicates that monocular laser-based HMDs, such as Focals, permit dark adaptation to occur naturally for the non-exposed eye.",
    "keywords": [
      "Laser light projection",
      "Head-mounted displays",
      "Human Factors",
      "Scotopic vision"
    ],
    "doi": "10.1145/3311823.3311858",
    "url": "https://doi.org/10.1145/3311823.3311858",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "31",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311859",
    "title": "MagniFinger: Fingertip Probe Microscope with Direct Micro Movements",
    "authors": [
      "Noriyasu Obushi",
      "Sohei Wakisaka",
      "Shunichi Kasahara",
      "Katie Seaborn",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "By adulthood, our fingers have developed a high level of dexterity: sensory and motor skills that developers have only just started to make use of in modern interfaces. Previous research has unveiled the possibilities of enhancing touch modalities by introducing visual feedback of the magnified touch image. Yet, most of the microscopes on the market require a complicated procedure to operate and this makes it difficult to move the felt/observed area. To address this, we introduce MagniFinger, a new finger-based microscope that allows users to magnify the contacting surface on their fingertips using two means of control: sliding and tilting. The tilting-based control enables a more precise movement under micro-environments. According to the results of our experiments, it shortens the time of reaching targets compared to the simple sliding-based control.",
    "keywords": [
      "finger-worn device",
      "magnification",
      "microscope"
    ],
    "doi": "10.1145/3311823.3311859",
    "url": "https://doi.org/10.1145/3311823.3311859",
    "citations": 6,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "32",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311860",
    "title": "Let Your World Open: CAVE-Based Visualization Methods of Public Virtual Reality towards a Shareable VR Experience",
    "authors": [
      "Akira Ishii",
      "Masaya Tsuruta",
      "Ippei Suzuki",
      "Shuta Nakamae",
      "Junichi Suzuki",
      "Yoichi Ochiai"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Virtual reality (VR) games are currently becoming part of the public-space entertainment (e.g., VR amusement parks). Therefore, VR games should be attractive for players, as well as for bystanders. Current VR systems are still mostly focused on enhancing the experience of the head-mounted display (HMD) users; thus, bystanders without an HMD cannot enjoy the experience together with the HMD users. We propose the \"ReverseCAVE\": a proof-of-concept prototype for public VR visualization using CAVE-based projection with translucent screens for bystanders toward a shareable VR experience. The screens surround the HMD user and the VR environment is projected onto the screens. This enables the bystanders to see the HMD user and the VR environment simultaneously. We designed and implemented the ReverseCAVE, and evaluated it in terms of the degree of attention, attractiveness, enjoyment, and shareability, assuming that it is used in a public space. Thus, we can make the VR world more accessible and enhance the public VR experience of the bystanders via the ReverseCAVE.",
    "keywords": [
      "mixed reality (MR)",
      "CAVE",
      "sharing experience",
      "Environmental VR"
    ],
    "doi": "10.1145/3311823.3311860",
    "url": "https://doi.org/10.1145/3311823.3311860",
    "citations": 26,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "33",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311862",
    "title": "Double Shellf: What Psychological Effects Can Be Caused through Interaction with a Doppelganger?",
    "authors": [
      "Yuji Hatada",
      "Shigeo Yoshida",
      "Takuji Narumi",
      "Michitaka Hirose"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Advances in 3D capture technology have made it easier to generate a realistic avatar, which can represent a person in virtual environments. Because avatars can be easily duplicated in the virtual environments, there can be an unrealistic situation where a person sees her/his own doppelgangers. Doppelganger is a double of a person and sometimes portrayed as a sinister existence. To investigate how people feel and react when they face their doppelgangers, we developed \"Double Shellf\", a virtual reality experience in which people can interact with their virtual doppelgangers in various situations. In this paper, we introduce the design of Double Shellf and discuss the reactions of 86 users. The user study revealed that most people felt intense eeriness when they see their doppelgangers which acts autonomously and when they were touched by their doppelgangers. We also found that there is a gender difference in reactions to their doppelgangers. We explore the effective way of utilizing doppelgangers.",
    "keywords": [
      "Avatar",
      "Social Psychology",
      "Doppelganger",
      "Virtual Reality"
    ],
    "doi": "10.1145/3311823.3311862",
    "url": "https://doi.org/10.1145/3311823.3311862",
    "citations": 11,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "34",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311847",
    "title": "Augmenting Human With a Tail",
    "authors": [
      "Haoran Xie",
      "Kento Mitsuhashi",
      "Takuma Torii"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Human-augmentation devices have been extensively proposed and developed recently and are useful in improving our work efficiency and our quality of life. Inspired by animal tails, this study aims to propose a wearable and functional tail device that combines physical and emotional-augmentation modes. In the physical-augmentation mode, the proposed device can be transformed into a consolidated state to support a user's weight, similar to a kangaroo's tail. In the emotional-augmentation mode, the proposed device can help users express their emotions, which are realized by different tail-motion patterns. For our initial prototype, we developed technical features that can support the weight of an adult, and we performed a perceptional investigation of the relations between the tail movements and the corresponding perceptual impressions. Using the animal-tail analog, the proposed device may be able to help the human user in both physical and emotional ways.",
    "keywords": [
      "Human Augmentation",
      "Tail Device",
      "Emotional Expression",
      "Physical Capability"
    ],
    "doi": "10.1145/3311823.3311847",
    "url": "https://doi.org/10.1145/3311823.3311847",
    "citations": 37,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "35",
    "numpages": "7",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311848",
    "title": "Prosthetic Tail: Artificial Anthropomorphic Tail for Extending Innate Body Functions",
    "authors": [
      "Junichi Nabeshima",
      "MHD Yamen Saraiji",
      "Kouta Minamizawa"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "For most mammals and vertebrate animals, tail plays an important role for their body providing variant functions to expand their mobility, or as a limb that allows manipulation and gripping. In this paper, we propose an exploratory biomimicry-inspired anthropomorphic tail design to allow engineering and expanding human body functions. The proposed tail consists of adjacent joints with a spring-based structure to handle shearing and tangential forces, and allow managing the length and weight of the target tail. The internal structure of the tail is driven by four pneumatic artificial muscles providing the actuation mechanism for the tail tip. Here we describe the design and implementation process, and highlight potential applications for using such prosthetic tail.",
    "keywords": [
      "Artificial Tail",
      "Biomechanics",
      "Biomimicry",
      "Embodied Robotics"
    ],
    "doi": "10.1145/3311823.3311848",
    "url": "https://doi.org/10.1145/3311823.3311848",
    "citations": 13,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "36",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311850",
    "title": "Orochi: Investigating Requirements and Expectations for Multipurpose Daily Used Supernumerary Robotic Limbs",
    "authors": [
      "Mohammed Al-Sada",
      "Thomas Höglund",
      "Mohamed Khamis",
      "Jaryd Urbani",
      "Tatsuo Nakajima"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Supernumerary robotic limbs (SRLs) present many opportunities for daily use. However, their obtrusiveness and limitations in interaction genericity hinder their daily use. To address challenges of daily use, we extracted three design considerations from previous literature and embodied them in a wearable we call Orochi. The considerations include the following: 1) multipurpose use, 2) wearability by context, and 3) unobtrusiveness in public. We implemented Orochi as a snake-shaped robot with 25 DoFs and two end effectors, and demonstrated several novel interactions enabled by its limber design. Using Orochi, we conducted hands-on focus groups to explore how multipurpose SRLs are used daily and we conducted a survey to explore how they are perceived when used in public. Participants approved Orochi's design and proposed different use cases and postures in which it could be worn. Orochi's unobtrusive design was generally well received, yet novel interactions raise several challenges for social acceptance. We discuss the significance of our results by highlighting future research opportunities based on the design, implementation, and evaluation of Orochi.",
    "keywords": [
      "Augmentation",
      "Design",
      "Wearable",
      "Unobtrusive",
      "Multipurpose"
    ],
    "doi": "10.1145/3311823.3311850",
    "url": "https://doi.org/10.1145/3311823.3311850",
    "citations": 46,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "37",
    "numpages": "9",
    "influentialCitations": 6
  },
  {
    "id": "10.1145/3311823.3311849",
    "title": "Naviarm: Augmenting the Learning of Motor Skills Using a Backpack-Type Robotic Arm System",
    "authors": [
      "Azumi Maekawa",
      "Shota Takahashi",
      "MHD Yamen Saraiji",
      "Sohei Wakisaka",
      "Hiroyasu Iwata",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "We present a wearable haptic assistance robotic system for augmented motor learning called Naviarm. This system comprises two robotic arms that are mounted on a user's body and are used to transfer one person's motion to another offline. Naviarm pre-records the arm motion trajectories of an expert via the mounted robotic arms and then plays back these recorded trajectories to share the expert's body motion with a beginner. The Naviarm system is an ungrounded system and provides mobility for the user to conduct a variety of motions. In this paper, we focus on the temporal aspect of motor skill and use a mime performance as a case study learning task. We verified the system effectiveness for motor learning using the conducted experiments. The results suggest that the proposed system has benefits for learning sequential skills.",
    "keywords": [
      "Augmented learning",
      "Motor Learning",
      "Haptics",
      "Robotics",
      "Wearable Device"
    ],
    "doi": "10.1145/3311823.3311849",
    "url": "https://doi.org/10.1145/3311823.3311849",
    "citations": 30,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "38",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3311823.3311855",
    "title": "BitoBody: Real-Time Human Contact Detection and Dynamic Projection System",
    "authors": [
      "Erwin Wu",
      "Mistki Piekenbrock",
      "Hideki Koike"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this research, we propose a novel human body contact detection and projection system with dynamic mesh collider. We use motion capture camera and generated human 3D models to detect the contact between user's bodies. Since it is difficult to update human mesh collider every frame, a special algorithm that divides body meshes into small pieces of polygons to do collision detection is developed and detected hit information will be dynamically projected according to its magnitude of damage. The maximum deviation of damage projection is about 7.9cm under a 240-fps optitrack motion capture system and 12.0cm under a 30-fps Kinect camera. The proposed system can be used in various sports where bodies come in contact and it allows the audience and players to understand the context easier.",
    "keywords": [
      "Real-time contact detection",
      "Human 3D model Generatrion",
      "Dynamic projection mapping"
    ],
    "doi": "10.1145/3311823.3311855",
    "url": "https://doi.org/10.1145/3311823.3311855",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "39",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311827",
    "title": "CompoundDome: A Wearable Dome Device That Enables Interaction with the Real World by Controlling the Transparency of the Screen",
    "authors": [
      "Eriko Maruyama",
      "Jun Rekimoto"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "The head-mounted display (HMD) is widely used as a method to experience virtual space. However, HMD has problems in mounting, such as skin touching the equipment used by others, functional issues such as easy to induce VR sickness. In this research, we propose a wearable dome device named \"CompoundDome\", which enables interaction with the real world by projecting images on the dome. In our system, we used a 600 mm diameter dome, and a projector projects images to the dome to cover the wearer's field of view. With this configuration, the equipment does not touch the skin, and motion sickness can be reduced. HMD also lacks in providing face-to-face communication, because it hides user's face. In addition, the wearer can not see the outside when wearing the HMD. Hence, we applied screen paint to the transparent dome in a mesh form. With this configuration, users can see the image when the image is projected, and they can see the outside of the dome when the image is not projected. Furthermore, users and the surrounding people can make face to face communication by photographing the face with the camera installed in the dome and projecting the face in the virtual space. In this paper, we describe the composition of CompoundDome, in comparison with other virtual space presentation means, and various applications enabled by CompoundDome.",
    "keywords": [
      "Dome shape",
      "Mixed Reality",
      "human augmentation"
    ],
    "doi": "10.1145/3311823.3311827",
    "url": "https://doi.org/10.1145/3311823.3311827",
    "citations": 4,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "40",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311853",
    "title": "Investigating Universal Appliance Control through Wearable Augmented Reality",
    "authors": [
      "Vincent Becker",
      "Felix Rauchenstein",
      "Gábor Sörös"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "The number of interconnected devices around us is constantly growing. However, it may become challenging to control all these devices when control interfaces are distributed over mechanical elements, apps, and configuration webpages. We investigate interaction methods for smart devices in augmented reality. The physical objects are augmented with interaction widgets, which are generated on demand and represent the connected devices along with their adjustable parameters. For example, a loudspeaker can be overlaid with a controller widget for its volume. We explore three ways of manipulating the virtual widgets: (a) in-air finger pinching and sliding, (b) whole arm gestures rotating and waving, (c) incorporating physical objects in the surrounding and mapping their movements to the interaction primitives. We compare these methods in a user study with 25 participants and find significant differences in the preference of the users, the speed of executing commands, and the granularity of the type of control.",
    "keywords": [
      "Ubiquitous Computing",
      "Tangible User Interfaces",
      "Augmented Reality",
      "Wearable Computing",
      "Smart Object"
    ],
    "doi": "10.1145/3311823.3311853",
    "url": "https://doi.org/10.1145/3311823.3311853",
    "citations": 12,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "41",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311874",
    "title": "CapMat: A Smart Foot Mat for User Authentication",
    "authors": [
      "Denys J. C. Matthies",
      "Don Samitha Elvitigala",
      "Sachith Muthukumarana",
      "Jochen Huber",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "We present CapMat, a smart foot mat that enables user identification, supporting applications such as multi-layer authentication. CapMat leverages a large form factor capacitive sensor to capture shoe sole images. These images vary based on shoe form factors, the individual wear, and the user's weight. In a preliminary evaluation, we distinguished 15 users with an accuracy of up to 100%.",
    "keywords": [
      "User Identification",
      "Implicit Authentication",
      "Capacitive Sensing",
      "Smart Home",
      "Floor mat"
    ],
    "doi": "10.1145/3311823.3311874",
    "url": "https://doi.org/10.1145/3311823.3311874",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "42",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311833",
    "title": "CricketCoach: Towards Creating a Better Awareness of Gripping Forces for Cricketers",
    "authors": [
      "Sachith Muthukumarana",
      "Denys J. C. Matthies",
      "Chamod Weerasinghe",
      "Don Samitha Elvitigala",
      "Suranga Nanayakkara"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we demonstrate a smart system that creates awareness of the hand-grip force for cricket players. A custom Force-Sensitive Resistor (FSR) matrix is attached to the bat's handle to sense the gripping. Two wrist bands, incorporating vibration motors, provide feedback that helps nonexpert users to understand the relative forces exerted by each hand while performing a stroke. A preliminary user study was conducted to collect first insights.",
    "keywords": [],
    "doi": "10.1145/3311823.3311833",
    "url": "https://doi.org/10.1145/3311823.3311833",
    "citations": 3,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "43",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311866",
    "title": "Second Language Vocabulary Learning While Walking",
    "authors": [
      "Shogo Fukushima",
      "Ari Hautasaari",
      "Takeo Hamada"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Second language (L2) learners often lack opportunities or motivation to dedicate their time to vocabulary learning over other daily activities. In this work, we introduce a mobile application that allows L2 learners to instead leverage their \"dead time\", such as when walking to and from school or work, to study new vocabulary items. The application combines audio learning and location-based contextually relevant L1-L2 word pairs to allow L2 learners to \"discover\" new foreign language words while walking. We report on the evaluation of the approach from three aspects: L2 vocabulary retention after 1 month, system usability and workload.",
    "keywords": [
      "Second language vocabulary learning",
      "contextualized audio",
      "incidental learning",
      "walking"
    ],
    "doi": "10.1145/3311823.3311866",
    "url": "https://doi.org/10.1145/3311823.3311866",
    "citations": 14,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "44",
    "numpages": "2",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3311823.3311852",
    "title": "Build Your Own! Open-Source VR Shoes for Unity3D",
    "authors": [
      "Jens Reinhardt",
      "Eike Lewandowski",
      "Katrin Wolf"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "Hand-held controllers enable all kinds of interaction in Virtual Reality (VR), such as object manipulation as well as for locomotion. VR shoes allow using the hand exclusively for naturally manual tasks, such as object manipulation, while locomotion could be realized through feet input -- just like in the physical world. While hand-held VR controllers became standard input devices for consumer VR products, VR shoes are only barely available, and also research on that input modality remains open questions. We contribute here with open-source VR shoes and describe how to build and implement them as Unity3D input device. We hope to support researchers in VR research and practitioners in VR product design to increase usability and natural interaction in VR.",
    "keywords": [
      "Virtual Reality",
      "Foot Interaction",
      "Locomotion"
    ],
    "doi": "10.1145/3311823.3311852",
    "url": "https://doi.org/10.1145/3311823.3311852",
    "citations": 5,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "45",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311867",
    "title": "Design of Enhanced Flashcards for Second Language Vocabulary Learning with Emotional Binaural Narration",
    "authors": [
      "Shogo Fukushima"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "In this paper, we report on the design of a flashcard application with which learners experience the meaning of written words with emotional binaural voice narrations to enhance second language vocabulary learning. Typically, voice used in English vocabulary learning is recorded by a native speaker with no accent, and it aims for accurate pronunciation and clarity. However, the voice can also be flat and monotonous, and it can be difficult for learners to retain the new vocabulary in the semantic memory. Enhancing textual flashcards with emotional narration in the learner's native language helps the retention of new second language vocabulary items in the episodic memory instead of the semantic memory. Further, greater emotionality in the narration reinforces the retention of episodic memory.",
    "keywords": [
      "Emotion",
      "vocabulary learning",
      "binaural recording"
    ],
    "doi": "10.1145/3311823.3311867",
    "url": "https://doi.org/10.1145/3311823.3311867",
    "citations": 0,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "46",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3311856",
    "title": "AR Pottery Wheel-Throwing by Attaching Omnidirectional Cameras to the Center of a User's Palms",
    "authors": [
      "Yusuke Maruyama",
      "Yasuyuki Kono"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "This research describes our system for AR pottery wheel-throwing employing an HMD and omnidirectional cameras each of which is attached to the center of a user's palm. The omnidirectional cameras enable the user's finger postures and the three-dimensional relative position and orientation between the user's hands and virtual clay model on the wheel to be estimated. Our system detects a marker on the desk and the wheel is set on its coordinate system along with the finger posture estimation in real time. The system then simulates the collision between the virtual clay model and the left/right hand model based on the above information. Pottery wheel-throwing is reproduced in Unity software environment by deforming the clay model by contact with hand models in this simulation.",
    "keywords": [
      "Omnidirectional Camera",
      "Finger Posture",
      "Marker-based AR"
    ],
    "doi": "10.1145/3311823.3311856",
    "url": "https://doi.org/10.1145/3311823.3311856",
    "citations": 2,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "47",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3311823.3313868",
    "title": "Demonstrating Naviarm: Augmenting the Learning of Motor Skills Using a Backpack-Type Robotic Arm System",
    "authors": [
      "Azumi Maekawa",
      "Shota Takahashi",
      "MHD Yamen Saraiji",
      "Sohei Wakisaka",
      "Hiroyasu Iwata",
      "Masahiko Inami"
    ],
    "year": 2019,
    "conference": "AHS",
    "conferenceYear": "AH2019",
    "abstract": "We present a wearable haptic assistance robotic system for augmented motor learning called Naviarm. This system comprises two robotic arms that are mounted on a user's body and are used to transfer one person's motion to another offline. Naviarm prerecords the arm motion trajectories of an expert via the mounted robotic arms and then plays back these recorded trajectories to share the expert's body motion with a beginner. The Naviarm system is an ungrounded system and provides mobility for the user to conduct a variety of motions. In our demonstration, the user will experience the recording of arm movement with backpack-type robotic arm. Then, the recorded movement will replayed and the user can experience the haptic feedback.",
    "keywords": [
      "Robotics",
      "Haptics",
      "Augmented learning",
      "Wearable Device",
      "Motor Learning"
    ],
    "doi": "10.1145/3311823.3313868",
    "url": "https://doi.org/10.1145/3311823.3313868",
    "citations": 1,
    "booktitle": "Proceedings of the 10th Augmented Human International Conference 2019",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Reims, France",
    "articleno": "48",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384787",
    "title": "Eye-based Interaction Using Embedded Optical Sensors on an Eyewear Device for Facial Expression Recognition",
    "authors": [
      "Katsutoshi Masai",
      "Kai Kunze",
      "Maki Sugimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Non-verbal information is essential to understand intentions and emotions and to facilitate social interaction between humans and between humans and computers. One reliable source of such information is the eyes. We investigated the eye-based interaction (recognizing eye gestures or eye movements) using an eyewear device for facial expression recognition. The device incorporates 16 low-cost optical sensors. The system allows hands-free interaction in many situations. Using the device, we evaluated three eye-based interactions. First, we evaluated the accuracy of detecting the gestures with nine participants. The average accuracy of detecting seven different eye gestures is 89.1% with user-dependent training. We used dynamic time warping (DTW) for gesture recognition. Second, we evaluated the accuracy of eye gaze position estimation with five users holding a neutral face. The system showed potential to track the approximate direction of the eyes, with higher accuracy in detecting position y than x. Finally, we did a feasibility study of one user reading jokes while wearing the device. The system was capable of analyzing facial expressions and eye movements in daily contexts.",
    "keywords": [
      "eyewear computing",
      "gaze gesture",
      "wearable computing"
    ],
    "doi": "10.1145/3384657.3384787",
    "url": "https://doi.org/10.1145/3384657.3384787",
    "citations": 18,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "1",
    "numpages": "10",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3384657.3384659",
    "title": "Altering the Speed of Reality? Exploring Visual Slow-Motion to Amplify Human Perception using Augmented Reality",
    "authors": [
      "Pascal Knierim",
      "Thomas Kosch",
      "Gabrielle LaBorwit",
      "Albrecht Schmidt"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Many events happen so fast that we cannot observe them well with our naked eye. The temporal and spatial limitations of visual perception are well known and determine what we can actually see. Over the last years, sensors and camera systems became available that have surpassed the limitations of human perception. In this paper, we investigate how we can use augmented reality to create a system that allows altering the speed in which we perceive the world around us. We contribute an experimental exploration of how we can implement visual slow-motion to amplify human perception. We outline the research challenges and describe a conceptual architecture for manipulating the temporal perception. Using augmented reality glasses, we created a proof-of-concept implementation and conducted a study of which we report qualitative and quantitative results. We show how providing visual information from the environment at different speeds has benefits for the user. We also highlight the required new approaches to design interfaces that deal with decoupling the perception of the real would.",
    "keywords": [
      "Augmented Reality",
      "Human Augmentation",
      "Mixed Reality",
      "Perceptual Amplification",
      "Proof of Concept"
    ],
    "doi": "10.1145/3384657.3384659",
    "url": "https://doi.org/10.1145/3384657.3384659",
    "citations": 22,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "2",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384781",
    "title": "DehazeGlasses: Optical Dehazing with an Occlusion Capable See-Through Display",
    "authors": [
      "Yuichi Hiroi",
      "Takumi Kaminokado",
      "Atsushi Mori",
      "Yuta Itoh"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "We present DehazeGlasses, a see-through visual haze removal system that optically dehazes the user's field of vision. Human vision suffers from a degraded view due to aspects of the scene environment, such as haze. Such degradation may interfere with our behavior or judgement in daily tasks. We focus on hazy scenes as one common degradation source, which whitens the view due to certain atmospheric conditions. Unlike typical computer vision systems that process recorded images, we aim to realize a see-through glasses system that can optically manipulate our field of view to dehaze the perceived scene. Our system selectively modulates the intensity of the light entering the eyes via occlusion-capable optical see-through head-mounted displays (OST-HMD). We built a proof-of-concept system to evaluate the feasibility of our haze removal method by combining a digital micromirror device (DMD) and an OST-HMD, and tested it with a user-perspective viewpoint camera. A quantitative evaluation with 80 scenes from a haze removal dataset shows that our system realizes a dehazed view that is significantly closer to the ground truth scene compared to the native view under a perceptual image similarity metric. This evaluation shows that our system achieves perceptually natural haze removal while maintaining the see-through view of actual scenes.",
    "keywords": [
      "Augmented Reality",
      "Haze Removal",
      "Head-Mounted Displays",
      "Occlusion-Capable HMD",
      "Vision Augmentation"
    ],
    "doi": "10.1145/3384657.3384781",
    "url": "https://doi.org/10.1145/3384657.3384781",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "3",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384802",
    "title": "Vision Extension for a Ball Camera by Using Image Completion",
    "authors": [
      "Tsubasa Kitayama",
      "Shio Miyafuji",
      "Hideki Koike"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "A ball camera is a high tech ball equipped with cameras to obtain images from the ball's point of view. To capture 360° images with a minimum number of cameras, we embedded two ultra-wide-angle cameras in the ball. However, the two cameras were set diametrically opposite to each other; therefore, a blind spot was present, which could not be captured by the two cameras. Consequently, the obtained 360° image had a discontinuous part. In this paper, we describe the design and implementation of the ball camera and propose a method for completing the blind spot using image completion. We applied our method to the videos captured by the ball camera and proved this method successfully completes the blind spot and provides more real-life images.",
    "keywords": [
      "Ball camera",
      "Image completion",
      "Mixed reality",
      "Omnidirectional camera"
    ],
    "doi": "10.1145/3384657.3384802",
    "url": "https://doi.org/10.1145/3384657.3384802",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "4",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384796",
    "title": "OmniView: An Exploratory Study of 360 Degree Vision using Dynamic Distortion based on Direction of Interest",
    "authors": [
      "Feng Liang",
      "Stevanus Kevin",
      "Holger Baldauf",
      "Kai Kunze",
      "Yun Suen Pai"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "The possibility of providing humans with a 360 field of view (FOV) is an area that has fascinated researchers for some time. We introduce OmniView, an exploratory study to determine an optimized 360 FOV vision using dynamic distortion methods for reducing distortion and enlarging the area of the direction of interest. We developed three variations of OmniView to trigger distortions: eye gaze selection, pointing selection, and automatic selection. Our first study (n=16) compares these methods with no dynamic distortion (equirectangular view) in terms of spatial perception change and reaction time. The second study (n=16) evaluated subjective simulator sickness while performing simple everyday tasks. There is no significant change between OmniView and a no distortion baseline regarding spatial perception and reaction time. However, there is a statistically significant rise in disorientation from before the study to after using Pointing OmniView and Automatic OmniView. Most users (n=13) have a strong preference towards Gaze OmniView. From the results, we provide design guidelines towards further optimizing 360 vision and preserving our spatial sense and reaction time while keeping simulator sickness to a minimum.",
    "keywords": [
      "360 degree vision",
      "projections",
      "visual augmentation"
    ],
    "doi": "10.1145/3384657.3384796",
    "url": "https://doi.org/10.1145/3384657.3384796",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "5",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384775",
    "title": "The Lateral Line: Augmenting Spatiotemporal Perception with a Tactile Interface",
    "authors": [
      "Matti Krüger",
      "Christiane B. Wiebel-Herboth",
      "Heiko Wersing"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this paper we describe a concept for artificially supplementing peoples' spatiotemporal perception. Our target is to improve performance in tasks that rely on a fast and accurate understanding of movement dynamics in the environment.To provide an exemplary research and application scenario, we implemented a prototype of the concept in a driving simulation environment and used an interface capable of providing vibrotactile stimuli around the waist to communicate spatiotemporal information. The tactile stimuli dynamically encode directions and temporal proximities towards approaching objects. Temporal proximity is defined as inversely proportional to the time-to-contact and can be interpreted as a measure of imminent collision risk and temporal urgency. Results of a user study demonstrate performance benefits in terms of enhanced driving safety. This indicates a potential for improving peoples' capabilities in assessing relevant properties of dynamic environments in order to purposefully adapt their actions.",
    "keywords": [
      "Augmented Perception",
      "Driver Assistance",
      "Sensory Augmentation",
      "TTC",
      "Tactile Interface",
      "Time Encoding"
    ],
    "doi": "10.1145/3384657.3384775",
    "url": "https://doi.org/10.1145/3384657.3384775",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "6",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384777",
    "title": "HapticPointer: A Neck-worn Device that Presents Direction by Vibrotactile Feedback for Remote Collaboration Tasks",
    "authors": [
      "Akira Matsuda",
      "Kazunori Nozawa",
      "Kazuki Takata",
      "Atsushi Izumihara",
      "Jun Rekimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "We designed a necklace-style device named HapticPointer for presenting a direction as pointing cues in remote collaboration tasks. The device has 16 vibration motors placed along a line of flexible string. Our vibration algorithm represents horizontal and vertical directions by changing the position and intensity of each vibration. In our experiment, participants attempted to find a specific target, and the accuracy of successful trials reached 90.65%. Moreover, the participants found the targets in 6 seconds on average. Furthermore, our user study implies that the device can simulates the sensation of walking together. It is assumed that the sensation improves engagement between the local and remote users.",
    "keywords": [
      "haptic feedback",
      "remote collaboration",
      "telepresence",
      "wearable device"
    ],
    "doi": "10.1145/3384657.3384777",
    "url": "https://doi.org/10.1145/3384657.3384777",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "7",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384794",
    "title": "GenVibe: Exploration of Interactive Generation of Personal Vibrotactile Patterns",
    "authors": [
      "Erik Pescara",
      "Florian Dreschner",
      "Karola Marky",
      "Kai Kunze",
      "Michael Beigl"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Research about vibrotactile patterns is traditionally conducted with patterns handcrafted by experts which are then subsequently evaluated in general user studies. The current empirical approach to designing vibrotactile patterns mostly utilizes expert decisions and is notably not adapted to individual differences in the perception of vibration. This work describes GenVibe: a novel approach to designing vibrotactile patterns by examining the automatic generation of personal patterns. GenVibe adjusts patterns to the perception of an individual through the utilization of interactive generative models. An algorithm is described and tested with a dummy smartphone made from off-the-shelf electronic components. Afterward, a user study with 11 participants evaluates the outcome of GenVibe. Results show a significant increase in accuracy from 73.6% to 84.0% and a higher confidence ratings by the users.",
    "keywords": [
      "generative models",
      "genetic algorithm",
      "haptics",
      "vibrotactile patterns"
    ],
    "doi": "10.1145/3384657.3384794",
    "url": "https://doi.org/10.1145/3384657.3384794",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "8",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384792",
    "title": "Manipulatable Auditory Perception in Wearable Computing",
    "authors": [
      "Hiroki Watanabe",
      "Tsutomu Terada"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "We proposed a framework to manipulate auditory perception. Since auditory perception is passive sense, we often do not notice important information and acquire unimportant information. In this study, we focused on earphone-type wearable computers (hearable devices) that not only have speakers but also microphones. In a hearable computing environment, we always attach microphones and speakers to the ears. Therefore, we can manipulate our auditory perception using a hearable device. We manipulated the frequency of the input sound from the microphones and transmitted the converted sound from the speakers. Thus, we could acquire the sound that is not heard with our normal auditory perception and eliminate the unwanted sound according to the user's requirements. We devised five types of frequency-manipulating techniques and implemented a prototype device. Moreover, we proposed seven assumed applications that can be realized by the proposed method.",
    "keywords": [
      "Wearable computing",
      "auditory perception",
      "hearable device",
      "ultrasound"
    ],
    "doi": "10.1145/3384657.3384792",
    "url": "https://doi.org/10.1145/3384657.3384792",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "9",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384785",
    "title": "Novel Input and Output opportunities using an Implanted Magnet",
    "authors": [
      "Paul Strohmeier",
      "Jess McIntosh"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this case study, we discuss how an implanted magnet can support novel forms of input and output. By measuring the relative position between the magnet and an on-body device, local position of the device can be used for input. Electromagnetic fields can actuate the magnet to provide output by means of in-vivo haptic feedback. Traditional tracking options would struggle tracking the input methods we suggest, and the in-vivo sensations of vibration provided as output differ from the experience of vibrations applied externally - our data suggests that in-vivo vibrations are mediated by different receptors than external vibration. As the magnet can be easily tracked as well as actuated it provides opportunities for encoding information as material experiences.",
    "keywords": [
      "haptics",
      "implant",
      "implanted magnet",
      "magnet",
      "perception"
    ],
    "doi": "10.1145/3384657.3384785",
    "url": "https://doi.org/10.1145/3384657.3384785",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "10",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384791",
    "title": "Sensor Glove Implemented with Artificial Muscle Set for Hand Rehabilitation",
    "authors": [
      "Biyuan Wang",
      "Nobuhiro Takahashi",
      "Hideki Koike"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Stroke is a disease caused by either a shortage of blood supply to the brain and causes upper limb motor impairment. Given that lots of the surviving stroke patients are required to undergo physical rehabilitation, we proposed a hand physical rehabilitation system which targets at the post stroke patients. The system consists of two parts: a digital glove implemented with sensors on each joint as the rehabilitation reference and a soft exoskeleton set as the motor system. The users are able to wear the equipment easily and can obtain real time performance monitoring via attached flex sensors. This is an ergonomic, light and portable rehabilitation system within a room area which provides the patient with more independence. In this paper, we described the design idea and the usage of this system.",
    "keywords": [
      "Artificial Muscles",
      "Human Centred Assistive Rehabilitation",
      "Sensor Glove",
      "Wearable Devices"
    ],
    "doi": "10.1145/3384657.3384791",
    "url": "https://doi.org/10.1145/3384657.3384791",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "11",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384780",
    "title": "Accelerating Skill Acquisition of Two-Handed Drumming using Pneumatic Artificial Muscles",
    "authors": [
      "Takashi Goto",
      "Swagata Das",
      "Katrin Wolf",
      "Pedro Lopes",
      "Yuichi Kurita",
      "Kai Kunze"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "While computers excel at augmenting user's cognitive abilities, only recently we started utilizing their full potential to enhance our physical abilities. More and more wearable force-feedback devices have been developed based on exoskeletons, electrical muscle stimulation (EMS) or pneumatic actuators. The latter, pneumatic-based artificial muscles, are of particular interest since they strike an interesting balance: lighter than exoskeletons and more precise than EMS. However, the promise of using artificial muscles to actually support skill acquisition and training users is still lacking empirical validation.In this paper, we unveil how pneumatic artificial muscles impact skill acquisition, using two-handed drumming as an example use case. To understand this, we conducted a user study comparing participants' drumming performance after training with the audio or with our artificial-muscle setup. Our haptic system is comprised of four pneumatic muscles and is capable of actuating the user's forearm to drum accurately up to 80 bpm. We show that pneumatic muscles improve participants' correct recall of drumming patterns significantly when compared to auditory training.",
    "keywords": [
      "Force-feedback",
      "motor learning",
      "pneumatic artificial muscles (PAMs)"
    ],
    "doi": "10.1145/3384657.3384780",
    "url": "https://doi.org/10.1145/3384657.3384780",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "12",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384658",
    "title": "PoseAsQuery: Full-Body Interface for Repeated Observation of a Person in a Video with Ambiguous Pose Indexes and Performed Poses",
    "authors": [
      "Natsuki Hamanishi",
      "Jun Rekimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "PoseAsQuery is an interactive browsing system used to repeatedly replay any specific segment of a video. To acquire and improve the body control skills that are essential for physical performance, it is necessary to continuously observe an individual's movement. However, previous research has several issues on the ability to repeatedly observe a user's movement without interruption during personal training. For these scenarios, the proposed system simultaneously provides a pose-based browsing method and its interface. The user can pin any frame from which a replay starts by using the person's body movement without another operation, such as pressing a button, that is not essential for improving the skill. The experimental results suggest that users can use posture to decide where to start replaying when they know that the corresponding pose enables them to replay the scene. We concluded that the proposed method enables users to repeatedly observe scenes more efficiently and in a less stressful way than pressing a button to replay from a fixed scene.",
    "keywords": [
      "Assisting Training",
      "Movement Guidance",
      "Video Observation"
    ],
    "doi": "10.1145/3384657.3384658",
    "url": "https://doi.org/10.1145/3384657.3384658",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "13",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384786",
    "title": "Investigation of Effective Parts for Rotation and Translation of the Legs Using Hanger Reflex",
    "authors": [
      "Hanamichi Sanada",
      "Masato Kobayashi",
      "Yuki Kon",
      "Hiroyuki Kajimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "When wearing a wire hanger on the head, an illusory force sense called the Hanger Reflex will occur. This phenomenon transpires not only at the head but also at the waists, wrists, and ankles. Dynamic control of the phenomenon by using pneumatic balloons has been proposed for the waist-type Hanger Reflex to enable direct rotation and translation of the user's body. However, this method is unsuitable for teaching different directions for the right and left legs, which is considered necessary for sports applications. In this study, we investigated which part of the leg is most effective for the direct rotation and translation of the leg. We also investigated the angle of the leg that was rotated when the Hanger Reflex device was attached to the thigh, knee, shin, and ankle, and the amount of translation. As a result, it was found that the most effective way to rotate and translate the leg was to rotate in the knee region, and that a greater extent of leg rotation was possible here than in other studies. These results suggest that a Hanger Reflex device is a possible candidate to rotate and translate the leg.",
    "keywords": [
      "Hanger-Reflex",
      "Haptic Display",
      "Haptics"
    ],
    "doi": "10.1145/3384657.3384786",
    "url": "https://doi.org/10.1145/3384657.3384786",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "14",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384784",
    "title": "Go-Through: Disabling Collision to Access Obstructed Paths and Open Occluded Views in Social VR",
    "authors": [
      "Jens Reinhardt",
      "Katrin Wolf"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Social Virtual Reality (VR) offers new opportunities for designing social experiences, but at the same time, it challenges the usability of VR as other avatars can block paths and occlude one's avatar's view. In contrast to designing VR similar to the physical reality, we allow avatars to go through and to see through other avatars. In detail, we vary the property of avatars to collide with other avatars. To better understand how such properties should be implemented, we also explore multimodal feedback when avatars collide with each other. Results of a user study show that multimodal feedback on collision yields to a significantly increased sensation of presence in Social VR. Moreover, while the loss of collision (the possibility to go through other avatars) causes a significant decrease of felt co-presence, qualitative feedback showed that the ability to walk through avatars can ease to access spots of interest. Finally, we observed that the purpose of Social VR determines how useful the possibility to walk through avatars is.We conclude with design guidelines that distinguish between Social VR with a priority on social interaction, Social VR supporting education and information, and hybrid Social VR enabling education and information in a social environment.",
    "keywords": [
      "Social VR",
      "Virtual Reality",
      "avatar collision",
      "avatar collision feedback",
      "avatar collision response"
    ],
    "doi": "10.1145/3384657.3384784",
    "url": "https://doi.org/10.1145/3384657.3384784",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "15",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384795",
    "title": "Remote Treatment System of Phantom Limb Pain by Displaying Body Movement in Shared VR Space",
    "authors": [
      "Kenta Saito",
      "Atsushi Okada",
      "Yu Matsumura",
      "Jun Rekimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "The phenomenon that amputees feel pain at the position of their lost limb is called phantom limb pain. Although the cause of phantom limb pain has not yet been medically elucidated, several hypotheses have been established from past studies, and some treatment systems of phantom limb pain have been developed based on these hypotheses. In these treatments, the instructions of the therapist who is familiar with phantom limb pain are essential. However, there is a problem that the number of therapists is insufficient and that existing treatment systems require the therapist to be next to the patient. In order to solve this problem, realizing remote treatment is expected. Therefore, in this research, we propose a phantom limb pain remote treatment system in which a therapist can give instructions by presenting not only voice information but also body movements to a remote patient in a shared VR space. In this study, we conducted a user study to investigate the difference between the case where the therapist gives instructions to the patient in the same place and the case where the therapist gives the patient instructions remotely by using our system. As a result, it was suggested that our proposed system would improve the efficiency of movement transmission. This study is thought to solve the problem of insufficiency of therapists for phantom limb pain.",
    "keywords": [
      "phantom limb pain",
      "remote treatment system",
      "sense of body ownership",
      "telescoping",
      "virtual reality"
    ],
    "doi": "10.1145/3384657.3384795",
    "url": "https://doi.org/10.1145/3384657.3384795",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "16",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384801",
    "title": "KissGlass: Greeting Gesture Recognition using Smart Glasses",
    "authors": [
      "Richard Li",
      "Juyoung Lee",
      "Woontack Woo",
      "Thad Starner"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Cheek kissing is a common greeting in many countries around the world. Many parameters are involved when performing the kiss, such as which side to begin the kiss on and how many times the kiss is performed. These parameters can be used to infer one's social and physical context. In this paper, we present KissGlass, a system that leverages off-the-shelf smart glasses to recognize different kinds of cheek kissing gestures. Using a dataset we collected with 5 participants performing 10 gestures, our system obtains 83.0% accuracy in 10-fold cross validation and 74.33% accuracy in a leave-one-user-out user independent evaluation.",
    "keywords": [
      "gesture recognition",
      "greeting gestures",
      "smart eyewear",
      "smart glasses"
    ],
    "doi": "10.1145/3384657.3384801",
    "url": "https://doi.org/10.1145/3384657.3384801",
    "citations": 12,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "17",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384788",
    "title": "ExemPoser: Predicting Poses of Experts as Examples for Beginners in Climbing Using a Neural Network",
    "authors": [
      "Katsuhito Sasaki",
      "Keisuke Shiro",
      "Jun Rekimoto"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "It is important for beginners to imitate poses of experts in various sports; especially in sport climbing, performance depends greatly on the pose that should be taken for given holds. However, it is difficult for beginners to learn the proper poses for all patterns from experts since climbing holds are completely different for each course. Therefore, we propose a system that predict a pose of experts from the positions of the hands and feet of the climber--the positions of holds used by the climber--using a neural network. In other words, our system simulates what pose experts take for the holds the climber is now using. The positions of hands and feet are calculated from a image of the climber captured from behind. To allow users to check what pose is ideal in real time during practice, we have adopted a simple and lightweight network structure with little computational delay. We asked experts to compare the poses predicted by our system with the poses of beginners, and we confirmed that the poses predicted by our system were in most cases better than or as good as those of beginners.",
    "keywords": [
      "climbing",
      "neural networks",
      "sports technologies"
    ],
    "doi": "10.1145/3384657.3384788",
    "url": "https://doi.org/10.1145/3384657.3384788",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "18",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384790",
    "title": "waveSense: Low Power Voxel-tracking Technique for Resource Limited Devices",
    "authors": [
      "Anusha Withana",
      "Tharindu Kaluarachchi",
      "Chanaka Singhabahu",
      "Shanaka Ransiri",
      "Yilei Shi",
      "Suranga Nanayakkara"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this paper, we describe waveSense, a power-efficient optical voxel tracking system for hand tracking in resource-limited devices. waveSense uses off-the-shelf, low-cost, non-focused infrared (IR) sensors and emitters. To achieve power efficiency, we introduce a novel selective volumetric illumination (SVI) technique, which illuminates different areas of a scene with variable power IR light and captures the reflected energy using non-focused IR sensors. Our technical evaluation suggests the feasibility of using waveSense for hand tracking performed within 45cm radius of the device. We believe with waveSense, gesture-based interactions would become more practical on resource-constrained wearables such as head-mounted displays (HMDs) and smartwatches.",
    "keywords": [
      "Gesture Recognition",
      "Interacting with small devices",
      "Selective Volumetric Illumination",
      "Smart Wearables"
    ],
    "doi": "10.1145/3384657.3384790",
    "url": "https://doi.org/10.1145/3384657.3384790",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "19",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384779",
    "title": "The Jungle Warm-Up Run: Augmenting Athletes with Coach-Guided Dynamic Game Elements",
    "authors": [
      "Frederik Wiehr",
      "Marko Vujic",
      "Antonio Krüger",
      "Florian Daiber"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In virtually all sports, warming up is considered important to physically and mentally prepare an athlete for intensive efforts. General warm-up procedures with a correct technique for exercises are widely accepted to prevent injuries. However, most athletes do not warm up at all, or they do not follow a correct procedure. To address this, we designed and evaluated an exergame for warm-up guidance in a user-centered design process involving a fitness expert. The game augments the athlete with coach-guided dynamic game elements. With an online survey (N=466), we investigated general warm-up habits and provide insights on design game elements in this domain. In a between-subject user study (N=12), we compared our proposed exergame against a classic video instructor. As a result of using the exergame, the participants were more engaged (i.e. longer warm-up duration, higher relative intensity, more enjoyment) while reporting the same level of exertion.",
    "keywords": [
      "Augmented sports",
      "avatar",
      "exergames",
      "sports",
      "warm-up"
    ],
    "doi": "10.1145/3384657.3384779",
    "url": "https://doi.org/10.1145/3384657.3384779",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "20",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384782",
    "title": "Archery shots visualization by clustering and comparing from angular velocities of bows",
    "authors": [
      "Midori Kawaguchi",
      "Hironori Mitake",
      "Shoichi Hasegawa"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In individual competitions consisting of repetitive movement sports, it is necessary to increase the reproducibility of movements by recognizing and correcting movement changes per second. Since it is difficult to obtain sufficient awareness only by subjectivity, a mechanism that can objectively confirm the movement is required. In this paper, we propose a system that can easily search for differences in multiple trial motions by the same person for archery movements. The proposed system uses Dynamic Time Warping to determine the similarity of multiple shots of one competitor from the time-series data from the angular velocity sensor attached to the competitor's bow. Based on the similarity distance, K-means Clustering is performed. In addition, the video corresponding to the time at which there is a difference is cut out from the video recorded simultaneously to the sensor data, and the two images are superimposed and presented to visualize the difference. When the proposed system was tested with five intermediate- and advanced-level archers, it was possible to detect differences such as minor shaking, the posture, and the motion speed for approximately 0.5 seconds. These differences can be found by advanced-level archers by carefully comparing the videos for many times, but are difficult to identify by intermediate-level archers.Feedback from interviews with the instructor suggested that the differences detected were meaningful to find out the points for improve archery skill.",
    "keywords": [
      "Archery",
      "Clustering",
      "DBA",
      "Difference Extraction",
      "k-means"
    ],
    "doi": "10.1145/3384657.3384782",
    "url": "https://doi.org/10.1145/3384657.3384782",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "21",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384773",
    "title": "Design of Altered Cognition with Reshaped Bodies",
    "authors": [
      "Kenichiro Shirota",
      "Makoto Uju",
      "Yurike Chandra",
      "Elaine Czech",
      "Roshan Peiris",
      "Kouta Minamizawa"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this research, we aimed to expand humans' physical ability by reshaping natural physicalities that allow body transformation. By modifying the existing body parts, we proposed a system that changed humans' cognition of reality and their environment, and presented its use case. We conducted a study that explored the use of actions: (1) opening and closing of the pinnas (ears) and (2) pulling of the nose to change humans' body schema and humans' cognition of the environment. The results showed that people were able to perceive changes in the position of the pinnas in three different conditions (100% Open, 50% Open, 0% Open) with high accuracy. We also observed that opening and closing the pinnas could alter the cognition of the sound field, as opposed to the normal ears. Furthermore, we found that the feeling of a nose is extending was felt more by using the system. This finding implied that the modified body schema improved our recognition of the odor source and its position.",
    "keywords": [
      "Actuating human body",
      "Body Schema",
      "Embodied Cognition",
      "Human Augmentation",
      "Perception"
    ],
    "doi": "10.1145/3384657.3384773",
    "url": "https://doi.org/10.1145/3384657.3384773",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "22",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384799",
    "title": "Wearable Reasoner: Towards Enhanced Human Rationality Through A Wearable Device With An Explainable AI Assistant",
    "authors": [
      "Valdemar Danry",
      "Pat Pataranutaporn",
      "Yaoli Mao",
      "Pattie Maes"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Human judgments and decisions are prone to errors in reasoning caused by factors such as personal biases and external misinformation. We explore the possibility of enhanced reasoning by implementing a wearable AI system as a human symbiotic counterpart. We present \"Wearable Reasoner\", a proof-of-concept wearable system capable of analyzing if an argument is stated with supporting evidence or not. We explore the impact of argumentation mining and explainability of the AI feedback on the user through an experimental study of verbal statement evaluation tasks. The results demonstrate that the device with explainable feedback is effective in enhancing rationality by helping users differentiate between statements supported by evidence and without. When assisted by an AI system with explainable feedback, users significantly consider claims supported by evidence more reasonable and agree more with them compared to those without. Qualitative interviews demonstrate users' internal processes of reflection and integration of the new information in their judgment and decision making, emphasizing improved evaluation of presented arguments.",
    "keywords": [
      "Argumentation Mining",
      "Assisted Reasoning",
      "Augmented Humans",
      "Cognitive Enhancement",
      "Decision Making",
      "Explainable Artificial Intelligence",
      "Human-Machine Symbiosis",
      "Informal Logic",
      "Judgment",
      "Wearable Computing"
    ],
    "doi": "10.1145/3384657.3384799",
    "url": "https://doi.org/10.1145/3384657.3384799",
    "citations": 32,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "23",
    "numpages": "12",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/3384657.3384800",
    "title": "SpotlessMind: A Design Probe for Eliciting Attitudes towards Sharing Neurofeedback",
    "authors": [
      "Passant Elagroudy",
      "Xiyue Wang",
      "Evgeny Stemasov",
      "Teresa Hirzle",
      "Svetlana Shishkovets",
      "Siddharth Mehrotra",
      "Albrecht Schmidt"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Mutual understanding via sharing and interpreting inner states is socially rewarding. Prior research shows that people find Brain-Computer Interfaces (BCIs) a suitable tool to implicitly communicate their cognitive states. In this paper, we conduct an online survey (N=43) to identify design parameters for systems that implicitly share cognitive states. We achieve this by designing a research probe called \"SpotlessMind\" to artistically share brain occupancy with another while considering the bystanders' experience to elicit user responses. Our results show that 98% would like to see the installation. People would use it as a gesture of openness and as a communication mediator. Abstracting visual, auditory, and somatosensory depictions is a good trade-off between understandability and users' privacy protection. Our work supports designing engaging prototypes that promote empathy, cognitive awareness and convergence between individuals.",
    "keywords": [
      "Brain Occupancy",
      "Brain-Computer Interfaces",
      "Cognitive States",
      "Collaborative Art",
      "Design Framework",
      "EEG",
      "Installation"
    ],
    "doi": "10.1145/3384657.3384800",
    "url": "https://doi.org/10.1145/3384657.3384800",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "24",
    "numpages": "8",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384798",
    "title": "Facilitating Experiential Knowledge Sharing through Situated Conversations",
    "authors": [
      "Ryo Fujikura",
      "Yasuyuki Sumi"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "This paper proposes a system that facilitates knowledge sharing among people in similar situations by providing audio of past conversations. Our system records all voices of conversations among the users in the specific fields such as tourist spots, museums, digital fabrication studio, etc. and then timely provides users in a similar situation with fragments of the accumulated conversations. For segmenting and retrieving past conversation from vast amounts of captured data, we focus on non-verbal contextual information, i.e., location, attention targets, and hand operations of the conversation participants. All voices of conversation are recorded, without any selection or classification. The delivery of the voices to a user is determined not based on the content of the conversation but on the similarity of situations between the conversation participants and the user. To demonstrate the concept of the proposed system, we performed a series of experiments to observe changes in user behavior due to past conversations related to the situation at the digital fabrication workshop. Since we have not achieved a satisfactory implementation to sense user's situation, we used Wizard of Oz (WOZ) method. That is, the experimenter visually judges the change in the situation of the user and inputs it to the system, and the system automatically provides the users with voices of past conversation corresponding to the situation. Experimental results show that most of the conversations presented when the situation perfectly matches is related to the user's situation, and some of them prompts the user to change their behavior effectively. Interestingly, we could observe that conversations that were done in the same area but not related to the current task also had the effect of expanding the user's knowledge. We also observed a case that although a conversation highly related to the user's situation was timely presented but the user could not utilize the knowledge to solve the problem of the current task. It shows the limitation of our system, i.e., even if a knowledgeable conversation is timely provided, it is useless unless it fits with the user's knowledge level.",
    "keywords": [
      "conversation",
      "experience sharing",
      "situation"
    ],
    "doi": "10.1145/3384657.3384798",
    "url": "https://doi.org/10.1145/3384657.3384798",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "25",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384778",
    "title": "VersaTouch: A Versatile Plug-and-Play System that Enables Touch Interactions on Everyday Passive Surfaces",
    "authors": [
      "Yilei Shi",
      "Haimo Zhang",
      "Jiashuo Cao",
      "Suranga Nanayakkara"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "We present VersaTouch, a portable, plug-and-play system that uses active acoustic sensing to track fine-grained touch locations as well as touch force of multiple fingers on everyday surfaces without having to permanently instrument them or do extensive calibration. Our system is versatile in multiple aspects. First, with simple calibration, VersaTouch can be arranged in arbitrary layouts in order to fit into crowded surfaces while retaining its accuracy. Second, various modalities of touch input, such as distance and position, can be supported depending on the number of sensors used to suit the interaction scenario. Third, VersaTouch can sense multi-finger touch, touch force, as well as identify the touch source. Last, VersaTouch is capable of providing vibrotactile feedback to fingertips through the same actuators used for touch sensing. We conducted a series of studies and demonstrated that VersaTouch was able to track finger touch using various layouts with average error from 9.62mm to 14.25mm on different surfaces within a circular area of 400mm diameter centred around the sensors, as well as detect touch force. Finally, we discuss the interaction design space and interaction techniques enabled by VersaTouch.",
    "keywords": [
      "Acoustic",
      "Finger Touch",
      "Force",
      "Localization"
    ],
    "doi": "10.1145/3384657.3384778",
    "url": "https://doi.org/10.1145/3384657.3384778",
    "citations": 17,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "26",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384797",
    "title": "WristLens: Enabling Single-Handed Surface Gesture Interaction for Wrist-Worn Devices Using Optical Motion Sensor",
    "authors": [
      "Hui-Shyong Yeo",
      "Juyoung Lee",
      "Andrea Bianchi",
      "Alejandro Samboy",
      "Hideki Koike",
      "Woontack Woo",
      "Aaron Quigley"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "WristLens is a system for surface interaction from wrist-worn wearable devices such as smartwatches and fitness trackers. It enables eyes-free, single-handed gestures on surfaces, using an optical motion sensor embedded in a wrist-strap. This allows the user to leverage any proximate surface, including their own body, for input and interaction. An experimental study was conducted to measure the performance of gesture interaction on three different body parts. Our results show that directional gestures are accurately recognized but less so for shape gestures. Finally, we explore the interaction design space enabled by WristLens, and demonstrate novel use cases and applications, such as on-body interaction, bimanual interaction, cursor control and 3D measurement.",
    "keywords": [
      "Single-handed",
      "gesture",
      "on-body interaction",
      "smartwatch"
    ],
    "doi": "10.1145/3384657.3384797",
    "url": "https://doi.org/10.1145/3384657.3384797",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "27",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384789",
    "title": "PDMSkin: On-Skin Gestures with Printable Ultra-Stretchable Soft Electronic Second Skin",
    "authors": [
      "Tobias Röddiger",
      "Michael Beigl",
      "Daniel Wolffram",
      "Matthias Budde",
      "Hongye Sun"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Innovative enabling technologies are key drivers of human augmentation. In this paper, we explore a new, conductive, and configurable material made from Polydimethylsiloxane (PDMS) that is capillary doped with silver particles (Ag) using an immiscible secondary fluid to build ultra-stretchable, soft electronics. Bonding silver particles directly with PDMS enables inherently stretchable Ag-PDMS circuits. Compared to previous work, the reduced silver consumption creates significant advantages, e.g., better stretchability and lower costs. The secondary fluid ensures self-assembling conductivity networks. Sensors are 3D-printed ultra-thin (&lt;100μm) onto a pure PDMS substrate in one step and only require a PDMS cover-layer. They exhibit almost stable electrical properties even for an intense stretching of &gt;200%. Therefore, printed circuits can attach tightly onto the body. Due to biocompatibility, devices can be implanted (e.g., open wounds treatment). We present a proof of concept on-skin interface that uses the new material to provide six distinct input gestures. Our quantitative evaluation with ten participants shows that we can successfully classify the gestures with a low spatial-resolution circuit. With few training data and a gradient boosting classifier, we yield 83% overall accuracy. Our qualitative material study with twelve participants shows that usability and comfort are well perceived; however, the smooth but easy to adapt surface does not feel tissue-equivalent. For future work, the new material will likely serve to build robust and skin-like electronics.",
    "keywords": [
      "novel material",
      "skin interface",
      "strain sensing"
    ],
    "doi": "10.1145/3384657.3384789",
    "url": "https://doi.org/10.1145/3384657.3384789",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "28",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384774",
    "title": "Sketching On-Body Interactions using Piezo-Resistive Kinesiology Tape",
    "authors": [
      "Paul Strohmeier",
      "Narjes Pourjafarian",
      "Marion Koelle",
      "Cedric Honnet",
      "Bruno Fruchard",
      "Jürgen Steimle"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Skin is personal and sensitive. As a result, design and placement of on-body physical interfaces need to be well thought out. One way of \"getting the design right\" is to quickly sketch a multitude of designs to be modified, adjusted and elaborated on. To date, on-body rapid prototyping methods do not afford these \"quick-and-dirty\" design processes. We propose using piezo-resistive kinesiology tape as a low-cost and versatile resource for sketching functional on-skin interfaces. Our method uses pretreated kinesiology tape, which is made piezo-resistive through polymerization, and serves as touch, pressure and stretch sensor. We illustrate ketching techniques with both pretreated and untreated tape for iterative design of on-skin interfaces. In addition, we contribute a set of sensor primitives that facilitate various input modalities for creating interactive sketches.",
    "keywords": [
      "E-textile",
      "Epidermal devices",
      "Fabrication",
      "On-Body interactions",
      "Rapid prototyping",
      "Sensors",
      "Wearables"
    ],
    "doi": "10.1145/3384657.3384774",
    "url": "https://doi.org/10.1145/3384657.3384774",
    "citations": 15,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "29",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3384657.3384783",
    "title": "Living Bits: Opportunities and Challenges for Integrating Living Microorganisms in Human-Computer Interaction",
    "authors": [
      "Pat Pataranutaporn",
      "Angela Vujic",
      "David S. Kong",
      "Pattie Maes",
      "Misha Sra"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "There are trillions of living biological \"computers\" on, inside, and around the human body: microbes. Microbes have the potential to enhance human-computer interaction (HCI) in entirely new ways. Advances in open-source biotechnology have already enabled designers, artists, and engineers to use microbes in redefining wearables, games, musical instruments, robots, and more. \"Living Bits\", inspired by Tangible Bits, is an attempt to think beyond the traditional boundaries that exist between biological cells and computers for integrating microorganism in HCI. In this work we: 1) outline and inspire the possibility for integrating organic and regenerative living systems in HCI; 2) explore and characterize human-microbe interactions across contexts and scales; 3) provide principles for stimulating discussions, presentations, and brainstorms of microbial interfaces. We aim to make Living Bits accessible to researchers across HCI, synthetic biology, biotechnology, and interaction design to explore the next generation of biological HCI.",
    "keywords": [
      "Biological Interfaces",
      "Biotechnology",
      "Living Interfaces",
      "Microorganism",
      "Synthetic Biology"
    ],
    "doi": "10.1145/3384657.3384783",
    "url": "https://doi.org/10.1145/3384657.3384783",
    "citations": 60,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "30",
    "numpages": "12",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/3384657.3385331",
    "title": "GymSoles++: Using Smart Wearbales to Improve Body Posture when Performing Squats and Dead-Lifts",
    "authors": [
      "Don Samitha Elvitigala",
      "Denys Matthies",
      "Chamod Weerasinghe",
      "Yilei Shi",
      "Suranga Nanayakkara"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Squats and dead-lifts are considered two important full-body exercises for beginners, which can be performed at home or the gymnasium. During the execution of these exercises, it is essential to maintain the correct body posture to avoid injuries. In this paper, we demonstrate an unobtrusive sensing approach, an insole-based wearable system that also provides feedback on the user's centre of pressure (CoP) via vibrotactile and visual aids. Solely visualizing the CoP can significantly improve body posture and thus effectively assist users when performing squats and dead-lifts. We explored different feedback modalities and conclude that a vibrotactile insole is a practical and effective solution.",
    "keywords": [
      "Center of Pressure",
      "Dead-lifts",
      "Google Glass",
      "Improving Body Posture",
      "Sensory Extrimities",
      "Smart Insoles",
      "Squats",
      "Vibrotactile Feedback",
      "Vibrotactile Insole",
      "Visual Feedback"
    ],
    "doi": "10.1145/3384657.3385331",
    "url": "https://doi.org/10.1145/3384657.3385331",
    "citations": 10,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "31",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385328",
    "title": "EgoSpace: Augmenting Egocentric Space by Wearable Projector",
    "authors": [
      "Yuya Adachi",
      "Haoran Xie",
      "Takuma Torii",
      "Haopeng Zhang",
      "Ryo Sagisaka"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this work, we propose a novel wearable device to augment the user's egocentric space to a wide range. To achieve this goal, the proposed device provides bidirectional projection using a head-mounted wearable projector and two dihedral mirrors. The included angle of the mirrors were set to reflect the projected image in front of and behind the user. A prototype system is developed to explore possible applications using the proposed device in different scenarios, such as riding a bike and map navigation.",
    "keywords": [
      "Wearable device",
      "interactive projection",
      "navigation",
      "personal space"
    ],
    "doi": "10.1145/3384657.3385328",
    "url": "https://doi.org/10.1145/3384657.3385328",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "32",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385329",
    "title": "Towards A Wearable for Deep Water Blackout Prevention",
    "authors": [
      "Frederik Wiehr",
      "Andreas Höh",
      "Antonio Krüger"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Freediving relies on a diver's ability to hold his breath until resurfacing. Many fatal accidents in freediving are caused by a sudden blackout of the diver right before resurfacing. In this work, we propose a wearable prototype for monitoring oxygen saturation underwater and conceptualize an early warning system with regard to the diving depth. Our predictive algorithm estimates the latest point of return in order to emerge with a sufficient oxygen level to prevent a blackout and notifies the diver via an acoustic signal.",
    "keywords": [
      "blackout",
      "freediving",
      "oxygen saturation",
      "sports",
      "wearable"
    ],
    "doi": "10.1145/3384657.3385329",
    "url": "https://doi.org/10.1145/3384657.3385329",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "33",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385330",
    "title": "High-speed Projection Method of Swing Plane for Golf Training",
    "authors": [
      "Tomohiro Sueishi",
      "Chikara Miyaji",
      "Masataka Narumiya",
      "Yuji Yamakawa",
      "Masatoshi Ishikawa"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "Display technologies that show dynamic information such as club swing motion are useful for golf training, but conventional methods have a large latency from sensing the motion to displaying them for users. In this study, we propose an immediate, high-speed projection method of swing plane geometric information onto the ground during the swing. The method utilizes marker-based clubhead posture estimation and a mirror-based high-speed tracking system. The intersection line with the ground, which is the geometric information of the swing plane, is immediately cast by a high-speed projector. We have experimentally confirmed the sufficiently low latency of the projection itself for swing motions and have demonstrated the temporal convergence and predictive display of the swing plane line projection around the bottom of the swing motion.",
    "keywords": [
      "high-speed image processing",
      "high-speed projection",
      "projection-based augmented reality",
      "sports training"
    ],
    "doi": "10.1145/3384657.3385330",
    "url": "https://doi.org/10.1145/3384657.3385330",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "34",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385334",
    "title": "Augmented Workplace: Human-Sensor Interaction for Improving the Work Environment",
    "authors": [
      "Yutaka Arakawa"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this paper, we proposed and implemented an augmented workplace where humans and deployed sensors have some interaction. Through the experiment, CO2 sensor asks humans to open the window when CO2 level exceeds the threshold, we found that a human has obeyed a message from a sensor almost every time.",
    "keywords": [
      "behavior change",
      "human-sensor interaction",
      "workplace"
    ],
    "doi": "10.1145/3384657.3385334",
    "url": "https://doi.org/10.1145/3384657.3385334",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "35",
    "numpages": "2",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385332",
    "title": "e2-MaskZ: a Mask-type Display with Facial Expression Identification using Embedded Photo Reflective Sensors",
    "authors": [
      "Akino Umezawa",
      "Yoshinari Takegawa",
      "Katsuhiro Suzuki",
      "Katsutoshi Masai",
      "Yuta Sugiura",
      "Maki Sugimoto",
      "Yutaka Tokuda",
      "Diego Martinez Plasencia",
      "Sriram Subramanian",
      "Masafumi Takahashi",
      "Hiroaki Taka",
      "Keiji Hirata"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "The goal of this research is to propose the e2-MaskZ, a mask-type display that changes the user's face to the face of an avatar. The e2-MaskZ is composed of a face-capture mask to recognize the facial expression, and a face-display mask to present the avatar that reflects the recognize expression of the system wearer. 40 photo reflective sensors are laid out across the entire surface of the face-capture mask, and the e2-Mask is made to learn the sensor data for each new facial expression.",
    "keywords": [
      "Augmented Face",
      "Communication",
      "Human Agent Interaction"
    ],
    "doi": "10.1145/3384657.3385332",
    "url": "https://doi.org/10.1145/3384657.3385332",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "36",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3385333",
    "title": "Understanding Face Gestures with a User-Centered Approach Using Personal Computer Applications as an Example",
    "authors": [
      "Yenchin Lai",
      "Benjamin Tag",
      "Kai Kunze",
      "Rainer Malaka"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "While face gesture input has been proposed by researchers, the issue of practical gestures remains unsolved. We present the first comprehensive investigation of user-defined face gestures as an augmented input modality. Based on a focus group discussion, we developed three sets of tasks, where we asked participants to spontaneously produce face gestures to complete these tasks. We report our findings of a user study and discuss the user preference of face gestures. The results inform the development of future interaction systems utilizing face gestures.",
    "keywords": [
      "Face gesture",
      "elicitation study",
      "user-centered",
      "user-defined gesture"
    ],
    "doi": "10.1145/3384657.3385333",
    "url": "https://doi.org/10.1145/3384657.3385333",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "37",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3384657.3384776",
    "title": "Conformal Wearable Devices for Expressive On-Skin Interaction",
    "authors": [
      "Aditya Shekhar Nittala",
      "Arshad Khan",
      "Jürgen Steimle"
    ],
    "year": 2020,
    "conference": "AHS",
    "conferenceYear": "AHs '20",
    "abstract": "In this demonstration we showcase our recent work on conformal wearable devices that can enable expressive interaction on skin. Our interactive exhibits demonstrate a variety of functionalities including ultrathin wearable devices for high-resolution touch input, expressive interaction on body landmarks, visual output, and physiological sensing. Our work furthermore demonstrates fabrication techniques to offer customization and personalization of epidermal devices, ultimately providing an intimate coupling with the human body.",
    "keywords": [
      "E-textile",
      "Electronic Skin",
      "Fabrication",
      "Ink-jet Printing",
      "Physiological sensing",
      "Rapid Prototyping",
      "Wearable Devices"
    ],
    "doi": "10.1145/3384657.3384776",
    "url": "https://doi.org/10.1145/3384657.3384776",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference",
    "pages": "",
    "publisher": "Association for Computing Machinery",
    "location": "Kaiserslautern, Germany",
    "articleno": "38",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458878",
    "title": "MultiSoma: Distributed Embodiment with Synchronized Behavior and Perception",
    "authors": [
      "Reiji Miura",
      "Shunichi Kasahara",
      "Michiteru Kitazaki",
      "Adrien Verhulst",
      "Masahiko Inami",
      "Maki Sugimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Human behavior and perception are optimized for a single body. Yet, the human brain has plasticity, which allows us to extend our body schema. By utilizing technology like robotics or virtual reality (VR) , we can modify our body parts or even add a new body to our own while retaining control over these parts. However, the update of body cognition when controlling multiple bodies has not been well examined. In this study, we explore the task performance and body cognition of humans when they have multiple full bodies as an extended embodiment. Our experimental system allows a participant to control up to four bodies at the same time and perceive sensory information from them. The participant experiences synchronizing behavior and vision perception in a virtual environment. We set up two tasks for multiple bodies and evaluated the cognition of these bodies by their task performances and subjective ratings. We found that humans can have the sense of body ownership and agency for each body when controlling multiple bodies simultaneously. Distributed embodiment has the potential to extend human behavior in cooperative work, parallel work, group behavior, and so on.",
    "keywords": [
      "virtual reality",
      "multiple bodies",
      "embodiment",
      "augmented human"
    ],
    "doi": "10.1145/3458709.3458878",
    "url": "https://doi.org/10.1145/3458709.3458878",
    "citations": 23,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "1–9",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3458709.3458932",
    "title": "Dynamic Shared Limbs: An Adaptive Shared Body Control Method Using EMG Sensors",
    "authors": [
      "Ryo Takizawa",
      "Takayoshi Hagiwara",
      "Adrien Verhulst",
      "Masaaki Fukuoka",
      "Michiteru Kitazaki",
      "Maki Sugimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "An inter-personal shared body is an extended body that allows an avatar or robot to be controlled by multiple people. It is possible for operators to share control over the actions of the shared body. A shared body can be operated well when the same actions are performed by its operators. However, when their actions are not the same, we need to have an adaptive control method for the operators to maintain a sense of agency and ownership. In this study, we propose dynamic adjustment methods to control a shared body using EMG sensors and movements of the operators.",
    "keywords": [
      "virtual reality",
      "shared body",
      "human augmentation",
      "EMG sensors"
    ],
    "doi": "10.1145/3458709.3458932",
    "url": "https://doi.org/10.1145/3458709.3458932",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "10–18",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458980",
    "title": "Independent Control of Supernumerary Appendages Exploiting Upper Limb Redundancy",
    "authors": [
      "Hideki Shimobayashi",
      "Tomoya Sasaki",
      "Arata Horie",
      "Riku Arakawa",
      "Zendai Kashino",
      "Masahiko Inami"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In the field of physical augmentation, researchers have attempted to extend human capabilities by expanding the number of human appendages. To fully realize the potential of having an additional appendage, supernumerary appendages should be independently controllable without interfering with the functionality of existing appendages. Herein, we propose a novel approach for controlling supernumerary appendages by exploiting upper limb redundancy. We present a headphone-style visual sensing device and a recognition system to estimate shoulder movement. Through a set of user experiments, we evaluate the feasibility of our system and reveal the potential of independent control using upper limb redundancy. Our results indicate that participants are able to intentionally give commands through their shoulder motions. Finally, we demonstrate the wide range of supernumerary appendage control applications that our novel approach enables and discuss future prospects for our work.",
    "keywords": [
      "Wearable Sensing",
      "Supernumerary Robotic Limbs",
      "Supernumerary Appendages",
      "Independent Control",
      "Human Body Redundancy"
    ],
    "doi": "10.1145/3458709.3458980",
    "url": "https://doi.org/10.1145/3458709.3458980",
    "citations": 15,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "19–30",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458981",
    "title": "Research on the transcendence of bodily differences, using sport and human augmentation medium",
    "authors": [
      "Ryoichi Ando",
      "Isao Uebayashi",
      "Hayato Sato",
      "Hayato Ohbayashi",
      "Shota Katagiri",
      "Shuhei Hayakawa",
      "Kouta Minamizawa"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In this study, we created a sport which applied with human augmentation technology, that is not affected by the differences in the participants’ lower limbs, using a wheelchair-type body-enhancing medium that provides a new motor ability called drifting that was not possible before. The sport was played by six players, three people with non lower limb disabilities and three without lower limb disabilities, and the results showed a change in the added ability especially for their motions’ accuracy. Some of these were not related to the differential status of the lower limb, while others may have been.",
    "keywords": [
      "Superhuman Sports",
      "Physical Differences",
      "Human Augmentation"
    ],
    "doi": "10.1145/3458709.3458981",
    "url": "https://doi.org/10.1145/3458709.3458981",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "31–39",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458935",
    "title": "Ubiquitous Body: Effect of Spatial Arrangement of Task's View on Managing Multiple Tasks",
    "authors": [
      "Yukiko Iwasaki",
      "Hiroyasu Iwata"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This research proposes a concept of the \"ubiquitous body system\", a new concept of using multiple body parts at various places simultaneously. This system enables users to access any number of the bodies at a time, and make physical intervention to any locations regardless of the current location where user's natural body is. However, when the number of ubiquitous bodies becomes larger, it becomes more difficult to see or manage all of those tasks.In this short paper, we focused on the effect of spatial arrangement of the vision information on managing multiple tasks. We developed a vision presenting system in which the environment images of each ubiquitous body are arranged in the same way as the positional relationships of ubiquitous bodies in the real world. The result of a case study regarding usability of this prototype suggested that presenting visual information with same spatial relationships as real would be effective to switch user's attention and manage multiple tasks.",
    "keywords": [
      "Virtual Reality",
      "Interface",
      "Human-Robot Interaction",
      "Human Augmentation"
    ],
    "doi": "10.1145/3458709.3458935",
    "url": "https://doi.org/10.1145/3458709.3458935",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "40–44",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458982",
    "title": "Deep Learning–Based Scene Simplification for Bionic Vision",
    "authors": [
      "Nicole Han",
      "Sudhanshu Srivastava",
      "Aiwen Xu",
      "Devi Klein",
      "Michael Beyeler"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Retinal degenerative diseases cause profound visual impairment in more than 10 million people worldwide, and retinal prostheses are being developed to restore vision to these individuals. Analogous to cochlear implants, these devices electrically stimulate surviving retinal cells to evoke visual percepts (phosphenes). However, the quality of current prosthetic vision is still rudimentary. Rather than aiming to restore “natural” vision, there is potential merit in borrowing state-of-the-art computer vision algorithms as image processing techniques to maximize the usefulness of prosthetic vision. Here we combine deep learning–based scene simplification strategies with a psychophysically validated computational model of the retina to generate realistic predictions of simulated prosthetic vision, and measure their ability to support scene understanding of sighted subjects (virtual patients) in a variety of outdoor scenarios. We show that object segmentation may better support scene understanding than models based on visual saliency and monocular depth estimation. In addition, we highlight the importance of basing theoretical predictions on biologically realistic models of phosphene shape. Overall, this work has the potential to drastically improve the utility of prosthetic vision for people blinded from retinal degenerative diseases.",
    "keywords": [
      "visually impaired",
      "vision augmentation",
      "simulated prosthetic vision",
      "scene simplification",
      "retinal implant",
      "deep learning"
    ],
    "doi": "10.1145/3458709.3458982",
    "url": "https://doi.org/10.1145/3458709.3458982",
    "citations": 37,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "45–54",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3458709.3458983",
    "title": "FaceRecGlasses: A Wearable System for Recognizing Self Facial Expressions Using Compact Wearable Cameras",
    "authors": [
      "Hiroaki Aoki",
      "Ayumi Ohnishi",
      "Naoya Isoyama",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Facial expression images are useful for life-logging because they represent one’s emotions and mental state. However, using a wearable system to capture facial expressions from the front of the user obstructs the field of view. In this paper, we propose FaceRecGlasses, a compact wearable system that constantly records the user’s face and surroundings as images and indexes them with emotions without obstructing the user’s daily life. Our eyeglass-shaped device comprises two compact cameras and three mirrors, which enable the user’s facial expressions and surroundings to be captured constantly. The proposed system outputs a real-time pseudo-face-image by combining the captured facial components with a stored base-face-image. We investigated suitable facial areas to be captured by the proposed system and evaluated the recognition accuracy of the facial expressions captured. The results confirmed accuracies of 87.5%, 66.7%, and 71.4% for neutral emotion, happiness, and surprise, respectively, indicating that this method reproduces these three facial expressions to a high degree and can be applied to indexing of facial images.",
    "keywords": [
      "Wearable Computing",
      "Facial Expression Reconstruction",
      "Eyewear Computing",
      "Affective Computing"
    ],
    "doi": "10.1145/3458709.3458983",
    "url": "https://doi.org/10.1145/3458709.3458983",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "55–65",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458938",
    "title": "CircadianVisor: Image Presentation with an Optical See-Through Display in Consideration of Circadian Illuminance",
    "authors": [
      "Takumi Tochimoto",
      "Yuichi Hiroi",
      "Yuta Itoh"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In modern society, the impact of nighttime artificial lighting on the human sleep/wake cycle (the circadian rhythm), has long been an important issue. In augmented reality, such health hazards should be prevented if we are to become a society that wears optical see-through head-mounted displays (OST-HMDs) on a daily basis. We present CircadianVisor, an OST display system that controls circadian performance. Our system combines an OST-HMD with a liquid crystal (LC) shutter and a spectrometer to control the circadian illuminance (CIL, biolux) of light incident on the user’s eyes. To prevent the CIL at the eyes from exceeding the threshold, correct for the displayed image based on RGB values, and adjust the transmittance of the LC visor to pass through the environment light based on spectral measurements, we build a proof-of-concept system to evaluate the feasibility of the system’s CIL control and test it with a spectrometer installed at the user’s viewpoint. The evaluation shows that the CIL at the user’s viewpoint can be kept below the threshold.",
    "keywords": [
      "Head-mounted displays",
      "Circadian rhythm",
      "Blue-light-blocking glasses",
      "Augmented reality"
    ],
    "doi": "10.1145/3458709.3458938",
    "url": "https://doi.org/10.1145/3458709.3458938",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "66–76",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458984",
    "title": "Advantage and Misuse of Vision Augmentation – Exploring User Perceptions and Attitudes using a Zoom Prototype",
    "authors": [
      "Chloe Eghtebas",
      "Francisco Kiss",
      "Marion Koelle",
      "Paweł Woźniak"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Consequences that deter adoption, such as asymmetrical encounters between wearers and bystanders, need to be explored in order to make Ubiquitous Augmented Reality (UAR) acceptable. In our work we outline how social perception is based on a Head Mounted Displays (HMD) capability, appearance, and the role of the wearer. We fixed the device capability to zooming in AR and explored the privacy implications in 12 interviews through a prototype with the mocked ability to ”super humanly” zoom in on targets. Next, we used the resulting themes to survey 100 participants to deeper explore augmented zoom while we permutate on the device appearance housed in three form-factors: contact lenses, glasses, and helmet and role of wearer based on level of involvement in an abstracted scenario transpiring in a public space. Our results showed that explicit visibility of an AR system provides social translucence as it is rated least likely to cause misuse but also perceived as least likely to have an advantage.",
    "keywords": [
      "Zoom Interaction",
      "Privacy Implications",
      "Mixed Method",
      "Augmented Reality"
    ],
    "doi": "10.1145/3458709.3458984",
    "url": "https://doi.org/10.1145/3458709.3458984",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "77–85",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458985",
    "title": "SilentMask: Mask-type Silent Speech Interface with Measurement of Mouth Movement",
    "authors": [
      "Hirotaka Hiraki",
      "Jun Rekimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Silent Speech Interaction (SSI) is a non-speech interaction used as an input method for speech recognition devices such as smartphones and as a support tool for people with speech difficulties. Conventional SSI methods using lip reading, electromyography(EMG), ultrasonic echo, and electrostatic positioning in the palate have been proposed, but there have been issues such as not being able to use one hand and being easily noticeable. In this study, we propose a mask-based SSI that recognizes silent speech by measuring the motion around the mouth using acceleration and angular velocity sensors attached to mask. Using two acceleration and angular velocity sensors to acquire 12-dimensional motion information around the mouth and analyzing it using deep learning, we were able to identify a total of 22 states (21 types of voice commands and no speech) with 79.9% accuracy. The results also showed that the device can be worn for a longer period of time compared to the method of applying the sensors directly to the skin. This research presents new possibilities for masks, as they are a non-contact, unobtrusive interface that does not use camera images and is therefore independent of lighting conditions.",
    "keywords": [
      "Wearble Device",
      "Silent Speech Interface",
      "Mask"
    ],
    "doi": "10.1145/3458709.3458985",
    "url": "https://doi.org/10.1145/3458709.3458985",
    "citations": 17,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "86–90",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3458709.3458941",
    "title": "Derma: Silent Speech Interaction Using Transcutaneous Motion Sensing",
    "authors": [
      "Jun Rekimoto",
      "Yu Nishimura"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Silent speech interaction (SSI) enables speech communication without uttering an actual voice and can have a potential to make speech interaction available in public places. However, commonly studied image-based lip-reading for SSI requires a camera in front of the face and thus it is not suitable for mobile use. Ultrasound imaging requires expensive and complex equipment. In contrast, we propose a much simpler method by skin motion sensing. Two small 6-DOF accelerometer/angular velocity sensors attached under the chin acquire 12-dimensional multidimensional information of skin motion caused by the silent utterance. With neural networks, 35 different silent commands are identified with a recognition rate of 94%. While previous lip-reading studies have normally inferred speech from images of video with vocal speech, this study also proposes a method of learning entirely from non-vocal speech only. Compared to previous studies, we consider the proposed solution is less visible, lightweight, and is not affected by lighting conditions.",
    "keywords": [
      "skin motion sensing",
      "silent speech interaction",
      "neural networks"
    ],
    "doi": "10.1145/3458709.3458941",
    "url": "https://doi.org/10.1145/3458709.3458941",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "91–100",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3458709.3458942",
    "title": "Conversational Partner’s Perception of Subtle Display Use for Monitoring Notifications",
    "authors": [
      "Jacob Logas",
      "Georgianna Lin",
      "Kelsie Belan",
      "Advait Gogate",
      "Thad Starner"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We examine whether the gaze direction of a user reveals the use of a subtle display during a face-to-face conversation with a partner who is not initially aware of the display. We measure twelve participants’ perceptions of a casual conversational partner’s engagement between a control condition of no notification and notifications displayed behind the participant’s head at 0, 10, and 20 degrees to the right of the conversational partner’s line of sight. No differences in reported conversational engagement were found. However, once the presence of the display was revealed, engagement scores went down over all conditions compared to the prior uninformed variant of the experiment. Still, no difference was found between the control and the subtle display conditions, and informed participants were only 40% accurate on average in detecting the use of the display. In a second study comparing subtle display user engagement with smartwatch user engagement, six participants rated a conversational partner more distracted when the partner used a smartwatch to monitor notifications than when the partner used a display secretly mounted behind the participant’s head. Participants in both studies did not realize the presence of the display until it was revealed. These results suggest that eye movement when using a subtle display detracts less from the conversational experience than the use of a smartwatch.",
    "keywords": [
      "wearable",
      "user studies",
      "head-up display",
      "evaluation methodology"
    ],
    "doi": "10.1145/3458709.3458942",
    "url": "https://doi.org/10.1145/3458709.3458942",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "101–110",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458943",
    "title": "Detecting Episodes of Increased Cough Using Kinetic Earables",
    "authors": [
      "Tobias Röddiger",
      "Michael Beigl",
      "Michael Hefenbrock",
      "Daniel Wolffram",
      "Erik Pescara"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper introduces the detection of episodes of increased cough (e.g. during illness) based on cough event classification using kinetic earables. In a twelve subject study, we collected voluntary weak and strong cough as well as five non-cough activities (e.g., talking) under various conditions (e.g., walking). During the activities, an in-ear worn sensor records acceleration and gyroscope data. In total, we collected 4,200 activity samples. A single step classification pipeline (0.77 overall accuracy) serves as the foundation for statistical analysis to achieve episodes of increased cough discrimination. As a digression, we reverse data and perform pose classification which could enable faster cough episode prediction. All-in-all, earables might help to objectify illness to encourage formal diagnosis.",
    "keywords": [
      "illness detection",
      "earable",
      "cough",
      "airway events",
      "activity recognition"
    ],
    "doi": "10.1145/3458709.3458943",
    "url": "https://doi.org/10.1145/3458709.3458943",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "111–115",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458986",
    "title": "Portable 3D Human Pose Estimation for Human-Human Interaction using a Chest-Mounted Fisheye Camera",
    "authors": [
      "Kohei Aso",
      "Dong-Hyun Hwang",
      "Hideki Koike"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We propose a system that estimates the 3D body pose of other parties using a single RGB chest-mounted ultra-wide fisheye camera. Although the fisheye camera can capture a wide field of view, it is difficult to apply image processing for perspective images because of its strong distortion. In our method, the input fisheye image is converted to an equirectangular image to detect another person and their 2D keypoints, and then convert them to a 3D pose. In order to adapt to the distortion of equirectangular images, we generate a synthetic dataset and fine-tune the model. We also estimate the location of the other person so that we can reconstruct the absolute camera-centered global pose. We evaluate the accuracy on real-world data and show that the fine-tuned model performs best.",
    "keywords": [
      "Mobile motion capture",
      "Human-human interaction",
      "Human pose estimation",
      "Fisheye camera",
      "Egocentric video",
      "Computer vision"
    ],
    "doi": "10.1145/3458709.3458986",
    "url": "https://doi.org/10.1145/3458709.3458986",
    "citations": 28,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "116–120",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3458709.3458945",
    "title": "CapGlasses: Untethered Capacitive Sensing with Smart Glasses",
    "authors": [
      "Denys J.C. Matthies",
      "Chamod Weerasinghe",
      "Bodo Urban",
      "Suranga Nanayakkara"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Augmenting the human body using wearable technology can be particularly interesting to sense context. The user’s context includes the mental and physical state, which is inferable by detecting facial and head related gestures. For the recognition of these gestures, we propose instrumenting a pair of glasses with Capacitive Sensing (CapSense) technology. We demonstrate proximity sensing with CapSense for mobile use despite its commonly known limitations in context of mobility. Moreover, we demonstrate how to incorporate transparent sensing electrodes into the glass and copper electrodes into the frame while being potentially invisible in a future specs product. We demonstrate an untethered battery-powered glasses prototype, CapGlasses, to sense facial expressions and head gestures. We selected a set of 12 gestures and ran a study with 12 users. We obtained an average accuracy of 89.6% by a user-dependent machine learning model. We focused on providing clear documentation to enable a straightforward replication of our technology.",
    "keywords": [
      "Wearable Computing",
      "Smart Glasses",
      "Prototyping",
      "Machine Learning",
      "Facial Expression Control",
      "Electric Field Sensing",
      "Data Mining",
      "Capacitive Sensing",
      "CapSense",
      "Body Potential Sensing",
      "Activity Recognition"
    ],
    "doi": "10.1145/3458709.3458945",
    "url": "https://doi.org/10.1145/3458709.3458945",
    "citations": 20,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "121–130",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 3
  },
  {
    "id": "10.1145/3458709.3458946",
    "title": "Exploratory Design of a Hands-free Video Game Controller for a Quadriplegic Individual",
    "authors": [
      "Atieh Taheri",
      "Ziv Weissman",
      "Misha Sra"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "From colored pixels to hyper-realistic 3D landscapes of virtual reality, video games have evolved immensely over the last few decades. However, video game input still requires two-handed dexterous finger manipulations for simultaneous joystick and trigger or mouse and keyboard presses. In this work, we explore the design of a hands-free game control method using realtime facial expression recognition for individuals with neurological and neuromuscular diseases who are unable to use traditional game controllers. Similar to other Assistive Technologies (AT), our facial input technique is also designed and tested in collaboration with a graduate student who has Spinal Muscular Atrophy. Our preliminary evaluation shows the potential of facial expression recognition for augmenting the lives of quadriplegic individuals by enabling them to accomplish things like walking, running, flying or other adventures that may not be so attainable otherwise.",
    "keywords": [
      "video gaming",
      "quadriplegia",
      "input methods",
      "hands-free",
      "facial expressions",
      "facial expression recognition",
      "Accessibility"
    ],
    "doi": "10.1145/3458709.3458946",
    "url": "https://doi.org/10.1145/3458709.3458946",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "131–140",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3458709.3458987",
    "title": "Virtual Whiskers: Spatial Directional Guidance using Cheek Haptic Stimulation in a Virtual Environment",
    "authors": [
      "Fumihiko Nakamura",
      "Adrien Verhulst",
      "Kuniharu Sakurada",
      "Maki Sugimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Spatial cues are an important element of navigating people in physical/virtual spaces. In terms of spatial navigation, integrating vision with other modalities, such as haptics, can guide users more effectively. Haptic cues are presented on the body parts that are sensitive to stimuli such as hands and a head. The head is reported to be superior to the body for spatial directional perception. In this paper, we propose Virtual Whiskers, a spatial directional guidance technique by haptic stimulation of the cheeks using tiny robot arms attached to a Head-Mounted Display (HMD). We deploy photo reflective sensors attached to the tip of 2 robotic arms to detect the distance between the tip and the cheek surface. Using the robot arms, we stimulate a point on the cheek obtained by calculating an intersection between the cheek surface and the target direction. We experimentally investigated how accurately participants identify the target direction provided by our guidance method. We evaluated an error between the actual target direction and the participant’s pointed direction. The experimental result shows that our method achieves the average absolute directional error of 2.76 degrees in the azimuthal plane and 7.32 degrees in the elevation plane. We also conducted a spatial guidance experiment to evaluate task performance in a target search task. We compared the condition of only vision and vision with haptics for task completion time. The average of task completion time in visual-only condition was M=12.45 s, SD=14.51 s, and visual with haptic condition resulted in M=6.91 s, SD=5.48 s. Statistical test revealed a significant difference in task completion time between the visual condition and the visual+haptic condition.",
    "keywords": [
      "virtual reality",
      "spatial guidance",
      "robot arm",
      "facial haptics"
    ],
    "doi": "10.1145/3458709.3458987",
    "url": "https://doi.org/10.1145/3458709.3458987",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "141–151",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458988",
    "title": "SmartAidView Jacket: Providing visual aid to lower the underestimation of assistive forces",
    "authors": [
      "Swagata Das",
      "Velika Wongchadakul",
      "Yuichi Kurita"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We present SmartAidView jacket to reduce the underestimation effect in human perception of assistive force. Pneumatic gel muscles (PGMs) were used as the wearable type actuators to assist wrist extension for this work. The SmartAidView jacket was made by attaching strip type LED light on the assisted forearm arm of the user. Two conditions (with and without visual feedback) were considered to identify the impact on assistive force perception. The results showed that humans incline towards underestimating of assistive forces. However, introducing a quantitative wearable visual feedback that displays a value equal to or higher than the real assistive force level can reduce this underestimation and also create an overestimation effect.",
    "keywords": [
      "visual feedback",
      "pneumatic gel muscles (PGMs)",
      "assist underestimation",
      "Wearable assist"
    ],
    "doi": "10.1145/3458709.3458988",
    "url": "https://doi.org/10.1145/3458709.3458988",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "152–156",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458989",
    "title": "Virtual Physical Task Training: Comparing Shared Body, Shared View and Verbal Task Explanation",
    "authors": [
      "Jens Reinhardt",
      "Marco Kurzweg",
      "Katrin Wolf"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "While video lectures are very successful in explaining theoretical aspects, Virtual Reality (VR) also offers the possibility to teach physical skills. While a large body of research focuses on virtual teaching explaining tasks during the task completion, it remains unclear what type of task instructions work best before task execution. In this paper, we compare three virtual teaching methods: purely verbal explanation, explanation by observing the trainer’s activity from exactly their perspective (shared-body), and explanation by observing the task execution in a shared view overlay within one’s view. Our quantitative results show that the purely verbal explanation can result in slower task replication for some tasks, e.g., path drawing in the air. Our qualitative results indicate that shared-body and shared view techniques work well but can be improved through additional verbal explanations. Moreover, task instruction might be hard to remember, which can be avoided through in-situ instructions or splitting the tasks into sub-tasks and letting the trainees replicate shorter physical actions.",
    "keywords": [
      "virtual task demonstration",
      "virtual reality",
      "cooperative distant learning"
    ],
    "doi": "10.1145/3458709.3458989",
    "url": "https://doi.org/10.1145/3458709.3458989",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "157–168",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458991",
    "title": "CV-Based Analysis for Microscopic Gauze Suturing Training",
    "authors": [
      "Mikihito Matsuura",
      "Shio Miyafuji",
      "Erwin Wu",
      "Satoshi Kiyofuji",
      "Taichi Kin",
      "Takeo Igarashi",
      "Hideki Koike"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper proposes the basis of a microscopic suture practice support system aimed to reduce the time required for neurosurgeons to practice microscopic gauze suturing. The system detects instruments in real-time from the video of the microscope camera and provides an immediate analysis. After practitioners have completed practicing, they can immediately view their results. We introduce a sequential image dataset in which the surgery phases, as well as the bounding boxes of surgical instruments, are annotated. A YOLO V4 network is fine-tuned with the proposed dataset and achieves an accuracy of approximately 94%. We also propose a tool for a phase estimation after each suturing using a Dynamic Programming based algorithm from the tracking data, which allows us to estimate the phase of the previous practice session with about 83 % accuracy. This is used for an application to detect and provide feedback on points of concern. This proposal augments the practitioners’ acquisition of skills, allowing them to immediately reflect on their most recent practice session, rather than repeating aimlessly.",
    "keywords": [
      "Visual feedback",
      "Object detection",
      "Neurosurgical training",
      "Dataset"
    ],
    "doi": "10.1145/3458709.3458991",
    "url": "https://doi.org/10.1145/3458709.3458991",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "169–173",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458952",
    "title": "A Machine Learning Model Perceiving Brightness Optical Illusions: Quantitative Evaluation with Psychophysical Data",
    "authors": [
      "Yuki Kubota",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Creating machine learning models that perceive optical illusions is essential to discover human perceptual bias for developing human augmentation technology. This study proposes a machine learning model perceiving brightness optical illusions and quantitative comparison methods of model output with psychophysical data. After considering handling methods of two psychophysical quantities, brightness and spatial frequency, we compared the output of convolutional neural networks (CNNs) that trained three types of tasks with two psychophysical data for Munker–White illusions depending on spatial frequency. Consequently, the three models reproduced perceptual characteristics corresponding to each learning task. Additionally, an optimal parameter was obtained wherein the model reproduced the previous data most accurately under our experimental condition; however, the model estimated a more substantial shift than that observed in the psychophysical data. A machine learning system perceiving optical illusions will enable us to augment human perception by reducing unnecessary optical illusions and enhancing essential perceptual information.",
    "keywords": [
      "perceptual augmentation",
      "optical illusion",
      "machine learning",
      "computer simulation"
    ],
    "doi": "10.1145/3458709.3458952",
    "url": "https://doi.org/10.1145/3458709.3458952",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "174–182",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458992",
    "title": "POV Display and Interaction Methods extending Smartphone",
    "authors": [
      "Yura Tamai",
      "Maho Oki",
      "Koji Tsukada"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Persistence of Vision (POV) display uses a technique that presents an after-image that occurs when rhythmic blinking LEDs are moved swiftly. It is a cost-effective and simple method used to turn a large surface into a display. In addition, a characteristic visual expression, such as one floating in the air, can be produced. This technique is widely used because of its advantages, however, it has the limitation of poor interactivity. In this study, a system in which a smartphone can be used as a POV display by mounting a rotation mechanism and installing original applications is suggested. Furthermore, examples of the application and the possibility of interaction methods using this system are presented, including the concept, implementation, experimental results, and implications.",
    "keywords": [
      "Smartphone",
      "POV Display",
      "Interaction",
      "Afterimage display"
    ],
    "doi": "10.1145/3458709.3458992",
    "url": "https://doi.org/10.1145/3458709.3458992",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "183–191",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458954",
    "title": "From Strangers to Friends: Augmenting Face-to-face Interactions with Faceted Digital Self-Presentations",
    "authors": [
      "Mikko Kytö",
      "Ilyena Hirskyj-Douglas",
      "David McGookin"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Sharing a digital presentation of self amongst collocated people can be used to enhance social interactions by supporting conversations. However, as there are different levels of disclosure within social relationships, it is currently unknown how to facet people’s digital content towards others. This research investigates faceting digital self-presentations according to the audience by looking at the differences in the creation and usage of private profiles (shared with a friend) and public profiles (shared amongst strangers) in face-to-face interactions. Digital profiles were accessed through head-mounted displays in social gatherings. Over three gatherings with twenty participants, we identified the importance of having different profiles. We found that, for strangers, public profile supported starting and maintaining conversations. For friends, the private profile was designed to support deeper social penetration, and for close friends, the private profile was designed from the friendship maintenance perspective. Additionally, participants wished to disclose content from their private profile to strangers as the conversations developed. These results suggest that there is a need for a tailored way of faceting digital self-presentation towards multiple audiences. We propose using augmentations that consist of a base profile that is shared with all collocated others, and a dynamically tailorable part, which can be targeted to specific individuals.",
    "keywords": [
      "tie strength",
      "social media",
      "multiple audience problem",
      "head-mounted display",
      "face-to-face interaction",
      "digital self-presentation",
      "augmented reality"
    ],
    "doi": "10.1145/3458709.3458954",
    "url": "https://doi.org/10.1145/3458709.3458954",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "192–203",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458955",
    "title": "Interactive Eye Aberration Correction for Holographic Near-Eye Display",
    "authors": [
      "Kenta Yamamoto",
      "Ippei Suzuki",
      "Kosaku Namikawa",
      "Kaisei Sato",
      "Yoichi Ochiai"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Distortions of observed images have been a long-standing problem in near-eye displays. Although many correction methods for optical system-dependent aberrations have been proposed, the image distortions caused by eye aberrations have not been studied thoroughly. In addition to the problem, eye aberrations are individual specific. Therefore, a system capable of correcting the aberration irrespective of the individual is necessary. In this study, we propose an aberration-correctable holographic near-eye display (HNED) that can be used to interactively compensate for image distortions caused by eye aberrations. We formulate a propagation equation that includes eye aberrations in the HNED and developed a GUI that enables a user to correct eye aberrations on their own. In this system, the image displayed on the HNED is updated based on the correction coefficients specified by the user. We performed experiments on human subjects to verify the effectiveness of the proposed method. Our results indicate that the minimum identifiable size in our HNED can be reduced by the aberration correction using our interface, and especially our aberration correction method is useful for the visibility of low visual-acuity users.",
    "keywords": [
      "Near Eye Display",
      "Eye Aberration",
      "Computer-Generated Hologram",
      "Aberration Correction"
    ],
    "doi": "10.1145/3458709.3458955",
    "url": "https://doi.org/10.1145/3458709.3458955",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "204–214",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458993",
    "title": "Wearable System for Promoting Salivation",
    "authors": [
      "Kai Washino",
      "Ayumi Ohnishi",
      "Tsutomu Terada",
      "Masahiko Tsukamoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "A decrease in salivary secretion can cause oral pathologies, such as glossitis and stomatitis. Previous studies have shown that the volume of saliva can be increased by various stimuli. These include warming or massaging the jaw and applying a citrus odor. However, no wearable system has been developed to promote saliva secretion in response to user requirements or mouth dryness. Our final goal was to implement a wearable system that continuously measures the volume of saliva and promotes saliva secretion. This was achieved by attaching actuators and sensors to the lower jaw. To investigate the most effective stimulus under different conditions, we conducted an experimental evaluation using three stimuli (thermal, pressure, and odor) under three different conditions (normal state, upon waking up, and after eating a meal). The results showed that the odor stimuli produced the largest amount of saliva. Finally, we discussed the design of a wearable olfactory display for promoting saliva secretion.",
    "keywords": [
      "Wearable computing",
      "Saliva secretion",
      "Oral environment"
    ],
    "doi": "10.1145/3458709.3458993",
    "url": "https://doi.org/10.1145/3458709.3458993",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "215–222",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458994",
    "title": "HemodynamicVR - Adapting the User’s Field Of View during Virtual Reality Locomotion Tasks to Reduce Cybersickness using Wearable Functional Near-Infrared Spectroscopy",
    "authors": [
      "Hiroo Yamamura",
      "Holger Baldauf",
      "Kai Kunze"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We present HemodynamicVR, a virtual reality headset combined with functional near-infrared spectroscopy (fNIRS). We believe that sensing brain activity will enable novel interactions in virtual reality. In this paper, we assess a user’s cybersickness based on the change of their total hemoglobin concentration measured via an fNIRS device, and we try to mitigate that by changing the field of view (FOV) in real-time. In this experiment, participants experienced VR locomotion with an added variable FOV controlled by velocity changes and by fNIRS. The results suggest that fNIRS can detect cybersickness as registered by the qualitative SSQ test.",
    "keywords": [
      "fNIRS",
      "VR locomotion",
      "FOV",
      "Cybersickness"
    ],
    "doi": "10.1145/3458709.3458994",
    "url": "https://doi.org/10.1145/3458709.3458994",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "223–227",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458958",
    "title": "Augmented Foot: A Comprehensive Survey of Augmented Foot Interfaces",
    "authors": [
      "Don Samitha Elvitigala",
      "Jochen Huber",
      "Suranga Nanayakkara"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Augmented foot interfaces have been studied since the beginning of wearable computers. The worlds’ first wearable computer was an instrumented shoe that consisted of a toe operated switch with a wireless module. Since then, academic research and commercial products on augmented foot interfaces are booming with novel interfaces every year. This paper surveys the body of work on augmented foot interfaces and shows the current trends and guidelines for future augmented foot interfaces. We contribute a classification of over 100 academic papers and commercially-available products. We discuss the integration of augmented foot interfaces, interaction schemes and application domains. Finally, we contribute a set of design considerations to scaffold future research of augmented foot interfaces based on the classification and inspired by the surveyed work.",
    "keywords": [
      "Survey",
      "Human Augmentation",
      "Foot Interactions",
      "Foot Augmentation",
      "Augmented Human",
      "Augmented Foot"
    ],
    "doi": "10.1145/3458709.3458958",
    "url": "https://doi.org/10.1145/3458709.3458958",
    "citations": 10,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "228–239",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3458709.3458959",
    "title": "Motion-specific browsing method by mapping to a circle for personal video Observation with Head-Mounted Displays",
    "authors": [
      "Natsuki Hamanishi",
      "Jun Rekimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Understanding the temporal changes of movement and posture in two-dimensional (2D) videos is not easy, leading users to repeat observations to understand them. However, the video interface relying on timeline representations, which depend on one’s memory, makes it hard more than necessary for a user to observe movements because they lack original motion information. Therefore, we propose a Motion Seek Circle as a user interface for Head-Mounted Displays. They visualize the motion in a video on a circle in three-dimensional space, representing temporal and spatial changes with trails and humanoid models. This automatically generated circle combined with a 2D video provides a visual overview of the movement. Our user test and study show proof of concept in a personal training context. Results indicate the proposed method is easily learnable for untrained people. By creating a visual overview of movements, we intend to solve traditional video-learning difficulties.",
    "keywords": [
      "Video Learning",
      "Personal Training",
      "Movement Browsing",
      "Head-Mounted Display"
    ],
    "doi": "10.1145/3458709.3458959",
    "url": "https://doi.org/10.1145/3458709.3458959",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "240–250",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458960",
    "title": "Exploring Pseudo Hand-Eye Interaction on the Head-Mounted Display",
    "authors": [
      "Myung Jin Kim",
      "Andrea Bianchi"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Virtual and augmented reality devices and applications have enabled the user to experience a variety of simulated real-life experiences through first-person visual, auditory, and haptic feedback. However, among the numerous everyday interactions that have been emulated, the familiar interaction of touching or rubbing the eyes is yet to be explored and remains to be understood. In this paper, we aim to understand the components of natural hand-eye interaction, propose an interaction technique through a proof-of-concept prototype head-mounted display, and evaluate the user experience of the prototype through a user study. In addition, we share insights emerged from the studies with suggestions for further development of interaction techniques based on combinations of hardware and software.",
    "keywords": [
      "visual effects",
      "touch input",
      "pseudo hand-eye interaction",
      "interaction technique",
      "head-mounted display",
      "first-person view"
    ],
    "doi": "10.1145/3458709.3458960",
    "url": "https://doi.org/10.1145/3458709.3458960",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "251–258",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458961",
    "title": "REFLECTIONS ON AIR: An Interactive Mirror for the Multisensory Perception of Air",
    "authors": [
      "Jessica Broscheit",
      "Susanne Draheim",
      "Kai von Luck",
      "Qi Wang"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper introduces an interactive mirror that aims to augment human perception and provide people with a visual-auditory experience with the air. To this end, we conceived and implemented a novel interface that responds to carbon dioxide with kinetic behavior and sound. In this study, we outline literature, related works, and the creative process of the practice-based research. We then discuss our findings and conclude with recommendations for the mirror’s use as a mediator for raising awareness of the increasing carbon dioxide concentrations caused by human activities.",
    "keywords": [
      "shape-memory alloy",
      "shape-changing interface",
      "mirror",
      "human-atmosphere interaction",
      "carbon dioxide",
      "augmented senses",
      "atmospheric interface",
      "air"
    ],
    "doi": "10.1145/3458709.3458961",
    "url": "https://doi.org/10.1145/3458709.3458961",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "259–264",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3458709.3458963",
    "title": "PAL: Wearable and Personalized Habit-support Interventions in Egocentric Visual and Physiological Contexts",
    "authors": [
      "Mina Khan",
      "Glenn Fernandes",
      "Pattie Maes"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Habits are automatic actions in stable contexts and can help sustain behavior change. We created a wearable device, called PAL, to recognize egocentric visual and physiological contexts and deliver just-in-time habit-support interventions using open-ear audio output. PAL supports personalized goals, reminder contexts, and reminder messages, and uses on-device deep learning for privacy-preserving and low-shot context detection to enable self-tracking and context-aware habit-support interventions for behavior change.",
    "keywords": [
      "Wearable",
      "Privacy-preserving",
      "Personalized",
      "On-device Deep learning",
      "Microphone.",
      "Interventions",
      "Heart Rate",
      "Habit-support",
      "Egocentric Camera",
      "Context-aware",
      "Computer Vision",
      "Behavior Change"
    ],
    "doi": "10.1145/3458709.3458963",
    "url": "https://doi.org/10.1145/3458709.3458963",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "265–267",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3458709.3458964",
    "title": "Jumple: Interactive Contents for the Virtual Physical Education Classroom in the Pandemic Era",
    "authors": [
      "Soohyun Shin",
      "Jaekyung Cho",
      "Seong-Woo Kim"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "The COVID-19 pandemic has disrupted all aspects of life, including children’s education. As classrooms move to online space, physical education teachers face unique challenges in facilitating physical activity. As a solution, we proposed Jumple, a virtual physical education classroom that allows students to jump and play in a remote learning environment. Artificial intelligence (AI) pose estimation technology is used to detect the position of the body parts and provide real-time feedback. We developed five augmented reality games that provided new possibilities to connect teachers and students.",
    "keywords": [
      "Virtual Classroom",
      "Remote Learning",
      "Physical Education",
      "Human Computer Interaction",
      "Game Design",
      "Augmented Reality"
    ],
    "doi": "10.1145/3458709.3458964",
    "url": "https://doi.org/10.1145/3458709.3458964",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "268–270",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459004",
    "title": "Sparkle: A Detachable and Versatile Wearable Sensing Platform in a Sustainable Casing",
    "authors": [
      "Adarsh Ravi",
      "Hsin-Liu Cindy Kao"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "An easy to wear versatile and a compact sensing platform, Sparkle, which consists of a customizable array of sensors: 3-axis accelerometer, temperature sensor, capacitive touch sensor, RFID chip, and light emitting diodes (LED) is presented. The sensing system is cased inside naturally occurring materials such as a seashell and a seed of an oak tree; thereby showing a possibility of usage of ecofriendly materials in smart wearable gadgets. Sparkle uses a Velcro patch that enables easy attachment and removal from any already worn ornament or a gadget. Prototypes of Sparkle attached to a silicone ring, a beaded necklace and to a watch were evaluated for their functionality. The results from an exploratory user study are presented.",
    "keywords": [
      "Wearable Computing",
      "Sustainable design",
      "Ecofriendly wearables",
      "Easy attachment",
      "Compact sensing system"
    ],
    "doi": "10.1145/3458709.3459004",
    "url": "https://doi.org/10.1145/3458709.3459004",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "271–273",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458996",
    "title": "Demo: Towards Universal User Interfaces for Mobile Robots",
    "authors": [
      "Dávid Rozenberszki",
      "Gábor Sörös"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We demonstrate the concept and a prototype of automatically generated virtual user interfaces for mobile robots. Human(s) and robot(s) are co-localized in the space via their own on-board navigation and shared spatial anchors. The robot sends the description of context-aware user interface elements to a head-mounted display, which renders the virtual widgets around and seemingly attached to the physical robot.",
    "keywords": [
      "user interface",
      "spatial anchor",
      "mobile robot",
      "mixed reality"
    ],
    "doi": "10.1145/3458709.3458996",
    "url": "https://doi.org/10.1145/3458709.3458996",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "274–276",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458997",
    "title": "GemiN’ I: Seamless Skin Interfaces Aiding Communication through Unconscious Behaviors",
    "authors": [
      "Shuyi Sun",
      "Neha Deshmukh",
      "Xin Chen",
      "Hao-Chuan Wang",
      "Katia Vega"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Voice command is the primary way of interacting with smart home devices and virtual assistants. However, being vocal is not always feasible or appropriate. This project aims to explore the potential of on-skin interfaces in social settings by discreetly communicating to a smart home device when enacting nearly unconscious behaviors. GemiN’I is a Beauty Technology that aesthetically and inconspicuously embeds sensors in face jewels to detect facial muscle movements, which then signals smart home devices without vocal commands, allowing the user and observers to interact without vocal interrupts. Our design rationale consists on: a) the form factor of a commercially available product for face decoration such as facial gems, b) novel discrete interfaces by making the technology invisible(circuits hidden within the gems), and the interaction by triggering devices when enacting unconscious behaviors (frowning), and c) seamlessly triggering a device in presence of an observer.",
    "keywords": [
      "wearable",
      "smart home",
      "gesture recognition",
      "Beauty Technology"
    ],
    "doi": "10.1145/3458709.3458997",
    "url": "https://doi.org/10.1145/3458709.3458997",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "277–279",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458998",
    "title": "Design and Implementation of an Input Interface for Wearable Devices using Pulse Wave Control by Compressing the Upper Arm",
    "authors": [
      "Yuma Akimoto",
      "Kazuya Murao"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In this paper we propose an input interface for wearable devices that enables simple command input by sensing blood flow changes using a pulse sensor installed in many wrist-worn wearable devices caused by body compression. The proposed method consists of four processes: pulse measurement, peak detection, calculation of the time interval between peaks, and command transmission. We proposed four input methods to determine a command based on the combination of compression patterns, and conducted an experiment in which four subjects were asked to choose one of several options with each input method.",
    "keywords": [
      "wearable devices",
      "pulse wave",
      "input interface",
      "PPG"
    ],
    "doi": "10.1145/3458709.3458998",
    "url": "https://doi.org/10.1145/3458709.3458998",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "280–282",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459008",
    "title": "DualBreath: Input Method Using Nasal and Mouth Breathing",
    "authors": [
      "Ryoya Onishi",
      "Tao Morisaki",
      "Shun Suzuki",
      "Saya Mizutani",
      "Takaaki Kamigaki",
      "Masahiro Fujiwara",
      "Yasutoshi Makino",
      "Hiroyuki Shinoda"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In this study, we propose DualBreath, an input method with eight commands using nasal and mouth breathing. To achieve this, we use an intentional timing shift of the nasal and mouth breathing rhythms as input commands. By combining DualBreath with other input methods such as gaze, we can interact without using our hands, including performing graphical user interface operations.",
    "keywords": [
      "physiological sensing",
      "input method",
      "hands-free",
      "breath"
    ],
    "doi": "10.1145/3458709.3459008",
    "url": "https://doi.org/10.1145/3458709.3459008",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "283–285",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459005",
    "title": "EarRecorder: A Multi-Device Earable Data Collection Toolkit",
    "authors": [
      "Likun Fang",
      "Tobias Röddiger",
      "Felix Schmid",
      "Michael Beigl"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Earables are a hot topic in wearable research. An increasing number of off-the-shelf devices support collecting sensor data for different use cases. Following this trend, we introduce EarRecorder – a unifying, open-source app that connects to multiple earables (eSense, Cosinuss°, regular earphones). The app can collect data from multiple sensor streams at the same time and lets users label the data during recording. Two user studies allowed us to optimize the usability of the app further and to validate its functionalities. In the future, we hope to include new earables with open APIs and establish a community that advances the app further.",
    "keywords": [
      "wearable",
      "toolkit",
      "open-source",
      "hearables",
      "earables"
    ],
    "doi": "10.1145/3458709.3459005",
    "url": "https://doi.org/10.1145/3458709.3459005",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "286–288",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458977",
    "title": "Reducing Muscle Activity when Playing Tremolo by Using Electrical Muscle Stimulation",
    "authors": [
      "Arinobu Niijima",
      "Toki Takeda",
      "Ryosuke Aoki",
      "Yukio Koike"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "When beginners play the piano, the activity of the forearm muscles tends to be greater than that of experts because beginners move their fingers with more force than necessary. Reducing forearm muscle activity is important for pianists to prevent fatigue and injury. However, it is difficult for beginners to learn how to do so. In this paper, we propose using electrical muscle stimulation (EMS) to teach beginners how to reduce this muscle activity while playing a tremolo. Since experts use wrist rotation efficiently when playing tremolos, we apply EMS not to muscles that are relevant to moving the fingers but to the supinator muscle and the pronator teres muscle, which are involved in wrist rotation. We conducted a user study with eight beginners to investigate how the forearm muscle activity changed through intervention with the EMS support system. After practicing with EMS, the activity level of the extensor pollicis longus muscle and extensor digitorum muscle on the forearm was significantly lower, and the participants felt less fatigue when playing tremolos. The results suggest that an EMS support system can reduce target muscle activity by applying EMS to other muscles to teach how to move limbs efficiently.",
    "keywords": [],
    "doi": "10.1145/3458709.3458977",
    "url": "https://doi.org/10.1145/3458709.3458977",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "289–291",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458978",
    "title": "Tranquillity at Home: Designing Plant-mediated Interaction for Fatigue Assessment",
    "authors": [
      "Michi Kanda",
      "Kai Kunze"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper presents a human-plant interaction system that helps observe daily mental fatigue levels by enabling the synchronisation of human eye blink data with plant health. We want to facilitate introspection on our subjective well-being by leveraging plants’ organic growth as a reflective medium. Based on users’ daily fatigue levels assessed from their eye blink data, the system controls the quality and intensity of the grow LED installed on the sensor-augmented plant. Another LED placed in front of the planter also displays the plant’s health, combining light intensity and daily average eye blink frequency. We aim to design a novel means to connect human biosignals with plant health and introduce human-plant interaction as a reflection on subjective well-being.",
    "keywords": [
      "Positive Computing",
      "Human-Plant Synchronisation",
      "Human-Plant Interaction",
      "Bio-Digital Hybrid Systems"
    ],
    "doi": "10.1145/3458709.3458978",
    "url": "https://doi.org/10.1145/3458709.3458978",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "292–294",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459000",
    "title": "Cough Activated Dynamic Face Visor",
    "authors": [
      "Timo Luukkonen",
      "Ashley Colley",
      "Tapio Seppänen",
      "Jonna Häkkilä"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "In this demo, we present a cough activated face visor, where the transparent visor screen moves to cover the wearer’s face when coughing sounds are detected. The cough detection is performed by a TinyML machine learning model, running on a microcontroller integrated to the visor’s headband. We make no claims regarding the direct impact of our prototype in preventing the spread of Covid-19, but hope it inspires discussion on future smart solutions in the area of personal protective equipment (PPE). Additionally, we note that visibility of the visor’s operation potentially encourages observers to adopt preventative measures such as hygiene practices and social distancing.",
    "keywords": [
      "vizor",
      "visor",
      "social distancing",
      "face mask",
      "PPE",
      "Covid-19",
      "Cough detection"
    ],
    "doi": "10.1145/3458709.3459000",
    "url": "https://doi.org/10.1145/3458709.3459000",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "295–297",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459007",
    "title": "Exploring a Dynamic Change of Muscle Perception in VR, Based on Muscle Electrical Activity and/or Joint Angle",
    "authors": [
      "Edouard Ferrand",
      "Adrien Verhulst",
      "Masahiko Inami",
      "Maki Sugimoto"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "We display a virtual avatar changing its biceps appearance to fit the user’s biceps contraction. The avatar’s biceps changes its size and color based on either: (EMG) the biceps activity (recorded with a electromyography [EMG] sensor); or (ANG) the elbow angle; or (EMG + ANG) the biceps activity and elbow angle. The users also wear an ad hoc haptic device around their biceps to give the illusion of stronger biceps contractions. In the Virtual Environment, the participants (N=10) look at their right biceps and do elbow flexion. They rate EMG, ANG and EMG + ANG (with and without the haptic device) along Body Ownership and Agency. We found that ANG (without the haptic device) was rated noticeably lower on the Agency than EMG and EMG + ANG (both without the haptic device). It is likely better to change the appearance of a muscle using the user’s muscle contraction rather than the user’s movement when considering the user’s muscle ownership and agency. The addition of the ad hoc haptic device appeared to not affect the Sense of Embodiment.",
    "keywords": [
      "Physical Activity",
      "Embodiment",
      "EMG",
      "*Virtual Reality"
    ],
    "doi": "10.1145/3458709.3459007",
    "url": "https://doi.org/10.1145/3458709.3459007",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "298–300",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459006",
    "title": "Boiling Mind - A Dataset of Physiological Signals during an Exploratory Dance Performance",
    "authors": [
      "Zhuoqi Fu",
      "Jiawen Han",
      "Dingding Zheng",
      "Moe Sugawa",
      "Taichi Furukawa",
      "Chernyshov George",
      "Hynds Danny",
      "Padovani Marcelo",
      "Marky Karola",
      "Kouta Minamizawa",
      "Jamie A Ward",
      "Kai Kunze"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "The relationship between audience and performers is crucial to what makes live events so special. The aim of this work is to develop a new approach amplifying the link between audiences and performers. Specifically, we explore the use of wearable sensors in gathering real-time audience data to augment the visuals of a live dance performance. We used the J!NS MEME, smart glasses with integrated electrodes enabling eye movement analysis (e.g. blink detection) and inertial motion sensing of the head (e.g. nodding recognition). This data is streamed from the audience and visualised live on stage during a performance, alongside we also collected heart rate and eye gaze of selected audience. In this paper we present the recorded dataset, including accelerometer, electrooculography(EOG), and gyroscope data from 23 audience members.",
    "keywords": [
      "Visualization",
      "Physiological Signals",
      "Audience Engagement"
    ],
    "doi": "10.1145/3458709.3459006",
    "url": "https://doi.org/10.1145/3458709.3459006",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "301–303",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459001",
    "title": "Using Body Tracking for Involving Museum Visitors in Digital Storytelling",
    "authors": [
      "Caglar Genc",
      "Jonna Häkkilä"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Technology involvement in museums leads to novel ways of communicating the museum content. Among other technologies, body tracking opens up the possibilities for embodied involvement in narratives presented in the museums. In this demo, we explore how body tracking could enable museum visitors to participate in historical storytelling installations. For this, we implemented a setup where the visitor’s body is tracked via a motion capture device and projected as a character in the narration of an indigenous Sámi myth about the constellation of the stars, the Heavenly Hunt.",
    "keywords": [
      "projection",
      "museum",
      "installation",
      "indigenous cultural heritage",
      "avatar",
      "Motion tracking"
    ],
    "doi": "10.1145/3458709.3459001",
    "url": "https://doi.org/10.1145/3458709.3459001",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "304–306",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458995",
    "title": "Designing Socially Acceptable Light Therapy Glasses for Self-managing Seasonal Affective Disorder",
    "authors": [
      "Christian Nordstrøm Rasmussen",
      "Minna Pakanen",
      "Marianne Graves Petersen"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper presents an autobiographical design process aiming towards aesthetically pleasing light-therapy device that can be augmented on the body and in the everyday life of the wearer. The design (fig. 1) is meant for easing seasonal affective disorder symptoms through longer and less bright light therapy sessions (e.g., 2 hours with max 2500 Lux brightness) than with typical light box. The glasses were chosen as a formfactor as they allow user to accomplish other tasks, such as eating breakfast, commuting, or working while on a light therapy. The resulting design is moderately small and easy to integrate into everyday life through classical eye-glasses look and adjustable lighting output. First author's initial experiences in real-life use, however, indicate that the design is not completely socially acceptable as light output puts him too much on display.",
    "keywords": [
      "Wearable",
      "Ultra-personalization",
      "Self-management",
      "Seasonal affective disorder",
      "Light therapy",
      "Autobiographical design"
    ],
    "doi": "10.1145/3458709.3458995",
    "url": "https://doi.org/10.1145/3458709.3458995",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "307–312",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459003",
    "title": "Towards Immersive Virtual Reality Simulations of Bionic Vision",
    "authors": [
      "Justin Kasowski",
      "Nathan Wu",
      "Michael Beyeler"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "Bionic vision is a rapidly advancing field aimed at developing visual neuroprostheses (‘bionic eyes’) to restore useful vision to people who are blind. However, a major outstanding challenge is predicting what people ‘see’ when they use their devices. The limited field of view of current devices necessitates head movements to scan the scene, which is difficult to simulate on a computer screen. In addition, many computational models of bionic vision lack biological realism. To address these challenges, we propose to embed biologically realistic models of simulated prosthetic vision (SPV) in immersive virtual reality (VR) so that sighted subjects can act as ‘virtual patients’ in real-world tasks.",
    "keywords": [
      "visually impaired",
      "vision augmentation",
      "virtual reality",
      "virtual patient",
      "simulated prosthetic vision",
      "retinal implant",
      "immersion"
    ],
    "doi": "10.1145/3458709.3459003",
    "url": "https://doi.org/10.1145/3458709.3459003",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "313–315",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3458973",
    "title": "Moving Visual Stimuli on Smart Glasses Affects the Performance of Subsequent Tasks",
    "authors": [
      "Eiichi Hasegawa",
      "Naoya Isoyama",
      "Nobuchika Sakata",
      "Kiyoshi Kiyokawa"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "There is a rich body of literature on the effects of visual information on the viewer’s psychology and behavior. However, most of the existing studies ignore such effects after viewing. With the advent of smart glasses, it has become possible to view visual information all the time. In this study, we investigate whether the visual information presented on smart glasses affects the subsequent work efficiency in cycling situation. In the experiment, the participants cycling in a virtual city were presented with a running avatar (3.00 steps/s), a fast-running avatar (5.25 steps/s), or no avatars, through simulated smart glasses in a virtual environment. They then solved a dot-clicking task on a desktop monitor. It was found that the participants tended to solve the task most quickly after viewing a running avatar.",
    "keywords": [
      "Work Efficiency",
      "Wearable Computing",
      "Subjective Time",
      "Smart Glasses",
      "Aftereffects"
    ],
    "doi": "10.1145/3458709.3458973",
    "url": "https://doi.org/10.1145/3458709.3458973",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "316–318",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3458709.3459002",
    "title": "Text Summary Augmentation for Intelligent Reading Assistant",
    "authors": [
      "Pramod Vadiraja",
      "Andreas Dengel",
      "Shoya Ishimaru"
    ],
    "year": 2021,
    "conference": "AHS",
    "conferenceYear": "AHs '21",
    "abstract": "This paper presents a technique to assist a reader. We aim to reduce manual efforts of the reader by leveraging the state-of-the-art document summarization techniques and providing summaries about unclear descriptions for each reader. Our system acts as a plug-and-play model that can be modified to support additional methodologies. As a backend of the system, we investigated several text summarization techniques and evaluated three techniques of them: TextRank, LexRank, and Luhn’s algorithm.",
    "keywords": [
      "text summarization",
      "knowledge acquisition",
      "information retrieval",
      "human-document interaction",
      "Cognitive augmentation"
    ],
    "doi": "10.1145/3458709.3459002",
    "url": "https://doi.org/10.1145/3458709.3459002",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2021",
    "pages": "319–321",
    "publisher": "Association for Computing Machinery",
    "location": "Rovaniemi, Finland",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519405",
    "title": "GazeBreath: Input Method Using Gaze Pointing and Breath Selection",
    "authors": [
      "Ryoya Onishi",
      "Tao Morisaki",
      "Shun Suzuki",
      "Saya Mizutani",
      "Takaaki Kamigaki",
      "Masahiro Fujiwara",
      "Yasutoshi Makino",
      "Hiroyuki Shinoda"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Gaze input is a promising input method that allows intuitive and fast pointing. There are two phases in gaze input: pointing and selection. In the pointing phase, the cursor follows the eye movement to the target object, whereas in the selection phase, the user clicks a button to generate the next event. Switching between the two phases is a general problem in gaze input. In this study, we propose a method for selection by applying an additional modality, namely breathing. In the proposed system, we measured the time of inhalation and exhalation from the change in the nose and mouth temperature observed by a thermal camera. Using this time, we judge which phase the user is in. Through user studies, we found that our method enables selection at a speed not quite inferior to the conventional method. In addition, this method has the advantage of allowing multiple selection commands.",
    "keywords": [
      "breath",
      "gaze pointing",
      "hands-free",
      "input method",
      "physiological sensing"
    ],
    "doi": "10.1145/3519391.3519405",
    "url": "https://doi.org/10.1145/3519391.3519405",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "1–9",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519411",
    "title": "EyeMove - Towards Mobile Authentication using EOG Glasses",
    "authors": [
      "Kirill Ragozin",
      "Karola Marky",
      "Jie Lu",
      "Kai Kunze"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Existing approaches for mobile authentication are prone to shoulder-surfing and side-channel attacks. Using gazes for authentication has been demonstrated to be more resistant to these attacks. Yet, existing solutions rely on eye-tracking by the device’s front camera that is not always reliable. In this paper, we present an approach for EOG-based authentication by determining the gaze-based on the electronic potential of the eyes. Our approach runs on commercially available smart glasses and there is no need for the user to look at the device. Through a user study with 15 participants, we demonstrate the feasibility of the approach, its usability, and its adoption by mobile device users. Even users without regular glasses would adopt EOG-based authentication.",
    "keywords": [
      "Gaze-Based Authentication",
      "Mobile Authentication",
      "Multimodal Authentication"
    ],
    "doi": "10.1145/3519391.3519411",
    "url": "https://doi.org/10.1145/3519391.3519411",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "10–14",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519396",
    "title": "Understanding Challenges and Opportunities of Technology-Supported Sign Language Learning",
    "authors": [
      "Sarah Faltaous",
      "Torben Winkler",
      "Christina Schneegass",
      "Uwe Gruenefeld",
      "Stefan Schneegass"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Around 466 million people in the world live with hearing loss, with many benefiting from sign language as a mean of communication. Through advancements in technology-supported learning, autodidactic acquisition of sign languages, e.g., American Sign Language (ASL), has become possible. However, little is known about the best practices for teaching signs using technology. This work investigates the use of different conditions for teaching ASL signs: audio, visual, electrical muscle stimulation (EMS), and visual combined with EMS. In a user study, we compare participants’ accuracy in executing signs, recall ability after a two-week break, and user experience. Our results show that the conditions involving EMS resulted in the best overall user experience. Moreover, ten ASL experts rated the signs performed with visual and EMS combined highest. We conclude our work with the potentials and drawbacks of each condition and present implications that will benefit the design of future learning systems.",
    "keywords": [
      "Sign language learning",
      "audio",
      "electrical muscle stimulation",
      "visual."
    ],
    "doi": "10.1145/3519391.3519396",
    "url": "https://doi.org/10.1145/3519391.3519396",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "15–25",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519399",
    "title": "E-MASK: A Mask-Shaped Interface for Silent Speech Interaction with Flexible Strain Sensors",
    "authors": [
      "Yusuke Kunimi",
      "Masa Ogata",
      "Hirotaka Hiraki",
      "Motoshi Itagaki",
      "Shusuke Kanazawa",
      "Masaaki Mochimaru"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We present E-MASK, a mask-shaped interface for silent speech interaction. As face masks have become daily accessories since the COVID-19 pandemic, it is reasonable to utilize a mask as a wearable interface. Unlike conventional speech recognition, we envision that silent speech interaction allows users to access digital services even in crowded public spaces. With flexible and highly sensitive strain sensors, E-MASK presents a new measurement principle for silent speech interactions. We built a dataset of sensor patterns corresponding to 21 fundamental commands of Alexa’s operation. All commands were silently spoken by five non-native English speakers. The dataset was used to estimate the silently spoken commands. Estimation accuracies of 84.4% while sitting on a chair and 79.1% while walking on a treadmill were archived. This result suggests that our system provides seamless interaction with digital devices in various situations in daily life, such as walking in a crowd.",
    "keywords": [
      "Mask-Shaped Interface",
      "Silent Speech Interaction",
      "Strain Sensor"
    ],
    "doi": "10.1145/3519391.3519399",
    "url": "https://doi.org/10.1145/3519391.3519399",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "26–34",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3519391.3519403",
    "title": "Knock Knock: A Children-oriented Vocabulary Learning Tangible User Interaction System",
    "authors": [
      "Xinrui Fang",
      "Takuro Watanabe",
      "Chengshuo Xia",
      "Arthur Torck"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Early childhood English learning is attracting growing attention. Among the different aspects of English learning: listening, speaking, reading and writing, vocabulary learning is fundamental for the following aspects. However, traditional vocabulary learning methods, such as dictionary learning or literacy-rich environment learning, are tedious and lack interaction between children and the environment. In this work, we propose Knock Knock, a children-oriented vocabulary learning tangible user interaction system. Children can learn English vocabulary with a natural and simple knock gesture using our proof-of-concept prototype. We also developed a mobile application that could augment children’s learning experiences and encourage them to make more friends.",
    "keywords": [
      "audio-based interaction",
      "children",
      "tangible interface",
      "vocabulary learning"
    ],
    "doi": "10.1145/3519391.3519403",
    "url": "https://doi.org/10.1145/3519391.3519403",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "35–39",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3519391.3519394",
    "title": "Pudica: A Framework For Designing Augmented Human-Flora Interaction",
    "authors": [
      "Olivia Seow",
      "Cedric Honnet",
      "Simon Perrault",
      "Hiroshi Ishii"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "This paper introduces and studies a design framework for designing human-flora interaction in plant-based interfaces, which could play a prominent role in a world where HCI strives to be less pollutive, more power saving, and humane. It discusses critical considerations (e.g. maintenance, reproducibility) for such interfaces, supported by a user study based on an interactive prototype. The results of our user study show that users’ interest in plants varies significantly with past experience. Users may create a strong emotional bond with plants, suggesting that organic interfaces should be used for emotionally strong use cases, such as keeping in touch with loved ones or checking important data.",
    "keywords": [
      "Augmented Health and Sensing",
      "Augmented human-flora interaction",
      "Organic Context-Awareness.",
      "Plant interfaces",
      "Shape-changing interfaces"
    ],
    "doi": "10.1145/3519391.3519394",
    "url": "https://doi.org/10.1145/3519391.3519394",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "40–45",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3519391.3519398",
    "title": "ARcall: Real-Time AR Communication using Smartphones and Smartglasses",
    "authors": [
      "Hemant Bhaskar Surale",
      "Yu Jiang Tham",
      "Brian A. Smith",
      "Rajan Vaish"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Augmented Reality (AR) smartglasses are increasingly regarded as the next generation personal computing platform. However, there is a lack of understanding about how to design communication systems using them. We present ARcall, a novel Augmented Reality-based real-time communication system that enables an immersive, delightful, and privacy-preserving experience between a smartphone user and a smartglasses wearer. ARcall allows a remote friend (Friend) to send and project AR content to a smartglasses wearer (Wearer). The ARcall system was designed with the practical limits of existing AR glasses in mind, including shorter battery life and a reduced field of view. We conduct a qualitative evaluation of the three main components of ARcall: Drop-In, ARaction, and Micro-Chat. Our results provide novel insights for building future AR-based communication methods, including, the importance of context priming, user control over AR content placement, and the feeling of co-presence while conversing.",
    "keywords": [
      "augmented reality",
      "calling",
      "immersive communication"
    ],
    "doi": "10.1145/3519391.3519398",
    "url": "https://doi.org/10.1145/3519391.3519398",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "46–57",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519406",
    "title": "Morphace: An Integrated Approach for Designing Customizable and Transformative Facial Prosthetic Makeup",
    "authors": [
      "Sijia Wang",
      "Cathy Mengying Fang",
      "Yiyao Yang",
      "Kexin Lu",
      "Maria Vlachostergiou",
      "Lining Yao"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "On-skin electronics are an emerging group of interactive devices, with challenges in both engineering functionalities and design aesthetics. One design approach that lacks extensive exploration is combining prosthetic makeup with transformative wearables that generate dynamic output modalities. We propose a design approach called Morphace that imbues prosthetic makeup with customizability and transformative properties, which allows wearables to ‘camouflage’ on the original face and transform it. We use a case study on the face for its rich affordance of expressions and high visibility, which emphasizes the appearance of epidermal electronics. We developed a three-step computational design and fabrication workflow that integrates the prosthetic makeup process to fabricate functional primitives. We further explore the utility of Morphace through interactive experiences in social communication, facial augmentation, and self-expression. We believe Morphace offers an integrative approach that enriches current wearable solutions and enables creative output modalities and affordances for designing future on-skin shape-changing interfaces.",
    "keywords": [
      "Prosthetics",
      "Shape-changing interface",
      "Wearable"
    ],
    "doi": "10.1145/3519391.3519406",
    "url": "https://doi.org/10.1145/3519391.3519406",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "58–67",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519414",
    "title": "On Eliciting a Sense of Self when Integrating with Computers",
    "authors": [
      "Valdemar Danry",
      "Pat Pataranutaporn",
      "Florian Mueller",
      "Pattie Maes",
      "Sang-won Leigh"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "While some Human-Computer Integration (HInt) systems have successfully demonstrated that humans and technology can be physically and functionally integrated, we find that these integrations are not necessarily part of the users identity (i.e. self-judgment) or felt as part the user (i.e. with a sense of self) and that they can even create feelings of self-dissociation. Literature on how to elicit these self-experiences is often inconsistent and vague, which complicates the metric for success and hinders the advancement of research. To help designers elicit and systematically evaluate in particular a sense of self, we draw metrics and theory from phenomenology and cognitive science. We find that experiential structures such as “pre-reflective experience”, “sense of body-ownership” and “sense of agency” are to be designed for as they together seem to elicit a “sense of self”.",
    "keywords": [
      "cognitive science",
      "experiential integration",
      "human augmentation",
      "human-computer integration",
      "phenomenology",
      "sense of self"
    ],
    "doi": "10.1145/3519391.3519414",
    "url": "https://doi.org/10.1145/3519391.3519414",
    "citations": 18,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "68–81",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "14",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3519391.3522752",
    "title": "Immersive Virtual Reality Simulations of Bionic Vision",
    "authors": [
      "Justin Kasowski",
      "Michael Beyeler"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Bionic vision uses neuroprostheses to restore useful vision to people living with incurable blindness. However, a major outstanding challenge is predicting what people “see” when they use their devices. The limited field of view of current devices necessitates head movements to scan the scene, which is difficult to simulate on a computer screen. In addition, many computational models of bionic vision lack biological realism. To address these challenges, we present VR-SPV, an open-source virtual reality toolbox for simulated prosthetic vision that uses a psychophysically validated computational model to allow sighted participants to “see through the eyes” of a bionic eye user. To demonstrate its utility, we systematically evaluated how clinically reported visual distortions affect performance in a letter recognition and an immersive obstacle avoidance task. Our results highlight the importance of using an appropriate phosphene model when predicting visual outcomes for bionic vision.",
    "keywords": [
      "retinal implant",
      "simulated prosthetic vision",
      "virtual reality"
    ],
    "doi": "10.1145/3519391.3522752",
    "url": "https://doi.org/10.1145/3519391.3522752",
    "citations": 11,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "82–93",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3522753",
    "title": "Synchronous and Asynchronous Manipulation Switching of Multiple Robotic Embodiment Using EMG and Eye Gaze",
    "authors": [
      "Yukiya Nakanishi",
      "Masaaki Fukuoka",
      "Shunichi Kasahara",
      "Maki Sugimoto"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Through the use of multiple avatars and robots, the construction of an alter ego whose senses and movements are synchronized with those of the operator has been explored. There are two possible states of operation when manipulating those bodies as synchronous operation: multiple bodies are synchronized simultaneously, and asynchronous operation: a specific body is selectively operated. We propose a system that allows intuitive switching between the active robot and the operation state by Eye gaze and EMG. A two-stage pick-and-place task was performed with the proposed system, and task performance and subjective evaluation of embodiment were analyzed. To consider the coordinates of the tracker in the asynchronous operation of the robot arm, we performed a similar study for a coordinate condition: the absolute coordinate condition, where the coordinates of the tracker are stored in space, and the relative coordinate condition, where only the motion of the tracker is added.",
    "keywords": [
      "EMG Sensor",
      "Eye Gaze",
      "Robot Arm",
      "augmented human",
      "embodiment",
      "multiple bodies"
    ],
    "doi": "10.1145/3519391.3522753",
    "url": "https://doi.org/10.1145/3519391.3522753",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "94–103",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3522754",
    "title": "The Reference Frame of Robotic Limbs Contributes to the Sense of Embodiment and Motor Control Process",
    "authors": [
      "Kuniharu Sakurada",
      "Ryota Kondo",
      "Fumihiko Nakamura",
      "Masaaki Fukuoka",
      "Michiteru Kitazaki",
      "Maki Sugimoto"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "The robotic limbs using a body remapping approach often follow either the user’s reference frame 1) fixed outside the user’s body (Space frame), 2) centered on a user’s torso (Torso frame), or 3) centered on a user’s head (Head frame). In this study, we investigate the effect of the reference frame on the sense of embodiment and the user’s motor control process. We asked 12 participants to perform some point-to-point tasks with the virtual robotic limbs remapped to the participants’ feet. The virtual robotic limbs tip followed the participants’ feet and each reference frame condition (Space frame, Torso frame, and Head frame). As a result, the reaction time, the movement straightness, and the movement priority were significantly high rate in the Torso frame condition. The subjective score on the sense of embodiment showed that the reference frame condition contributed to the sense of embodiment.",
    "keywords": [
      "motor control process",
      "reference frame",
      "robotic limbs",
      "sense of embodiment"
    ],
    "doi": "10.1145/3519391.3522754",
    "url": "https://doi.org/10.1145/3519391.3522754",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "104–115",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519413",
    "title": "Analysis and Observation of Behavioral Factors Contributing to Improvement of Embodiment to a Supernumerary Limb",
    "authors": [
      "Nonoka Nishida",
      "Yukiko Iwasaki",
      "Theophilus Teo",
      "Masaaki Fukuoka",
      "Maki Sugimoto",
      "Po-han Chen",
      "Fumihiro Kato",
      "Michiteru Kitazaki",
      "Hiroyasu Iwata"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Research has been conducted on attaching a robotic arm to a human body and using it as a supernumerary limb. However, it remains unclear what type of behaviors contribute to the induction of embodiment in a supernumerary limb. In this study, we created a virtual environment where we could observe behaviors that contribute to the embodiment of avatars and tools, such as ”looking at one’s own body in a mirror,” ”moving the supernumerary limb quickly and accurately,” and ”paying constant attention to the supernumerary limb,” and verified whether these behaviors are also effective in inducing the embodiment of the supernumerary limb. After spending 40 min in a behavior observation space, the left hand, the side with the supernumerary limb, was used significantly less frequently. In addition to these behaviors, ”self-touching,” ”transferring objects between their own hand and the supernumerary limb,” and ”whole-body movement” were also observed. The observations suggest that these behaviors may change the awareness of the supernumerary limb and contribute to the improvement of its embodiment.",
    "keywords": [
      "Human Augmentation",
      "Human-Robot Interaction",
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519413",
    "url": "https://doi.org/10.1145/3519391.3519413",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "116–120",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519408",
    "title": "Parallel Ping-Pong: Exploring Parallel Embodiment through Multiple Bodies by a Single User",
    "authors": [
      "Kazuma Takada",
      "Midori Kawaguchi",
      "Akira Uehara",
      "Yukiya Nakanishi",
      "Mark Armstrong",
      "Adrien Verhulst",
      "Kouta Minamizawa",
      "Shunichi Kasahara"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We propose Parallel Ping-Ping, a system that realizes “Parallel Embodiment”, the experience of a single user controlling multiple bodies simultaneously. The user plays ping-pong by controlling 2 robot arms using a Virtual Reality (VR) controller, while looking at 2 tables through a Head Mounted Display (HMD). The computer constantly calculates the ball trajectory and the motion for the robot arm to hit the balls back automatically through color cameras. Based on this calculation, the system integrates the motion of the user’s controller, maintaining the sense of agency of the robot arms even though single user plays ping-pong with two opponents. In addition, the user’s view through the HMD is automatically switched to the appropriate table according to the calculated position and direction, so that the user can perceive both tables’ situation smoothly. We exhibited a three-day Parallel Ping-Pong demonstration and surveyed 142 participants about their perception while controlling multiple bodies. In this paper, we introduce the Parallel Ping-Pong system framework and the insights from the survey of demonstration. We also discuss the design implication for Parallel Embodiment based on these results.",
    "keywords": [
      "Body Ownership",
      "Cybernetic Avatar",
      "Parallel Embodiment",
      "Robotic Arm",
      "Sense of Agency"
    ],
    "doi": "10.1145/3519391.3519408",
    "url": "https://doi.org/10.1145/3519391.3519408",
    "citations": 22,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "121–130",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519400",
    "title": "SkiSim: A comprehensive Study on Full Body Motion Capture and Real-Time Feedback in VR Ski Training",
    "authors": [
      "Jana Hoffard",
      "Xuan Zhang",
      "Erwin Wu",
      "Takuto Nakamura",
      "Hideki Koike"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Ski Training has many restrictions, such as environmental requirements and the constraint for indirect cyclic feedback given by coaches. Though, several researcher introduced different systems in order to overcome these limitations, there is no comprehensive study on which body parts trainees concentrate on the most and which feedback is suited for an intuitive training of the skier. We therefore conducted two user studies to determine which body parts the user concentrates on while using a ski simulator and whether or not a more informative non-visual feedback on said body parts can compete to the state of the art visual feedback.",
    "keywords": [
      "HCI design and evaluation methods",
      "Human Computer Interaction (HCI)",
      "Human-centered computing",
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519400",
    "url": "https://doi.org/10.1145/3519391.3519400",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "131–141",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519402",
    "title": "Skiing, Fast and Slow: Evaluation of Time Distortion for VR Ski Training",
    "authors": [
      "Takashi Matsumoto",
      "Erwin Wu",
      "Hideki Koike"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Virtual reality-based sports simulators are widely developed, which makes training in a virtual environment possible. One of the advantages of virtual training is the customizability and the user can be augmented by extra cues or different physical environments, which is difficult to realize in real training. In this paper, we study the effect of time distortion on alpine ski training to find out how modifying the temporal space can affect sports training. Two different experiments are conducted to investigate how a fast/slow and a static/dynamic time distortion-based training, respectively, can impact the performance of users.",
    "keywords": [
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519402",
    "url": "https://doi.org/10.1145/3519391.3519402",
    "citations": 16,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "142–151",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519404",
    "title": "A System for Augmenting Humans’ ability to Learn Kendama Tricks through Virtual Reality Training",
    "authors": [
      "Hitoshi Kawasaki",
      "Sohei Wakisaka",
      "Hiroto Saito",
      "Atsushi Hiyama",
      "Masahiko Inami"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We developed a system for augmenting humans’ ability to learn Kendama tricks through virtual reality (VR) training. It is often considered to be difficult to play because the ball moves fast, and it is difficult to understand the factors that lead to the success or failure. With this system, we reduced the difficulty of Kendama by slowing down the speed of the ball within a VR space. In addition, this system used the results of detailed measurements of the factors that lead to the success or failure and presented its information to users in an easy-to-understand manner. As a result of the experiment, we confirmed that VR training which used the proposed method has the potential to increase the proficiency of the users in both physical and VR spaces as well as their confidence in succeeding, as compared to VR training which did not.",
    "keywords": [
      "success factors",
      "time-based interventions",
      "training",
      "virtual reality"
    ],
    "doi": "10.1145/3519391.3519404",
    "url": "https://doi.org/10.1145/3519391.3519404",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "152–161",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519407",
    "title": "Human balance ability assessment through Pneumatic Gel Muscle (PGM)-based Augmentation",
    "authors": [
      "Priyanka Ramasamy",
      "Masato Hamada",
      "Swagata Das",
      "Yuichi Kurita"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Despite many articles and reports on balance ability evaluation with postural control, only a few studies have reported the relationship between sudden perturbations and variations in the centre of pressure (COP). This paper introduces automated control of a balance exercise suit equipped with pneumatic gel muscles (PGMs) to cause sudden perturbations. The effectiveness of this PGM suit has been quantitatively analysed for the first time in this paper. COP data were recorded while perturbation-based interventions were provided using the PGM suit during different standing conditions: single-leg standing, tandem, and closed and visual conditions: eyes open and closed. Eight indices were calculated from the COP data to visualize various patterns of balancing ability. The highest effect size values (comparing activated and deactivated PGM conditions) were observed for the variance of the trajectory (0.79) and sway area (0.70) indices during open-eyed single-leg standing posture.",
    "keywords": [
      "Balance ability",
      "perturbation intervention",
      "pneumatic gel muscles (PGMs)",
      "postural control"
    ],
    "doi": "10.1145/3519391.3519407",
    "url": "https://doi.org/10.1145/3519391.3519407",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "162–169",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519412",
    "title": "Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances",
    "authors": [
      "Courtney N. Reed",
      "Sophie Skach",
      "Paul Strohmeier",
      "Andrew P. McPherson"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer’s vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.",
    "keywords": [
      "Design",
      "biosignals",
      "electromyography",
      "fabric sensors",
      "knit",
      "music performance",
      "singing",
      "wearables"
    ],
    "doi": "10.1145/3519391.3519412",
    "url": "https://doi.org/10.1145/3519391.3519412",
    "citations": 17,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "170–183",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "14",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519416",
    "title": "Scenario-based Investigation of Acceptance of Electric Muscle Stimulation",
    "authors": [
      "Ambika Shahu",
      "Philipp Wintersberger",
      "Florian Michahelles"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Electrical Muscle Stimulation (EMS) provides multiple interaction possibilities in various fields. It may transmit object affordances, user motions or give rich haptic feedback by adjusting stimulation settings, especially in virtual worlds. Parameters necessary to accept EMS, impacting how users perceive the technology, are yet not thoroughly studied. We investigated users’ acceptance of EMS by conducting an online survey (N=113) and an interview (N=5) study using four scenarios derived from the literature. We used the technology acceptance paradigm (TAM) to assess user acceptance and gathered factors for accepting/rejecting the technology. By connecting survey results and participants’ impressions of the four EMS scenarios to emerging themes, we can show that potential users reject EMS systems if they perceive a high level of risk and a lack of the sense of agency. We drew inferences based on our findings that will guide in designing and developing products that provide acceptable experiences.",
    "keywords": [
      "Electrical muscle stimulation",
      "Human actuation",
      "Human-computer interaction",
      "User acceptance"
    ],
    "doi": "10.1145/3519391.3519416",
    "url": "https://doi.org/10.1145/3519391.3519416",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "184–194",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519415",
    "title": "DualEMS: Two-Channel Arbitrary Waveform Electrical Muscle Stimulation Device to Design Interference Stimulation",
    "authors": [
      "Hiroki Ohara",
      "Shoichi Hasegawa"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We propose a device to design interference stimulation, which has not been feasible in HCI. Also, we suggest parameters of stimulus pulses and electrodes’ placement that facilitate stimulation of the supinator muscle (deep muscle). EMS (Electrical Muscle Stimulation) has been used in the medical field for a long time and, more recently, has been applied to HCI. It has been noticed that the diversity of stimulus pulses enables new stimulation methods. However, there is no compact EMS device that can output arbitrary waveforms in multichannel. Therefore, we developed ”DualEMS”, which can simultaneously output arbitrary waveforms with two channels. The user study validated the performance of the DualEMS, as it was possible to achieve the forearm pronation with the previously proposed method and the forearm supination with the newly proposed approach. In the future, we expect to realize new stimulation methods and interactions by using this system and approach.",
    "keywords": [
      "Electrical deep muscle stimulation",
      "Electrical muscle stimulation",
      "Forearm pronation",
      "Forearm supination",
      "Interference stimulation"
    ],
    "doi": "10.1145/3519391.3519415",
    "url": "https://doi.org/10.1145/3519391.3519415",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "195–202",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519395",
    "title": "DragTapVib:&nbsp;An On-Skin Electromagnetic Drag, Tap, and Vibration Actuator for Wearable Computing",
    "authors": [
      "Likun Fang",
      "Ting Zhu",
      "Erik Pescara",
      "Yiran Huang",
      "Yexu Zhou",
      "Michael Beigl"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "The skin, as the largest organ distributed all over the human body, offers excellent opportunities for different kinds of input stimuli. However, most of the haptic devices can only render single sensations or they need to combine multiple complex components for generating multiple sensations. We present “DragTapVib” in this paper, a novel, ultra-low-cost, wearable actuator that can reliably provide dragging, tapping, and vibrating sensations to the user. Our actuator is fully electromagnetically-actuated with a moving tactor that can render three haptic feedbacks through systematically controlling the current inside the flexible PCBs. The actuator can be arranged with varying parts of the body which enriches the potentials to implement promising application scenarios including delivering the notification and providing immersive haptic feedback either in virtual reality or in gameplay. A prototypical technical evaluation demonstrated the mechanical properties of our actuator. We quantitatively conducted a series of psychophysical user studies (N= 12) to reveal the feasibility of our prototype. The overall absolute identification study for distinguishing three sensations accuracy at two body locations reached up to 97.2%.",
    "keywords": [
      "Haptic device design",
      "Multimodal haptics",
      "On-skin device",
      "Tactile actuator",
      "Wearable computing"
    ],
    "doi": "10.1145/3519391.3519395",
    "url": "https://doi.org/10.1145/3519391.3519395",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "203–211",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519393",
    "title": "Exploring Feedback-based Testing Effects for Skin Reading",
    "authors": [
      "Granit Luzhnica",
      "Aleksandra Krajnc",
      "Eduardo Veas"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Recent studies demonstrate people’s ability to interpret vibrotactile encoded symbols and words with a few hours of training. However, reducing the time and effort required to learn the associations between vibrotactile patterns and encoded information remains a challenge and a potential barrier to adopting such a technology. This work investigates how feedback during intermediate testing reinforces the training effect. A study with 25 participants shows that feedback during testing leads to a steeper learning curve and increases participants’ confidence during the training procedure.",
    "keywords": [
      "HCI",
      "feedback-based testing",
      "skin reading",
      "tactile displays",
      "tactile feedback",
      "testing",
      "training",
      "user study",
      "wearable"
    ],
    "doi": "10.1145/3519391.3519393",
    "url": "https://doi.org/10.1145/3519391.3519393",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "212–217",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519410",
    "title": "Increasing the Perceived Speed of Dynamic Handheld Shape Displays through Visuo-Haptic Illusions",
    "authors": [
      "Antonin Cheymol",
      "Yutaro Hirao",
      "Shigeo Yoshida",
      "Hideaki Kuzuoka"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Dynamic handheld shape displays, with the capacity to change their shapes dynamically while being grasped by the user, have been developed to enable realistic haptic interaction with various virtual objects in virtual reality. However, these devices tend to be limited in their actuation speed owing to the small size requirement necessary for users to grab, which reduces the variability of virtual objects that they can render. In this study, we employ visuo-haptic illusions to improve the perceived actuation speed of dynamic handheld shape displays in VR to alleviate this problem. We present two approaches: distance modification, which increases the visual travel distance, and time modification, which decreases the visual travel time, both of which were applied to an existing handheld shape display. Experimental results suggest that the perception of the device’s actuation speed is improved by our approach, while participants remain unaware of the discrepancy between visual and haptic speed.",
    "keywords": [
      "Handheld Device",
      "Haptic Device",
      "Illusion",
      "Perception",
      "Shape Display",
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519410",
    "url": "https://doi.org/10.1145/3519391.3519410",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "218–229",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519401",
    "title": "Cyborgs, Human Augmentation, Cybernetics, and JIZAI Body",
    "authors": [
      "Masahiko Inami",
      "Daisuke Uriu",
      "Zendai Kashino",
      "Shigeo Yoshida",
      "Hiroto Saito",
      "Azumi Maekawa",
      "Michiteru Kitazaki"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We propose a concept called “JIZAI Body” that allows each person to live the way they wish to live in society. One who acquires a JIZAI Body can (simultaneously) control (or delegate control) of their natural body and extensions of it, both in physical and cyberspace. We begin by describing the JIZAI Body and the associated JIZAI state in more detail. We then provide a review of the literature, focusing on human augmentation and cybernetics, robotics and virtual reality, neuro and cognitive sciences, and the humanities; fields which are necessary for the conception, design, and understanding of the JIZAI Body. We then illustrate the five key aspects of a JIZAI Body through existing works. Finally, we present a series of example scenarios to suggest what a JIZAI society may look like. Overall, we present the JIZAI Body as a preferred state to aspire towards when developing and designing augmented humans.",
    "keywords": [
      "Cognition",
      "Cybernetics",
      "Cyborg",
      "Embodiment",
      "Extended Body",
      "Human Augmentation",
      "JIZAI",
      "Neuroscience",
      "Phenomenology",
      "Robotics",
      "Virtual Body",
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519401",
    "url": "https://doi.org/10.1145/3519391.3519401",
    "citations": 26,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "230–242",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "13",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519409",
    "title": "A New Mask for a New Normal: Investigating an AR Supported Future under COVID-19",
    "authors": [
      "Zendai Kashino",
      "Daisuke Uriu",
      "Ziyue Zhang",
      "Shigeo Yoshida",
      "Masahiko Inami"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Wearing masks and social distancing have become the norm during the COVID-19 pandemic. However, these are increasingly seen as a source of frustration in face-to-face communications. While efforts have been made to overcome these impediments to communication, they typically focus on recovering lost communication quality. Herein, we envision a future where everyone augments their vision using face masks with Augmented Reality capabilities, such that people can conduct safe and expressive face-to-face communication in public. To speculate on this vision, we developed an AR mask prototype which can overlay dynamic virtual “masks” on other users. The virtual mask is dynamic in that it accelerates towards any observer who approaches the wearer. Using this system, we conducted an explorative study to further our speculations on the impact of ubiquitous AR technologies.",
    "keywords": [
      "COVID-19",
      "augmented reality",
      "mask",
      "social distancing",
      "speculative design",
      "virtual reality"
    ],
    "doi": "10.1145/3519391.3519409",
    "url": "https://doi.org/10.1145/3519391.3519409",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "243–253",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519397",
    "title": "The Butterfly Effect: Novel Opportunities for Steady-State Visually-Evoked Potential Stimuli in Virtual Reality",
    "authors": [
      "Jonas Auda",
      "Uwe Gruenefeld",
      "Thomas Kosch",
      "Stefan Schneegass"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "In Virtual Reality (VR), Steady-State-Visual Evoked Potentials (SSVEPs) can be used to interact with the virtual environment using brain signals. However, the design of SSVEP-eliciting stimuli often does not match the virtual environment, and thus, disrupts the virtual experience. In this paper, we investigate stimulus designs with varying suitability to blend in virtual environments. Therefore, we created differently-shaped, virtual butterflies. The shapes vary from rectangular wings, over round wings, to a wing shape of a real butterfly. These butterflies elicit SSVEP responses through different animations – flickering or flapping wings. To evaluate our stimuli, we first extracted suitable frequencies for SSVEP responses from the literature. In a first study, we determined three frequencies yielding the best detection accuracy in VR. We used these frequencies in a second study to analyze detection accuracy and appearance ratings using our stimuli designs. Our work contributes insights into the design of SSVEP stimuli that blend into virtual environments and still elicit SSVEP responses.",
    "keywords": [
      "Brain-Computer Interface",
      "Realism",
      "Steady State Visually Evoked Potential",
      "Virtual Reality"
    ],
    "doi": "10.1145/3519391.3519397",
    "url": "https://doi.org/10.1145/3519391.3519397",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "254–266",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "13",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3522751",
    "title": "Dynamic Appearance Augmentation Method that Enables Easy Prototyping of Masks for Performance",
    "authors": [
      "Motoyasu Masui",
      "Yoshinari Takegawa",
      "Keiji Hirata"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "In this study, we propose a dynamic facial expression augmentation method that enables easy prototyping of masks for performance. The proposed method uses thermochromic ink, i.e., ink that changes color in response to change in temperature. We also propose a mechanism that allows the wearer to perceive the change in the color of the part of the mask drawn with thermochromic ink, by using the residual heat of the thermochromic ink. During performance, the wearer can check whether the mask is working without any trouble and without looking in a mirror and can therefore concentrate on the performance. We assume that a typical use of the system is in a theatrical performance such as a school play, where the performers make masks by hand in order to reproduce the characters in the story. The method is evaluated from four perspectives (cost, support for performance, diversity of expression, and change perception).",
    "keywords": [
      "Prototyping",
      "Stage Performance",
      "Thermochromism"
    ],
    "doi": "10.1145/3519391.3522751",
    "url": "https://doi.org/10.1145/3519391.3522751",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "267–275",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3519392",
    "title": "SocialSlider: Changing the Transparency of Avatars",
    "authors": [
      "Anna E. M. Wolf",
      "Jens Reinhardt",
      "Marco Kurzweg",
      "Katrin Wolf"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Due to the growing number of users, social virtual reality can get crowded. As we sometimes prefer to be only surrounded by friends or more enjoy empty galleries to have a better view, we designed SocialSlider: an interaction technique that allows us to manipulate other avatars’ transparency. We implemented SocialSlider with two transparency control modes and four avatar selection techniques: (1) select a single avatar, (2) select all, (3) select a co-located group, or (4) select a semantic group, such as unknown users. Through a user study, we found that (A) The concept of SocialSlider is appreciated and promising to overcome issues of crowded VR, such as occlusion. (B) The avatar’s level of transparency can affect their perceived co-presence. Thus, we show that fading avatars’ visibility is a beneficial technique in multi-user VR with benefits beyond avoiding occlusion, such as controlling the felt privacy or the experience of being social.",
    "keywords": [
      "Virtual Reality",
      "multi-user VR",
      "occluded view",
      "social VR"
    ],
    "doi": "10.1145/3519391.3519392",
    "url": "https://doi.org/10.1145/3519391.3519392",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "276–283",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3519391.3524173",
    "title": "DynamicSkin: Bio-inspired Scaled Sleeve for Body Temperature Regulation and Dynamic Self-Expression",
    "authors": [
      "Yuanhao Zhu",
      "Annice Lee",
      "Ziyue Hu",
      "Hsin-Liu (Cindy) Kao"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "This paper presents DynamicSkin, a bio-inspired scaled sleeve that offers body heat regulation for the wearer while serving as a channel for self-expression and making a fashion statement. We present the design process and system design enabling the dynamic scaled sleeve, exploring the meaning of unobtrusive wearable technology and beautification of features that are often chosen to be hidden instead. This research aims to advance body crafts by exploring a wearable that benefits the human body while providing ample aesthetic value for the user.",
    "keywords": [
      "Clothing",
      "Skin electronics",
      "Textile",
      "Wearable Computing"
    ],
    "doi": "10.1145/3519391.3524173",
    "url": "https://doi.org/10.1145/3519391.3524173",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "284–286",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524174",
    "title": "Investigate the Piano Learning Rate with Haptic Actuators in Mixed Reality",
    "authors": [
      "Likun Fang",
      "Reimann Malte",
      "Erik Pescara",
      "Michael Beigl"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "With mixed reality (MR) becoming widely available, it could enhance learning because special equipment like musical instruments or access to instructors will be less of a concern. Furthermore, passive haptic learning systems to learn piano are promising research subjects. We combine both trends of MR and haptic learning to build a piano learning application. Through a study with diverse participants, we evaluate the piano application. The study results show the potentiality of the on-skin actuators and we hope our work could foster the future iterations of the actuators for a fun and effective learning environment.",
    "keywords": [
      "Haptics",
      "Human-Computer-Interaction",
      "Mixed Reality",
      "Wearable"
    ],
    "doi": "10.1145/3519391.3524174",
    "url": "https://doi.org/10.1145/3519391.3524174",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "287–290",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524175",
    "title": "Towards Underwater Augmented Reality Interfaces to Improve the Navigation Experience",
    "authors": [
      "Ewa Anna Szyszka",
      "Kai Kunze"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "In this paper, we present initial work towards evaluating augmented reality interfaces to enhance underwater navigation. We propose a conceptual framework that combines real-time GPS coordinates fetched from an Aqua-Fi module with computer vision approach to overlay a first-person view (FPV) rendering real-time AR-generated arrows pointing in the direction of the exit point of the dive. The system will allow the diver to trace the progression of the dive and easily find the way out in low visibility and high turbidity conditions. We present an initial requirements analysis based on test dives of the researchers to understand the problem domain better and an initial proposed system with early feasibility tests. We are testing an integrated AR system (inertial motion sensing, GPS and Computer Vision, DolphinSLAM&nbsp;[6]) with visual feedback for a first test, yet are considering also haptic and other modalities for interaction.",
    "keywords": [
      "AR-based navigation",
      "Aquatic navigation",
      "SLAM"
    ],
    "doi": "10.1145/3519391.3524175",
    "url": "https://doi.org/10.1145/3519391.3524175",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "291–293",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524026",
    "title": "Introducing a Concept of Gamification to Microscopic Suturing Training",
    "authors": [
      "Yuka Tashiro",
      "Mikihito Matsuura",
      "Dong-Hyun Hwang",
      "Shio Miyafuji",
      "Satoshi Kiyofuji",
      "Taichi Kin",
      "Takeo Igarashi",
      "Hideki Koike"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Microscopic suturing in neurosurgery is a difficult medical technique that requires time to master. Since skilled neurosurgeons are busy and have little time to train novice surgeons, novice surgeons must train alone in monotonous tasks to acquire the skill. This paper proposes a training system that incorporates gamification elements, such as scoring and displaying feedback, to increase motivation for the training. The system detects the technical factors required for microsurgery from the microscope training videos and calculates the scoring using those elements. This paper formulates important elements for microsurgery and discusses the detection method. We also implement scoring and visual feedback to make the user aware of these elements.",
    "keywords": [
      "CV-based tracking",
      "Gamification",
      "Neurosurgical training",
      "Visual feedback"
    ],
    "doi": "10.1145/3519391.3524026",
    "url": "https://doi.org/10.1145/3519391.3524026",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "294–297",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524027",
    "title": "FroggyHand: A Gesture Based Control System for Omni-Directional Projections",
    "authors": [
      "Jana Hoffard",
      "Shio Miyafuji",
      "Jefferson Pardomuan",
      "Toshiki Sato",
      "Hideki Koike"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We would like to introduce FroggyHand, a novel interactive display which projects images onto a webbed skin between the user’s fingers. Applications connected to these images are then activated via hand tracking and gesture recognition. We present the implementation steps as well as small, prototype applications and discuss future plans.",
    "keywords": [
      "Augmented Reality",
      "HCI design and evaluation methods",
      "Human Computer Interaction (HCI)",
      "Human-centered computing"
    ],
    "doi": "10.1145/3519391.3524027",
    "url": "https://doi.org/10.1145/3519391.3524027",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "298–300",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524176",
    "title": "Effect of Switching from a Teleoperated Excavator with Different Size on Work Efficiency",
    "authors": [
      "Junya Masunaga",
      "Masaru Ito",
      "Chiaki Raima",
      "Yuzuki Okawa",
      "Ryota Sekizuka",
      "Seiji Saiki",
      "Yoichiro Yamazaki",
      "Yuichi Kurita"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "In recent years, the number of people involved in the construction industry has been decreasing, and it is necessary to improve productivity per worker. Teleoperated excavators are being introduced as a solution to this problem. This makes it possible to instantly switch between excavators of the appropriate size according to the work to be performed. Different excavators are different dynamic characteristics and arm lengths, which may affect the operation after switching. In this study, we created an excavator switching simulator and investigated the effects of switching. As a result of excavation work experiments with a 13-ton class excavator and a 20-ton class excavator, it was confirmed that switching increased the work time and decreased the bucket tip speed.",
    "keywords": [
      "hydraulic excavator",
      "teleoperation"
    ],
    "doi": "10.1145/3519391.3524176",
    "url": "https://doi.org/10.1145/3519391.3524176",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "301–304",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524029",
    "title": "T2Snaker: a Robotic Coach for Table Tennis",
    "authors": [
      "Kodai Fuchino",
      "Mohammed Al-Sada",
      "Tamon Miyake",
      "Tatsuo Nakajima"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "The restrictions imposed by the Covid-19 pandemic has significantly affected all aspects of daily life, especially human contact. Accordingly, an essential aspect of human contact is for training and skill acquisition, which is difficult to conduct under such restrictions. Therefore, we developed T2Snaker, a table tennis training system that comprises a robotic appendage to guide user’s hand movements within a VR environment. T2Snaker’s novelty lies in its flexibility to guide users movements, yet as it is not directly attached to the user’s limbs, it does not impose restrictions on their movements like traditional exoskeleton systems. We explain the implementation specifics of T2Snaker and discuss its preliminary evaluation that focused on table-tennis skill acquisition. The results show that T2Snaker has high potential in skill acquisition, and users praised is ability to guide their movements and proposed various potential application domains. We discuss some design insights based on our work and present future research directions.",
    "keywords": [
      "Appendages",
      "Augmentation",
      "Education",
      "Robotic",
      "Serpentine",
      "Snake;",
      "Table Tennis",
      "VR"
    ],
    "doi": "10.1145/3519391.3524029",
    "url": "https://doi.org/10.1145/3519391.3524029",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "305–308",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524031",
    "title": "Developing a wearable Augmented Reality for treating phantom limb pain using the Microsoft Hololens 2",
    "authors": [
      "Cosima Prahm",
      "Michael Bressler",
      "Korbinian Eckstein",
      "Hideaki Kuzuoka",
      "Adrien Daigeler",
      "Jonas Kolbenschlag"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "After the amputation of a limb, up to 90% of the patients report a feeling of the missing body part still being present in their perception. This effect is known as phantom limb sensation and ranges from the simple feeling of presence to a specific position, shape or phantom pain. To alleviate this pain, patients engage in mirror therapy during which a mirror is placed in front of the patient’s midsection, and the patient, while looking into this mirror, imagines that the amputated limb is in fact the healthy limb reflected in the mirror. However, this method is not without limitations. To address these limitations of conventional Mirror Therapy therapy and take advantage of the inherent potential of commercial technology, we developed an innovative assistive therapy tool based on mixed reality (MR) on the Microsoft Hololens 2. In this way, the patient’s residual limb would be augmented by a superimposed virtual arm that is completely independent from the movements of their sound limb and can interact with the environment. Patients should move around freely and perform bi-manual tasks. To achieve this, we developed an MR appliaction that includes controllers for moving the virtual hand via myoelectric sensors (EMG) and inertial units and created and 4 different interaction scenes. We anticipate that this type of immersive MR rehabilitation will have a positive impact on outcomes related to pain scores, hand/arm functionality, range of motion, and motivation to perform therapy, even when no therapist is present.",
    "keywords": [
      "arm amputation",
      "augmented reality",
      "hand amputation",
      "hololens",
      "mirror therapy",
      "mixed reality",
      "prosthesis"
    ],
    "doi": "10.1145/3519391.3524031",
    "url": "https://doi.org/10.1145/3519391.3524031",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "309–312",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524169",
    "title": "A Model for Selecting Media Type of Memory Cues in Ubiquitous Prostheses",
    "authors": [
      "Passant Elagroudy",
      "Sebastian Feger",
      "Albrecht Schmidt"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Memories shape our interactions with the world. Thus, there is a new trending research direction to build memory prostheses that help us augment our cognitive capabilities and become better versions of ourselves. Such prostheses alter our memories through augmentation or degradation. Nevertheless, there is a research gap in systematic processes to design such systems. In this work, we reflect on our experience designing several memory prostheses and present a two-dimensional design space to contextualize existing literature. We also present eight criteria to choose media types to capture, store and present memory cues. Our work is one step forward to identifying elements of a holistic framework to build targeted memory prostheses.",
    "keywords": [
      "design space",
      "media design",
      "memory cues",
      "memory prostheses"
    ],
    "doi": "10.1145/3519391.3524169",
    "url": "https://doi.org/10.1145/3519391.3524169",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "313–316",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524033",
    "title": "Improving Motivation by Increasing Tactile Sensitivity to Skin Conditions during Skin Care using Stochastic Resonance",
    "authors": [
      "Mone Konno",
      "Shigeo Yoshida",
      "Tomohiro Amemiya",
      "Takuji Narumi"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "As it can take up to several months for recognizable skin improvement, it is difficult to realize the improvement of a skin condition in daily skin care. In this study, with the aim of making it easier to realize the improvement in skin condition and obtain motivating effect of skin care, we developed a system to enhance tactile sensitivity to skin conditions. We used the stochastic resonance to enhance tactile sensitivity to roughness by short-time exposure of minute white-noise vibrations to the fingertips. Additionally, we found that the use of this system in actual skin care routine helped the users to feel the improvement in their skin and improved their motivation.",
    "keywords": [
      "Improving Motivation",
      "Skin Care",
      "Skin Conditions",
      "Stochastic Resonance",
      "Tactile Sensitivity"
    ],
    "doi": "10.1145/3519391.3524033",
    "url": "https://doi.org/10.1145/3519391.3524033",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "317–319",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524177",
    "title": "Effects of Body Duplication and Split on Body Schema",
    "authors": [
      "Ryota Kondo",
      "Maki Sugimoto"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Visual-tactile synchronization or visual-motor synchronization makes a not innate body feel as if it was one’s own body (illusory body ownership). Although recent studies have shown that body ownership can be induced in multiple bodies, it is unclear whether the body ownership is switched between bodies, or we feel that multiple bodies belong to us simultaneously. If the body ownership is induced in multiple bodies simultaneously, we expected the body schema to be extended to correspond to the number of bodies. In this study, we investigated whether body ownership is induced in multiple bodies simultaneously by measuring the body schema when the bodies are replicated and split. Experiment 1 investigated whether two side-by-side virtual bodies moved synchronously with the participant’s movements to extend the body schema. In Experiment 2, virtual split bodies in which a virtual body was split into left and right sides were presented. As a result, the body schema was extended only in the split bodies. However, it changed even in the no body ownership condition, suggesting that the appearance of the split bodies simply affected the body schema, not that body ownership of the split bodies occurred at the same time. The results of the questionnaire indicated that the split bodies might be perceived as a single body.",
    "keywords": [
      "body ownership",
      "body schema",
      "embodiment",
      "multiple bodies"
    ],
    "doi": "10.1145/3519391.3524177",
    "url": "https://doi.org/10.1145/3519391.3524177",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "320–322",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524034",
    "title": "Deep Learning–Based Perceptual Stimulus Encoder for Bionic Vision",
    "authors": [
      "Lucas Relic",
      "Bowen Zhang",
      "Yi-Lin Tuan",
      "Michael Beyeler"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Retinal implants have the potential to treat incurable blindness, yet the quality of the artificial vision they produce is still rudimentary. An outstanding challenge is identifying electrode activation patterns that lead to intelligible visual percepts (phosphenes). Here we propose a perceptual stimulus encoder (PSE) based on convolutional neural networks (CNNs) that is trained in an end-to-end fashion to predict the electrode activation patterns required to produce a desired visual percept. We demonstrate the effectiveness of the encoder on MNIST using a psychophysically validated phosphene model tailored to individual retinal implant users. The present work constitutes an essential first step towards improving the quality of the artificial vision provided by retinal implants.",
    "keywords": [
      "deep learning",
      "retinal implants",
      "stimulus optimization"
    ],
    "doi": "10.1145/3519391.3524034",
    "url": "https://doi.org/10.1145/3519391.3524034",
    "citations": 15,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "323–325",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3519391.3524180",
    "title": "“This” and “That” in Teleshopping with Possessive Telepresence Systems using 5G Mobile Networks",
    "authors": [
      "Takayoshi Yamada",
      "Kelvin Cheng",
      "Soh Masuko",
      "Keiichi Zempo"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "With the growth of communication technology, it has become possible to use telepresence systems to allow people to feel present when they are in a remote location. The way that people in remote locations share knowledge and act together toward people in local locations can be thought of as the possession of people in remote locations by people in local locations. Therefore, by using possession, users can consume the service as if they were two people. In this paper, we propose a possession-based teleshopping system that allows a person with the intention to purchase something in a remote location to possess a person wearing an MR-HMD in a local location, thereby enabling a local purchasing experience. To carry out this study, we visited a clothes shop, a convenience store, and a movie theater using two patterns of communication lines over 4G and 5G, where the remote possessor instructed the local possessee to do the shopping. The fieldwork revealed that the possessee could respond flexibly to the possessor’s directive words such as “this” and “that” due to the ease of sharing visibility. In addition, because the possessee was able to accurately convey visual information such as the price and color of the product, the possessor was able to make smoother purchasing decisions, and the possessee was able to make payments and perform other tasks without delay. Furthermore, the relationship between the possessor and the possessee was such that they were able to compensate for each other’s lack of knowledge. As a result, the possession system makes it possible to perform a task with the knowledge of two people, even though they appear to be one person face-to-face with other people in economic activities.",
    "keywords": [
      "Mixed Reality",
      "Possession system",
      "Telepresence",
      "Teleshopping"
    ],
    "doi": "10.1145/3519391.3524180",
    "url": "https://doi.org/10.1145/3519391.3524180",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "326–329",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524036",
    "title": "Embodied Noise – Towards Augmenting the Dart-Throwing Practice over a Sleeve with Randomized Haptic Actuation",
    "authors": [
      "Takuro Nakao",
      "Keitaro Tsuchiya",
      "Shinya Shimizu",
      "Megumi Isogai",
      "Kai Kunze"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Effective training lives from a large degree of variation according to physiological research principles of deliberate practice. In this paper, we introduce a novel way of training by adding kinetic noise (randomized haptic vibrations) to the training process. Over the vibrations, we make the training process more difficult and hope to add variation (an important aspect of skill training in the psychology literature). We present an initial study (n=8) of dart-throwing training as a system test. Four users train with a sleeve (random vibro-tactile and thermal actuation) and 4 users are the control group (no sleeve). Both groups improve significantly over the training period (p &lt;.05). Yet,the effect size is too low to make any claims regarding effectiveness between the groups. Still we believe our approach could be promising, we will explore in future work how to enhance skill acquisition training over randomized haptic actuation.",
    "keywords": [
      "Human Computer Interaction",
      "Human Sense",
      "Noise",
      "Training"
    ],
    "doi": "10.1145/3519391.3524036",
    "url": "https://doi.org/10.1145/3519391.3524036",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "330–333",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524037",
    "title": "Shared Wind: Creating Awareness by Nature-related Ambient Design for Emotional Communication",
    "authors": [
      "Xuqin Yu",
      "Masa Inakage"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "In this paper, we discussed a Long-distance relationship (LDR) design system for subtle emotional communication. In-the-wild user studies with qualitative interviews were conducted. The results suggest that slowness, content-less, limitation, and ambiguity that are elements often less considered in interaction design, actually are valuable for creating the sense of awareness and companionship between LDR groups.",
    "keywords": [
      "Ambient Technology",
      "Awareness Strategy",
      "Human-Centered Design",
      "LDR Design",
      "Mediated intimacy",
      "Relatedness",
      "Subtle Emotional Design"
    ],
    "doi": "10.1145/3519391.3524037",
    "url": "https://doi.org/10.1145/3519391.3524037",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "334–336",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524179",
    "title": "Towards Applying Pneumatic Gel Muscles to Augment Plantar Flexor Muscle Stretching for Children with Cerebral Palsy",
    "authors": [
      "Zilan Chen",
      "Sujuan Wang",
      "Swagata Das",
      "Yuichi Kurita",
      "Takashi Goto",
      "Chun Zhai",
      "Lei Xu",
      "Kai Kunze"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Many children in the world suffer from cerebral palsy (CP), which is the leading cause of children’s disabilities, affecting their body movement and muscle coordination [18]. At present, the rehabilitation training of children with cerebral palsy has long relied on the physical therapist (PT) workforce. Our research wants to empower parents of children suffering from cerebral palsy to engage in simple training exercises themselves, supporting the rehabilitation process. We collaborate with physical therapists of a children’s hospital to design a system that uses pneumatic artificial muscles (PAMs) to help children with cerebral palsy train their plantar flexor muscles (Figure 1). We present an initial experimental prototype validated with physical therapists and doctors. We also show a feasibility test with patients with informed consent from parents and monitored by the physical therapists.",
    "keywords": [
      "Home rehabilitation",
      "artificial muscles",
      "children with cerebral palsy",
      "soft exoskeleton"
    ],
    "doi": "10.1145/3519391.3524179",
    "url": "https://doi.org/10.1145/3519391.3524179",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "337–340",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524028",
    "title": "Telepresence Robot with Novel Stereoscopic Camera Configuration",
    "authors": [
      "Yasuyuki Inoue",
      "Michiteru Kitazaki"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "A novel stereoscopic camera configuration which has two movable wide-lens cameras and one stationary fisheye-lens camera is proposed to build ”view-sharing” telepresence robot for multiple users. The camera system provide 180-deg monocular field-of-view obtained from the stationary fish-eye camera for more than two users, and provide additional stereoscopic information of parallax for at most two users independently. The parallax is obtained from the movable cameras which revolves around the fish-eye camera according to each user’s head motion so that the inter-lens distance keeps constant length, and stereoscopic view of remote environment is provides for each user. We demonstrate some applications using the proposed system such as body-sharing experience, joint attention via shared-view, and improvement of sense of presence.",
    "keywords": [
      "avatar",
      "omni-directional camera",
      "stereopsis",
      "telepresence"
    ],
    "doi": "10.1145/3519391.3524028",
    "url": "https://doi.org/10.1145/3519391.3524028",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "341–343",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524030",
    "title": "StressMincer: Enhancement of Catharsis Effect by Visualization of Words and Destruction with Haptic Feedback",
    "authors": [
      "Taiki Takami",
      "Shohei Ando",
      "Shota Nakayama",
      "Asahi Saito",
      "Yui Suga",
      "Takumi Hamazaki",
      "Yutaro Yano",
      "Hiroyuki Kajimoto"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "We are constantly exposed to mental stress in our daily lives, especially under the COVID-19 pandemic. To cope with this issue, we developed StressMincer, an interactive system that combines two stress-relieving factors: expression of negative emotion and destruction. The users obtain catharsis effect by putting negative emotions into words, and a force feedback mechanism enables users to destroy these words by their actions. Three different destructive experiences were developed; shredding, burning by flamethrower, and flushing with toilet water.",
    "keywords": [
      "Cathartic Effect",
      "Mental Health",
      "Stress",
      "Tactile Interface",
      "VR/AR",
      "Voice Recognition"
    ],
    "doi": "10.1145/3519391.3524030",
    "url": "https://doi.org/10.1145/3519391.3524030",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "344–346",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3519391.3524035",
    "title": "Alju Dress: Pets as a metaphor for wearable display designs",
    "authors": [
      "Caglar Genc",
      "Paula Roinesalo",
      "Jonna Häkkilä"
    ],
    "year": 2022,
    "conference": "AHS",
    "conferenceYear": "AHs '22",
    "abstract": "Alju is inspired by the pet-owner relationship and mimics exotic birds living on our bodies. It is a dress that interacts with sudden changes in its surroundings by reacting to the sounds around it. It indicates moderate and high noise levels by imitating bird wings moving on the garment in different velocities. Just like a pet, it acts ambiguously and requires us to pay attention to its intentions and try to interpret why it acts the way that it does. In this regard, our work aims to present pets as a metaphor for designing wearable displays and transferring animal-like behaviors to a garment to replicate emotional communication between pets and their owners, easing the way the wearer perceives the challenges of wearable displays automatically modifying her appearance.",
    "keywords": [
      "Ambient Displays",
      "Fashion",
      "Pet",
      "Self-expression",
      "Social Wearables",
      "Wearable Displays"
    ],
    "doi": "10.1145/3519391.3524035",
    "url": "https://doi.org/10.1145/3519391.3524035",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2022",
    "pages": "347–350",
    "publisher": "Association for Computing Machinery",
    "location": "Kashiwa, Chiba, Japan",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582728",
    "title": "Designing Interactive Shoes for Tactile Augmented Reality",
    "authors": [
      "Dennis Wittchen",
      "Valentin Martinez-Missir",
      "Sina Mavali",
      "Nihar Sabnis",
      "Courtney N. Reed",
      "Paul Strohmeier"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Augmented Footwear has become an increasingly common research area. However, as this is a comparatively new direction in HCI, researchers and designers are not able to build upon common platforms. We discuss the design space of shoes for augmented tactile reality, focussing on physiological and biomechanical factors as well as technical considerations. We present an open source example implementation from this space, intended as an experimental platform for vibrotactile rendering and tactile AR and provide details on experiences that could be evoked with such a system. Anecdotally, the new prototype provided experiences of material properties like compliance, as well as altered perception of their movements and agency. We intend our work to lower the barrier of entry for new researchers and to support the field of tactile rendering in footwear in general by making it easier to compare results between studies.",
    "keywords": [
      "haptic footwear",
      "haptic rendering",
      "vibrotactile augmentation"
    ],
    "doi": "10.1145/3582700.3582728",
    "url": "https://doi.org/10.1145/3582700.3582728",
    "citations": 17,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "1–14",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "14",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582704",
    "title": "GAuze-MIcrosuture-FICATION: Gamification in Microsuture training with real-time feedback",
    "authors": [
      "Yuka Tashiro",
      "Shio Miyafuji",
      "Dong-Hyun Hwang",
      "Satoshi Kiyofuji",
      "Taichi Kin",
      "Takeo Igarashi",
      "Hideki Koike"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Microscopic suturing in neurosurgery is a challenging medical technique that requires time to master. Since skilled surgeons are too busy to spend much time with novice surgeons, novice surgeons need to train alone in monotonous tasks to acquire skills. To address this problem, this study proposes a system that incorporates gamification elements, such as scoring and displaying real-time feedback, to improve motivation for training. This system detects the technical factors necessary for microscopic suturing from video capture, and calculates a score using these factors. According to neurosurgeons, suturing has three important factors: speed, accuracy, and carefulness. These technical factors are detected by tracking instruments and gauze using machine learning and image processing. The experiment was conducted with ten novices using this system. The results of this experiment showed that the system is easy to use and contributed to increased motivation, according to the User Experience Questionnaire (UEQ) and System Usability Scale (SUS).",
    "keywords": [
      "CV-based tracking",
      "Gamification",
      "Neurosurgical training",
      "Real-time feedback",
      "Visual feedback"
    ],
    "doi": "10.1145/3582700.3582704",
    "url": "https://doi.org/10.1145/3582700.3582704",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "15–26",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582711",
    "title": "Standing Balance Improved by Electrical Muscle Stimulation to Popliteus Muscles",
    "authors": [
      "Masato Shindo",
      "Arinobu Niijima"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Fall prevention is extremely important when it comes to ensuring the well-being of an aging population. There are currently two main fall prevention strategies: wearing exoskeletons to assist with postural stability and using electrical muscle stimulation (EMS) for training the lower limb muscles. However, the former has issues regarding size and weight, and the latter does not have immediate improvement effects. In this paper, we propose a small and lightweight EMS-based system that immediately improves standing balance by applying EMS to the popliteus muscles behind the knees, which leads to unlocking and bending the knees. Bending the knees lowers the center of mass of the human body and stabilizes the standing balance against unexpected perturbations. We conducted a user study with 20 participants to evaluate the proposed system and found that the postural sway decreased significantly more when EMS was applied within 200&nbsp;ms after perturbation occurrence compared to when no EMS was applied.",
    "keywords": [
      "electrical muscle stimulation",
      "fall prevention",
      "postural control"
    ],
    "doi": "10.1145/3582700.3582711",
    "url": "https://doi.org/10.1145/3582700.3582711",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "27–34",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582701",
    "title": "Tactile Vectors for Omnidirectional Arm Guidance",
    "authors": [
      "Hesham Elsayed",
      "Martin Weigel",
      "Johannes Semsch",
      "Max Mühlhäuser",
      "Martin Schmitz"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We introduce and study two omnidirectional movement guidance techniques that use two vibrotactile actuators to convey a movement direction. The first vibrotactile actuator defines the starting point and the second actuator communicates the endpoint of the direction vector. We investigate two variants of our tactile vectors using phantom sensations for 3D arm motion guidance. The first technique uses two sequential stimuli to communicate the movement vector (Sequential Tactile Vectors). The second technique creates a continuous vibration vector using body-penetrating phantom sensations (Continuous Tactile Vectors). In a user study (N = 16), we compare these two new techniques with state of the art push and pull metaphors. Our findings show that users are 20% more accurate in their movements with sequential tactile vectors.",
    "keywords": [
      "movement guidance",
      "vibrotactile",
      "wearables"
    ],
    "doi": "10.1145/3582700.3582701",
    "url": "https://doi.org/10.1145/3582700.3582701",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "35–45",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582713",
    "title": "Coldness Presentation to Ventral Forearm using Electrical Stimulation with Elastic Gel and Anesthetic Cream",
    "authors": [
      "Taiga Saito",
      "Takumi Hamazaki",
      "Seitaro Kaneko",
      "Hiroyuki Kajimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "To augment and enhance VR experience, various devices have been proposed to provide thermal sensations. In particular, Peltier devices are commonly used to induce cold sensations. However, these devices are unsuitable for long-term use due to high energy consumption. This study investigates the use of electrical stimulation to generate thermal sensation in the arm for future wearable applications. Due to its small size and low power requirements, electrical stimulation is unlikely to interfere with body movement or disrupt immersion. Furthermore, providing thermal sensation to the arm is expected to enhance immersion in VR content without interfering with hand movements. In addition, tactile sensation can also be presented by the electrical stimulation. However, electrical stimulation to the arm normally cannot provide a stable temperature sensation because pain threshold is too close to tactile and temperature threshold. We tackled this issue by two ways; one is by applying a gel layer to the arm to suppress the pain sensation by diffusing the current, and the other is by using local anesthetic cream. As a result, we found that electrical stimulation to the arm generated a cold sensation at several points out of 61 electrodes in both cases. The results of the evaluation experiments revealed that stimulation pulse width and polarity of electrical stimulation gave little effect, while there seems to be a trend that anodic stimulation using the gel tended to generate a cold sensation at relatively high intensity, and cathodic stimulation using either the gel and local anesthetic cream tended to have cold sensation over a wider area.",
    "keywords": [
      "Cold Sensation",
      "Electrical Stimulation",
      "Forearm"
    ],
    "doi": "10.1145/3582700.3582713",
    "url": "https://doi.org/10.1145/3582700.3582713",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "46–54",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582709",
    "title": "Virtual Omnibus Lecture: Investigating the Effects of Varying Lecturer Avatars as Environmental Context on Audience Memory",
    "authors": [
      "Takato Mizuho",
      "Tomohiro Amemiya",
      "Takuji Narumi",
      "Hideaki Kuzuoka"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We propose a novel form of remote lecturing, Virtual Omnibus Lecture, in which one lecturer gives a lecture by changing his/her virtual avatars, giving students the experience of being lectured by multiple lecturers. Because previous studies have shown that learning in various environmental contexts can improve memory performance, we posit that learning from various lecturers virtually can have a similar effect. To examine this concept, we divided a 90-minute lecture into four sessions and conducted a between-participant experiment to compare the four-avatar condition, where the lecturer taught using avatars with different appearances for each session, to the one-avatar condition, where the lecturer used a single avatar through all sessions. The results showed that the comprehension testing scores after the lecture were significantly higher for the four-avatar condition than for the one-avatar condition. This result suggests that multiple avatars provide a promising education method beyond traditional in-person/online lectures.",
    "keywords": [
      "avatar",
      "environmental context",
      "memory",
      "multiple-context effect",
      "remote lecture"
    ],
    "doi": "10.1145/3582700.3582709",
    "url": "https://doi.org/10.1145/3582700.3582709",
    "citations": 10,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "55–65",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582712",
    "title": "LocatAR: An AR Object Search Assistance System for a Shared Space",
    "authors": [
      "Hiroto Oshimi",
      "Monica Perusquía-Hernández",
      "Naoya Isoyama",
      "Hideaki Uchiyama",
      "Kiyoshi Kiyokawa"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Item-finding tasks due to memory lapse are costly activities commonly experienced by many people. However, conventional systems are not suitable for use in a collaborative environment. Therefore, we propose a multi-functional, pre-registration-free, and 3D location-based item management system. The system has two main functions: registration and search. The automatic registration is performed by image-based item movement recognition from the user’s grasping and placing motions. The registered item movement data comprises the item category, and the start and end locations. We ensure privacy protection by storing item movement data without images. Also, we provide a user interaction to refuse to share the items with other users. The search is based on the item list or item location. The location-based search is performed by specifying where the user last saw the item. To optimize and test the performance of the system, we first performed parameter optimization and then conducted a user study investigating the performance of a search task. The parameter optimization performed in the registration system led to the discovery of optimal values that are difficult to reach empirically. The search experiment showed that the proposed system’s search and guidance functions are effective as an assistance system for finding items, both in terms of search time and user experience. Overall, our system demonstrated the potential to be a useful assistance system for managing items in a shared space. We further discuss the possibility of further exploiting the limited registered information by treating item location as an identifier of the moved item.",
    "keywords": [
      "area-based search",
      "hand-object interaction",
      "lost objects",
      "object discovery"
    ],
    "doi": "10.1145/3582700.3582712",
    "url": "https://doi.org/10.1145/3582700.3582712",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "66–76",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582705",
    "title": "Generation of realistic facial animation of a CG avatar speaking a moraic language",
    "authors": [
      "Ryoto Kato",
      "Yusuke Kikuchi",
      "Vibol Yem",
      "Yasushi Ikei"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We propose a new method for generating real-time realistic facial animation using face mesh data corresponding to the fifty-six C+V (Consonant and Vowel) type morae that form the basis of Japanese speech. This method produces facial expressions by weighted addition of fifty-three face meshes based on the mapping of voice streaming to registered morae in real-time. Both photogrammetric models and existing off-the-shelf head models can be used as face meshes. Natural facial expressions of speech can be synthesized from a modeling to live animation in less than two hours. The results of a user study of our method showed that the facial expression during Japanese speech was more natural than popular real-time methods to generate facial animation, English-base Oculus Lipsync and volume intensity based facial animations.",
    "keywords": [
      "Real-time facial animation",
      "Realistic avatar",
      "Speech expression",
      "Virtual human"
    ],
    "doi": "10.1145/3582700.3582705",
    "url": "https://doi.org/10.1145/3582700.3582705",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "77–85",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582703",
    "title": "Effect of Weight Adjustment in Virtual Co-embodiment During Collaborative Training",
    "authors": [
      "Daiki Kodama",
      "Takato Mizuho",
      "Yuji Hatada",
      "Takuji Narumi",
      "Michitaka Hirose"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Acquisition of motor skills plays an essential role in various contexts, such as sports, factory jobs, and nursing. For effective motor skill learning, “virtual co-embodiment” has been proposed as a novel virtual reality (VR) based method in which a virtual avatar is controlled based on the weighted average of the learner’s and teacher’s movements. Using virtual co-embodiment, a learner can learn the motor intention because they can feel a strong sense of agency in the avatar’s movements modified by the teacher. However, after the assistance of the virtual co-embodiment vanishes, there is a performance drop problem; the learner cannot move as they learned, even if they understand the correct movement or motor intention, because the difference in body positions between the co-embodied avatar and the learner requires the latter to move differently after termination of the assistance. One way to match their positions is to increase the weight assigned to the learner’s weight. However, simply assigning the learner a high weight does not allow the teacher to correct the avatar’s movements and convey the correct movement and motor intention. By allowing the teacher a greater influence in the early stages of learning, and decreasing the influence as the learning progresses, it is expected to gradually allow the student to learn to operate independently. Therefore, we propose a method to prevent performance drop by adjusting weights according to the learning performance, thereby maintaining a high learning efficiency and helping advanced learners learn to independently demonstrate their abilities. In this study, we experimented with dual task learning to evaluate the automation of movement, which is considered an essential element of motor skills. We compared the performance drop when the virtual co-embodiment assist was terminated with static or adjusted weights based on the performance. Consequently, although the learning efficiency was slightly lower, the use of adjusted weights resulted in a significantly smaller performance drop after the termination of virtual co-embodiment assistance than that after the use of static weight.",
    "keywords": [
      "Adjustment",
      "Collaborative Training",
      "Motor Skill Learning",
      "Virtual Co-embodiment"
    ],
    "doi": "10.1145/3582700.3582703",
    "url": "https://doi.org/10.1145/3582700.3582703",
    "citations": 9,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "86–97",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 2
  },
  {
    "id": "10.1145/3582700.3582717",
    "title": "Workspace Scaling in Virtual Reality based Robot Teleoperation",
    "authors": [
      "Bukeikhan Omarali",
      "Shafiq Javaid",
      "Maurizio Valle",
      "Ildar Farkhatdinov"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We explore how human-operators perform manual scaling of virtual reality (VR) scenes used to represent remote physical environments in robotic telemanipulation tasks. In our experiment, 15 human-participants were asked to use manual gesture-based navigation in a VR scene that allowed them to change the virtual-to-real scale of the virtual world and perform a simple pick-and-place task in supervised robot control mode. We have compared the virtual world scale of participants at the beginning of a 3-day experiment when they were considered to be novice teleoperators and at the end of it when they were considered to be expert teleoperators. Expert teleoperators as a group used a smaller virtual world scale that allowed them to perform the experimental task faster than novices, although this behaviour was not exhibited by every teleoperator individually. Our study also demonstrated that participants’ prior video gaming experience affects the virtual world scale as participants with video gaming experience used smaller virtual world scales and performed the experimental task faster.",
    "keywords": [
      "human-operator’s perception",
      "robot teleoperation",
      "virtual reality"
    ],
    "doi": "10.1145/3582700.3582717",
    "url": "https://doi.org/10.1145/3582700.3582717",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "98–104",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582716",
    "title": "Challenges in Virtual Reality Studies: Ethics and Internal and External Validity",
    "authors": [
      "Sarah Delgado Rodriguez",
      "Radiah Rivu",
      "Ville Mäkelä",
      "Florian Alt"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "User studies on human augmentation nowadays frequently involve virtual reality (VR) technology. This is because VR studies allow augmentations of the human body or senses to be evaluated virtually without having to develop elaborate physical prototypes. However, there are many challenges in VR studies that stem from a multitude of factors. In this paper, we first discuss different types of VR studies and suggest high-level terminology to facilitate further discussions in this space. Then, we derive challenges from the literature that researchers might face when conducting research with VR technology. In particular, we discuss ethics, internal validity, external validity, the technological capabilities of VR hardware, and the costs of VR studies. We further discuss how the challenges might apply to different types of VR studies, and formulate recommendations.",
    "keywords": [
      "Challenges",
      "Ethics",
      "User Study",
      "Validity",
      "Virtual Reality",
      "Virtual Reality Studies"
    ],
    "doi": "10.1145/3582700.3582716",
    "url": "https://doi.org/10.1145/3582700.3582716",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "105–111",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582720",
    "title": "Effects of Wearing Knee-tightening Devices and Presenting Shear Forces to the Knee on Redirected Walking",
    "authors": [
      "Gaku Fukui",
      "Takuto Nakamura",
      "Keigo Matsumoto",
      "Takuji Narumi",
      "Hideaki Kuzuoka"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Natural and realistic walking experience in the virtual environment is critical in increasing immersion in virtual reality. Redirected walking (RDW) was proposed to improve the walking experience, and many studies were conducted to expand its detection threshold (DT). In this study, we investigated the effect on redirected walking when the knee was subjected to the hanger reflex, which creates the illusion of a strong force by applying a shearing force to the skin. The results suggest that the hanger reflex at the knee affects gait, but contrary to expectations, the hanger reflex at the knee did not affect the DT of RDW, and wearing the device on the knee itself expands the DT. Although the results are not what one would expect, the findings of this study provide an interesting perspective for more effective RDW in the future.",
    "keywords": [
      "Detection Threshold",
      "Hanger Reflex",
      "Redirected Walking",
      "Virtual Reality"
    ],
    "doi": "10.1145/3582700.3582720",
    "url": "https://doi.org/10.1145/3582700.3582720",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "112–121",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582714",
    "title": "LUNAChair: Remote Wheelchair System Linking Users to Nearby People and Assistants",
    "authors": [
      "Luna Takagi",
      "Shio Miyafuji",
      "Jefferson Pardomuan",
      "Hideki Koike"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "With the growing need for wheelchairs, much wheelchair research has focused on wheelchair control, such as fully automatic driving and remote control. For wheelchair users, conversations with caregivers and others around the wheelchair are also important and must be considered. Therefore, we propose a system that connects the users to remote caregivers and others by using omnidirectional displays. Our system allows remote caregivers to observe both the users and their surroundings and to control the wheelchair, while the users and others feel the existence of remote caregivers. We conducted a system evaluation and user interviews to demonstrate that the proposed system augments wheelchair users’ experience.",
    "keywords": [
      "Care",
      "Communication",
      "Eye Gaze",
      "Remote Control",
      "Spherical Display",
      "Telepresence",
      "Wheelchair"
    ],
    "doi": "10.1145/3582700.3582714",
    "url": "https://doi.org/10.1145/3582700.3582714",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "122–134",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "13",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582715",
    "title": "Visuospatial abilities and cervical spine range of motion improvement effects of a non-goal-oriented VR travel program at an older adults facility:A pilot randomized controlled trial",
    "authors": [
      "Atsuko Miyazaki",
      "Takashi Okuyama",
      "Hayato Mori",
      "Kazuhisa Sato",
      "Kenta Toshima",
      "Atsushi Hiyama"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Virtual reality (VR) programs using head-mounted displays (HMD) give older adults the opportunity for unrestricted spatial exploration, with greater movement. We conducted a single-blind randomized control trial study to determine the beneficial effects of a non-goal-directed VR traveling program using an HMD on older adults living in an assisted living facility that also caters for people with dementia. Twenty-four participants, with an average age of 88.5 years, were randomly assigned to the VR program group or the control group. The VR group participated in three weekly VR travel sessions of 30 minutes each, for four weeks. The results showed that the VR group not only experienced improvement in simple visuospatial abilities, but also in tasks involving executive function. There was also improvement in vertical and horizontal cervical spine range of motion. However, cervical range of motion in lateral bending was worse in the VR group than in the control group, suggesting a possible effect of the weight of the HMD. Training in wide visual exploration that improves cervical spine range of motion may provide an orientation opportunity to avoid falls among older adults.",
    "keywords": [
      "cervical spine range of motion",
      "immersive virtual reality",
      "older adults",
      "visuospatial ability"
    ],
    "doi": "10.1145/3582700.3582715",
    "url": "https://doi.org/10.1145/3582700.3582715",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "135–146",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582702",
    "title": "Exploration of Sonification Feedback for People with Visual Impairment to Use Ski Simulator",
    "authors": [
      "Yusuke Miura",
      "Erwin Wu",
      "Masaki Kuribayashi",
      "Hideki Koike",
      "Shigeo Morishima"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Training opportunities for visually impaired (VI) skiers are limited because it is essential for them to have sighted people who guide them with their voices. This study investigates an auditory feedback system that enables ski training using a ski simulator for VI skiers alone. Based on the results of interviews with actual VI skiers and their guides, we designed the following three types of sounds: 1) a single sound (ATS: Advance Turn Sound) that conveys information about turns; 2) a continuous sound (CES: Continuous Error Sound) that is emitted according to the difference between the user’s future position and the position he/she should progress to; and 3) a single sound (Gate-Passed Sound) which is emitted when a user passed through a gate. We conducted an evaluation experiment with four blind skiers and three sighted guides. Results showed that three out of four skiers performed better under the conditions in which ATS and gate-passed sound were emitted than the condition in which a human guide gave calls. The result suggests that a sonification-based method such as ATS is effective for ski training on the ski simulator for VI skiers.",
    "keywords": [
      "ski simulator",
      "ski training",
      "sonification",
      "visual impairment"
    ],
    "doi": "10.1145/3582700.3582702",
    "url": "https://doi.org/10.1145/3582700.3582702",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "147–158",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582718",
    "title": "DecluttAR: An Interactive Visual Clutter Dimming System to Help Focus on Work",
    "authors": [
      "Kaito Yokoro",
      "Monica Perusquia-Hernandez",
      "Naoya Isoyama",
      "Hideaki Uchiyama",
      "Kiyoshi Kiyokawa"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Over time, the desks of office workers often get cluttered with documents, items, and other objects that are unrelated to the work being done. These objects become visual noise. Visual noise tends to reduce the worker’s concentration on their work. Therefore, it is desirable to have an uncluttered environment to work. We propose DecluttAR, a system that controls the visibility of clutter objects by using a video see-through head-mounted display and an RGB-D camera. Our system first generates a 3D model of the empty desk, and then automatically generates a 3D model of each object that is newly placed on the desk. Then, the user can interactively control each object’s visibility which is superimposed on the corresponding real object. Four types of object appearance adjustments are implemented: normal, grayscale, outline, and transparency. To investigate changes in the ability to concentrate in different appearances, a user study was conducted in which a simple arithmetic task was performed in four appearances. The results show that the number of answers in the arithmetic task for the transparent display increased compared to the normal display in the first half of the task. This result indicates that the participants are able to enter a state of concentration on the task more quickly by making the object invisible. In addition, the questionnaire-based subjective evaluation revealed that the transparent display had a lower workload and a higher user evaluation followed by the outline display.",
    "keywords": [
      "Diminished Reality",
      "Mixed Reality",
      "Visual Noise",
      "Work Support System"
    ],
    "doi": "10.1145/3582700.3582718",
    "url": "https://doi.org/10.1145/3582700.3582718",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "159–170",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582706",
    "title": "ShadowClones: an Interface to Maintain a Multiple Sense of Body-space Coordination in Multiple Visual Perspectives",
    "authors": [
      "Kazuma Takada",
      "Nanako Kumasaki",
      "Tom Froese",
      "Kazuhisa Shibata",
      "Jun Nishida",
      "Shunichi Kasahara"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In this paper, we propose ShadowClones, an interface that supports interactions in which a single user can interact with multiple bodies in multiple spaces. Recent teleoperation technologies have allowed a user controlling multiple objects simultaneously, but at the same time, it also exhibited a significant challenge, which can be attributed to the high cognitive load caused by switching and recogning various spaces/perspectives repeatedly and instantly. To tackle this challenge, by taking advantage of pre-attentive visual cues for users’ simultaneous information processing, we designed and evaluated a new user interface, called Shadow Clones, that projects self-body information in unattended areas for increasing the awareness of body-space relationships and allowing users to seamlessly switch across different visual perspectives from avatars or remote robots. We then explored the proposed approach through a simple visual reaching task with a performance evaluation in terms of task completion time and success rate. The results showed superior performance when compared with a condition that presents no projections of users’ body movements in unattended areas. We conclude by discussing possible mechanisms of this enhancement as well as two potential scenarios using the shadow clones approach, including new entertainment content for virtual reality e-sports and multiple robot teleoperation such as in a construction site or a disaster site, without compromising operational performance.",
    "keywords": [
      "motor control",
      "multiple body projection",
      "multiple spatial perception",
      "parallel interaction",
      "unattended visual processing"
    ],
    "doi": "10.1145/3582700.3582706",
    "url": "https://doi.org/10.1145/3582700.3582706",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "171–178",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "8",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582710",
    "title": "AI Coach: A Motor Skill Training System using Motion Discrepancy Detection",
    "authors": [
      "Chen-Chieh Liao",
      "Dong-Hyun Hwang",
      "Erwin Wu",
      "Hideki Koike"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Spatial and temporal clues found in a professional’s motion are essential for designing a training system for learning a motor skill. We investigate the potential of using neural networks to learn spatial and temporal features of advanced players in sports and to detect the fine-grained differences between motions. As a training system, we implement an AI Coach prototype application that finds the differences between two input motions and visualizes a recommendation motion for the users to correct their forms. In the user study, we investigate the effects of the proposed AI Coach and discuss the findings based on quantitative questionnaires and qualitative interviews. In the study, the proposed system can help the user better understand the difference between them and the coach. The study also reveals the necessity of coaching beginners in the early learning phases.",
    "keywords": [
      "Motor skill training",
      "discrepancy detection",
      "golf swing",
      "neural networks"
    ],
    "doi": "10.1145/3582700.3582710",
    "url": "https://doi.org/10.1145/3582700.3582710",
    "citations": 14,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "179–189",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582707",
    "title": "CC-Glasses: Color Communication Support for People with Color Vision Deficiency Using Augmented Reality and Deep Learning",
    "authors": [
      "Zhenyang Zhu",
      "Jiyi Li",
      "Ying Tang",
      "Kentaro Go",
      "Masahiro Toyoura",
      "Kenji Kashiwagi",
      "Issei Fujishiro",
      "Xiaoyang Mao"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "People who suffer from color vision deficiency (CVD) can face difficulties when communicating with others by failing to identify target objects referred by their color names. While most existing studies on CVD compensation have focused on the issue of color contrast loss. Although there are approaches can provide clues of color name to users, these techniques either require training, or cannot protect users’ privacy, i.e., the fact of having CVD. In this paper, based on augmented reality (AR) and deep learning technologies, we propose a novel system to provide supporting information to users affected by CVD for color communication assistance. The state-of-the-art deep neural network (DNN) model for referring segmentation (RS) is adopted to generate supporting information, and AR glasses are utilized for information presentation. To improve the performance of the proposed system further, a new dataset is constructed based on a novel concept called Color–Object Noun Pair. The results of evaluation experiments show that the new dataset can enhance the performance of the adopted DNN model, and the proposed system can help users affected by CVD successfully identify target objects by their color names.",
    "keywords": [
      "artificial intelligence",
      "augmented reality",
      "color vision deficiency assistance",
      "referring segmentation"
    ],
    "doi": "10.1145/3582700.3582707",
    "url": "https://doi.org/10.1145/3582700.3582707",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "190–199",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582722",
    "title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models",
    "authors": [
      "Kazuki Kawamura",
      "Jun Rekimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user’s condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear. This method can be used to produce fast but intelligible speech. In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.",
    "keywords": [
      "deep neural network",
      "playback speed",
      "self-supervised learning",
      "speech recognition",
      "video information"
    ],
    "doi": "10.1145/3582700.3582722",
    "url": "https://doi.org/10.1145/3582700.3582722",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "200–208",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582725",
    "title": "Exoskeleton for the Mind: Exploring Strategies Against Misinformation with a Metacognitive Agent",
    "authors": [
      "Yeongdae Kim",
      "Takane Ueno",
      "Katie Seaborn",
      "Hiroki Oura",
      "Jacqueline Urakami",
      "Yuto Sawa"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Misinformation is a global problem in modern social media platforms with few solutions known to be effective. Social media platforms have offered tools to raise awareness of information, but these are closed systems that have not been empirically evaluated. Others have developed novel tools and strategies, but most have been studied out of context using static stimuli, researcher prompts, or low fidelity prototypes. We offer a new anti-misinformation agent grounded in theories of metacognition that was evaluated within Twitter. We report on a pilot study (n=17) and multi-part experimental study (n=57, n=49) where participants experienced three versions of the agent, each deploying a different strategy. We found that no single strategy was superior over the control. We also confirmed the necessity of transparency and clarity about the agent’s underlying logic, as well as concerns about repeated exposure to misinformation and lack of user engagement.",
    "keywords": [
      "Design study",
      "Intelligent agent",
      "Metacognition",
      "Misinformation",
      "Social media"
    ],
    "doi": "10.1145/3582700.3582725",
    "url": "https://doi.org/10.1145/3582700.3582725",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "209–220",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582721",
    "title": "Investigating Effects of Facial Self-Similarity Levels on the Impression of Virtual Agents in Serious/Non-Serious Contexts",
    "authors": [
      "Masayasu Niwa",
      "Katsutoshi Masai",
      "Shigeo Yoshida",
      "Maki Sugimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Recent technological advances have enabled the use of AI agents to assist with human tasks and augment human cognitive abilities in a variety of contexts, including decision making. It is critical that users trust these AI agents in order to use them effectively. Given that people tend to trust other people who are similar to themselves, incorporating features of one’s own face into the AI agent’s face may improve one’s trust in the AI agent. However, it is still unclear how impressions differ when comparing agents with the same appearance as one’s own and some similarities under the same conditions. Recognizing the appropriate level of similarity when using a self-similar agent is important for establishing a trustworthy agent relationship between people and the AI agent. Therefore, we investigated the effect of the degree of self-similarity of the face of the AI agent on the user’s trust in the agent. We examined users’ impressions of four AI agents with different degrees of face self-similarity in different scenarios. The results showed that the AI agent, whose similarity to the user’s facial feature was slightly recognizable but not obvious, received higher ratings on the feeling of closeness, attractiveness, and facial preferences. These self-similar AI agents were also more trustworthy in everyday non-serious decisions and were more likely to improve people’s trustworthiness in such situations. Finally, we discuss the potential applications of our findings to design real-world AI agents.",
    "keywords": [
      "AI",
      "Agent",
      "Decision-making",
      "Self-Similarity"
    ],
    "doi": "10.1145/3582700.3582721",
    "url": "https://doi.org/10.1145/3582700.3582721",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "221–230",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582727",
    "title": "Affective Umbrella – A Wearable System to Visualize Heart and Electrodermal Activity, towards Emotion Regulation through Somaesthetic Appreciation",
    "authors": [
      "Kanyu Chen",
      "Jiawen Han",
      "Holger Baldauf",
      "Ziyue Wang",
      "Dunya Chen",
      "Akira Kato",
      "Jamie A Ward",
      "Kai Kunze"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In this paper, we introduce Affective Umbrella, a novel system to record, analyze and visualize physiological data in real time via an umbrella handle. We implement a biofeedback loop design in the system that triggers visualization changes to reflect and regulate emotions through somaesthetic appreciation. We report the methodology, processes, and results of data reliability and visual feedback impact on emotions. We evaluated the system using a real-life user study (n=21) in rainy weather at night. The statistical results demonstrate the potential of applying the visualization of biofeedback to regulate emotional arousal with a significantly higher (p=.0022) score, a lower (p=.0277) dominance than baseline from self-reported SAM Scale, and physiological arousal, which was shown to be significantly increased (p&lt;.0001) with biofeedback in terms of pNN50 and a significant difference in terms of RMSSD. There was no significant difference in terms of emotional valence changes from SAM scale. Furthermore, we compared the difference between two biofeedback patterns (mirror and inversion). The mirror effect was with a significantly higher emotional arousal than the inversion effect (p=.0277) from SAM results and was with a significantly lower RMSSD performance than the inversion effect (p&lt;.0001). This work demonstrates the potential for capturing physiological data using an umbrella handle and using this data to influence a user’s emotional state via lighting effects.",
    "keywords": [
      "affective computing",
      "biofeedback loop",
      "emotion regulation",
      "interaction design",
      "physiological signal",
      "somaesthetic appreciation design",
      "wearable sensing"
    ],
    "doi": "10.1145/3582700.3582727",
    "url": "https://doi.org/10.1145/3582700.3582727",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "231–242",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582719",
    "title": "TOMURA: A Mountable Hand-shaped Interface for Versatile Interactions",
    "authors": [
      "Shigeo Yoshida",
      "Tomoya Sasaki",
      "Zendai Kashino",
      "Masahiko Inami"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We introduce TOMURA, a mountable hand-shaped interface for conducting a wide range of interactions by leveraging the versatility of the human hand. TOMURA can be mounted in any number of locations and orientations on the body and environment. By combining freedom in mounting with the versatile expressivity of the human hand, TOMURA enables interactions that integrate elements of shape-changing interfaces and wearable robots. For example, TOMURA can be worn to the wrist to assist in grasping and to enable haptic interactions with a remote operator. By placing TOMURA on the desk, it can be used as a physical avatar representing a remote user’s hand during a video meeting. We illustrated TOMURA’s design space and demonstrated the feasibility of our concept by implementing a prototype and employing it in several application scenarios. We then discussed the possibilities and limitations of the prototype based on user feedback.",
    "keywords": [
      "hand-shaped interface",
      "haptics",
      "human augmentation",
      "robotic interface",
      "telecommunication"
    ],
    "doi": "10.1145/3582700.3582719",
    "url": "https://doi.org/10.1145/3582700.3582719",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "243–254",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582726",
    "title": "DUMask: A Discrete and Unobtrusive Mask-Based Interface for Facial Gestures",
    "authors": [
      "Arpit Bhatia",
      "Aryan Saini",
      "Isha Kalra",
      "Manideepa Mukherjee",
      "Aman Parnami"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Interactions using the face, not only enable multi-tasking but also enable us to create hands-free applications. Previous works in HCI used sensors attached directly to the person’s face or inside their mouth. However, a mask, which has now become a norm in our everyday life and is socially acceptable, has rarely been used to explore facial interactions. We designed, “DUMask”, an interface that uses face parts covered by a mask to discretely enable 14 (+1 default) interactions. DUMask uses an infrared camera embedded inside an off-the-shelf face mask to recognize the gestures, and we demonstrate the effectiveness of our interface through in-lab studies. We conducted two user studies evaluating the experience of both the wearer and the onlooker, which validated that the interface is indeed inconspicuous and unobtrusive.",
    "keywords": [
      "Facial Gestures",
      "Mask",
      "Wearables"
    ],
    "doi": "10.1145/3582700.3582726",
    "url": "https://doi.org/10.1145/3582700.3582726",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "255–266",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582723",
    "title": "Dynamic Derm: Body Surface Deformation Display for Real-World Embodied Interactions",
    "authors": [
      "Ryo Murata",
      "Arata Horie",
      "Masahiko Inami"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "The body surface is an essential interface that dynamically reflects states inside and outside the body. To realize a computer mediated embodied interaction, focusing on its characteristic as a visual display, we propose dynamically intervening in the shape of the body surface. In this paper, we define the design requirement for a system that deforms the body surface, organize the design space, and build a prototype. Dynamic Derm is a prototype that dynamically deforms clothes by pushing them up from inside, where and each module can present two degrees of freedom in translation. We investigated the spatial accuracy of the system, the actuator response under load and a clothes deformation as a basic technical evaluation of the system. We also designed several presentation scenarios based on the design space, and conducted a qualitative evaluation of the adequacy of their representations.",
    "keywords": [
      "Body Surface Deformation",
      "Embodied Display",
      "Real-World Interactions",
      "Shape-Changing Device"
    ],
    "doi": "10.1145/3582700.3582723",
    "url": "https://doi.org/10.1145/3582700.3582723",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "267–277",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582724",
    "title": "Human Coincident Robot: A Non-contact Surrounding Robot Sharing the Coordinate with a Human Inside",
    "authors": [
      "Takafumi Watanabe",
      "Tomoya Sasaki",
      "Zendai Kashino",
      "Masahiko Inami"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "The use of wearable robots is gaining traction in the field of human augmentation due to their potential to augment human physical capabilities. However, the design and specification of these robots are often constrained by the physical limitations of the human body. This paper proposes a novel approach called the Human Coincident Robot (HCR), which maintains a fixed positional and rotational relationship with a human without physical contact using a mobile mechanism. We verify the feasibility of this concept through the design and implementation of a two-wheeled mobile robot controlled by sliding mode control. Our experiments with the prototype demonstrate that the implemented system can be used in a situation where a human walks naturally and suggest that the HCR has the potential to overcome the limitations of conventional wearable robots and provide new opportunities for human augmentation.",
    "keywords": [
      "autonomous mobile robot",
      "human augmentation",
      "wearable robots"
    ],
    "doi": "10.1145/3582700.3582724",
    "url": "https://doi.org/10.1145/3582700.3582724",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "278–286",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582708",
    "title": "Tablet Cutting Board: Tablet-based Knife-control Support System for Cookery Beginners",
    "authors": [
      "Hatsune Masuda",
      "Kunihiro Kato",
      "Kaori Ikematsu",
      "Yoshinari Takegawa",
      "Keiji Hirata"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "It is difficult for novice cooks to cut food evenly and quickly with a knife. In this study, we propose a learning support system for kitchen knife skills using a tablet terminal. The system aims to help novice cooks cut food evenly and vertically. The system has a function to display guide lines for cutting food, and a function to detect the movement of the knife using a touch panel and provide real-time feedback on whether the width of the cut food is appropriate or not. In addition, an inertial sensor was attached to the knife to estimate the tilt of the knife based on acceleration data, and a function was implemented to provide feedback on whether the knife is cutting at an appropriate angle or not. A prototype system was implemented, and the accuracy of estimating the width of the food and the accuracy of tilt detection were measured when the prototype system was used to cut the food. It was found that the prototype system was able to estimate the width of the food with an average error of 0.5 mm and the tilt of the knife with an average error of 0.5 degrees.",
    "keywords": [
      "Cookery beginner",
      "learning support.",
      "tablet PC"
    ],
    "doi": "10.1145/3582700.3582708",
    "url": "https://doi.org/10.1145/3582700.3582708",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "287–293",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583695",
    "title": "Rapport Building via a Digital Avatar with a “Voice” Entering into a Customer’s Personal Space",
    "authors": [
      "Azusa Yamazaki",
      "Naoto Wakatsuki",
      "Koichi Mizutani",
      "Yukihiko Okada",
      "Keiichi Zempo"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In this paper, we attempt to extend the interpersonal skills of salespeople in a virtual store through operations unique to virtual environments (VEs) that are not possible in a real environment. Utilizing the superiority of visual information over auditory information, we created an interpersonal situation in which only the sound image of an avatar invades the user’s personal space (PS). We investigated the impressions that 16 participants had of this avatar. The results showed that when the sound image invaded the user’s PS and approached the user, rapport significantly increased (p&lt;0.05) for the questionnaire item “I have a harmonious relationship with this salesperson,” while rapport significantly decreased (p&lt;0.05) for the questionnaire item “I feel comfortable engaging with this salesperson.” These findings suggest that the manipulation of sound image position may allow the salesperson to manipulate the impression held by the customer and there is potential for improving the interpersonal skills of salespeople. The method proposed in this paper, in which the position of the visual and sound image diverge, is expected to improve rapport building and thereby the value cocreation of the interpersonal service in VEs, such as virtual stores, which have been increasing in recent years.",
    "keywords": [
      "interpersonal service",
      "personal space",
      "service on metaverse",
      "ventriloquism effect"
    ],
    "doi": "10.1145/3582700.3583695",
    "url": "https://doi.org/10.1145/3582700.3583695",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "294–297",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583696",
    "title": "Comparative Evaluation of Joystick and Leaning-Based Locomotion in Immersive Telepresence with Body Motion Feedback",
    "authors": [
      "Shogo Shimada",
      "Yukiya Ojima",
      "Yusuke Kikuchi",
      "Yasushi Ikei",
      "Vibol Yem"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Some previous studies have focused on the effects of leaning-based methods and motion feedback for reducing cybersickness and improving usability during locomotion in virtual environments. However, it is actually challenging to combine of these technologies for practical use in the real environments using telepresence robots. In this study, we developed an immersive telepresence system with body motion feedback and comparatively evaluated the effectiveness of using a joystick and a leaning-based method in terms of reducing cybersickness and improving usability during locomotive tasks. The results of the user study (N=8) showed that leaning-based method with body motion feedback is effective for improving comfort and reducing cybersickness compared with the cases of using the joystick. Moreover, the use of body motion display to generate walking like sensation was shown to be effective for improving the telepresence experience.",
    "keywords": [
      "Body Motion",
      "Joystick-Based",
      "Leaning-Based",
      "Locomotion",
      "Telepresence",
      "Virtual Reality"
    ],
    "doi": "10.1145/3582700.3583696",
    "url": "https://doi.org/10.1145/3582700.3583696",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "298–300",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583697",
    "title": "Cast Shadow and Color Change Improve Social Impression of Robotic Massage with Virtual Out-of-Body Experience",
    "authors": [
      "Gakumaru Haraguchi",
      "Michiteru Kitazaki"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Massage therapy has been shown to have numerous benefits, including improvements in physical and mental health. In this study, we examined how integrating virtual reality with a robotic massage system could enhance users' social perception of the robot. We implemented a virtual out-of-body experience in the robotic massage system, allowing users to view their own bodies as avatars and observe the robot's approach. Our hypothesis was that adding visual information, such as a color change or cast shadow motion, could improve the social impression of the robot. The results of our human-subject study demonstrated that the robot was perceived as more competent and warmer when the approach of the robotic arm was accompanied by a gradual change in color, while the robot was evaluated as more comfortable when the approach was accompanied by a cast shadow motion. These findings suggest that the combination of a robotic massage system with virtual reality, such as a virtual out-of-body experience, could improve users' social perception of the robotic system, resulting in a feeling of safety, warmth, and comfort during the massage.",
    "keywords": [
      "Cast shadow",
      "Color",
      "Massage",
      "Out-of-body experience"
    ],
    "doi": "10.1145/3582700.3583697",
    "url": "https://doi.org/10.1145/3582700.3583697",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "301–303",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583699",
    "title": "The Effect of Posture on Virtual Walking Experience Using Foot Vibrations",
    "authors": [
      "Junya Nakamura",
      "Yasushi Ikei",
      "Michiteru Kitazaki"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "The virtual walking systems that do not involve physical movement of the legs has the advantage of being able to be experienced while seated or supine. The sensation of virtual walking can be effectively elicited through the combination of optic flow and rhythmic foot vibrations. However, the effects of posture have yet to be fully understood. In light of this, the present study sought to investigate the effects of posture (standing, sitting, and lying supine) on the virtual walking experience utilizing foot vibrations and optic flow. Our hypothesis posited that the synchronization of foot vibrations would augment the walking sensation even in a seated or supine position. Our findings indicate that synchronized foot vibrations produced the sensation of virtual walking in all three postures, with the standing posture eliciting the strongest virtual walking sensation. No significant differences were observed between the sitting and supine postures. These results suggest that virtual walking systems utilizing foot vibrations have the potential to provide a certain degree of walking experience even for individuals unable to leave their bed.",
    "keywords": [
      "Avatar",
      "Optic flow",
      "Posture",
      "Tactile sensation",
      "Walking"
    ],
    "doi": "10.1145/3582700.3583699",
    "url": "https://doi.org/10.1145/3582700.3583699",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "304–306",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583701",
    "title": "SMArt Bracelet: On-Skin interface for user-regulated breathing and stress management",
    "authors": [
      "Yagmur Dulger",
      "Tulasi Elangovan",
      "Nolan Harrington",
      "Boxin Xu",
      "Hsin-Liu (Cindy) Kao"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In this paper, we present SMArt bracelet, a smart on-skin wearable interface integrated with Shape-Memory Alloy (SMA) springs that is aimed to promote well-being of individuals by allowing self-regulation of stress and anxiety using mindful breathing practices. The SMArt bracelet comprises a woven band with elastic which can be customized for aesthetics and integrated with hardware that allows the bracelet to be programmed to react to elevated stress and anxiety levels of the user. An increase in heart rate triggers the bracelet, offering visual and haptic feedback to remind them to regulate their breathing. Integration of the hardware in the soft woven fabric is the main challenge, but the potential benefits of the on-skin interface in health monitoring and improved well-being present opportunities for further exploration.",
    "keywords": [
      "Stress management",
      "health monitoring",
      "woven interfaces"
    ],
    "doi": "10.1145/3582700.3583701",
    "url": "https://doi.org/10.1145/3582700.3583701",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "307–309",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583703",
    "title": "The Intuitive Jacket: Creating A Wearable Interface Acknowledging the Role of the Body in Trauma Mental Health",
    "authors": [
      "Tor Alexander Bruce",
      "Linda Lightley",
      "Clive Wright",
      "Annessa Rebair",
      "Lars Erik Holmquist"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We present a novel, wearable interface as an investigative tool for digital mental healthcare. Immersive environments for therapeutic interventions can potentially involve the whole body of the user in the experience, but often the interaction is through handheld controllers or virtual buttons. Building on suggestions in a previous study from users of an immersive environment for trauma mental healthcare, we developed a new interface, The Intuitive Jacket, to offer control and personalization to the therapeutic process. Our design consists of a physical jacket, allowing users to perform an emotionally powerful interaction: By touching the region of their heart at the end of the session, they “close the door” to the trauma the have been processing. The jacket contains a conductive thread sensor that communicates the garment wearer's gesture to the software via Bluetooth. It was created via a multidisciplinary collaboration between HCI, Fashion Design and Electronics.",
    "keywords": [
      "Digital Mental Healthcare",
      "Embodiment",
      "Immersive Environments",
      "Investigative Tools",
      "Trauma",
      "Wearable Technology"
    ],
    "doi": "10.1145/3582700.3583703",
    "url": "https://doi.org/10.1145/3582700.3583703",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "310–313",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583704",
    "title": "Effects of the Number of Bodies on Ownership of Multiple Bodies",
    "authors": [
      "Ryota Kondo",
      "Maki Sugimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Virtually reality can induce body ownership (Sense of being one’s own body) of a not innate body or multiple bodies. However, the relationship between the number of bodies and body ownership has not been clarified. In a study that investigated body ownership for one, two, and four virtual bodies, the greater the number of bodies, the closer the body ownership tended to approach the strength of ownership for a single body. Therefore, in the present study, we investigated whether increasing the number of bodies strengthens body ownership for multiple bodies. In the experiment, multiple virtual bodies that moved in synchronization with the participant’s movement were lined up in a single file line, and the participant observed the multiple virtual bodies through a head-mounted display from the position of the hindmost virtual body. We measured body ownership and drifts in self-location. Our results showed that contrary to expectations, the greater the number of bodies, the weaker body ownership.",
    "keywords": [
      "body ownership",
      "embodiment",
      "multiple bodies",
      "self-location"
    ],
    "doi": "10.1145/3582700.3583704",
    "url": "https://doi.org/10.1145/3582700.3583704",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "314–316",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583705",
    "title": "Exploring the Effect of Transfer Learning on Facial Expression Recognition using Photo-Reflective Sensors embedded into a Head-Mounted Display",
    "authors": [
      "Fumihiko Nakamura",
      "Maki Sugimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "As one of the techniques to recognize head-mounted display (HMD) user’s facial expressions, the photo-reflective sensor (PRS) has been employed. Since the classification performance of PRS-based method is affected by rewearing an HMD and difference in facial geometry for each user, the user have to perform dataset collection for each wearing of an HMD to build a facial expression classifier. To tackle this issue, we investigate how transfer learning improve within-user and cross-user accuracy and reduce training data in the PRS-based facial expression recognition. We collected a dataset of five facial expressions (Neutral, Smile, Angry, Surprised, Sad) when participants wore the PRS-embedded HMD five times. Using the dataset, we evaluated facial expression classification accuracy using a neural network with/without fine tuning. Our result showed fine tuning improved the within-user and cross-user facial expression classification accuracy compared with non-fine-tuned classifier. Also, applying fine tuning to the classifier trained with the other participant dataset achieved higher classification accuracy than the non-fine-tuned classifier.",
    "keywords": [
      "facial expression recognition",
      "fine tuning",
      "head-mounted display",
      "photo-reflective sensor"
    ],
    "doi": "10.1145/3582700.3583705",
    "url": "https://doi.org/10.1145/3582700.3583705",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "317–319",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583706",
    "title": "Muscle Synergy Analysis Under Fast Sit-to-stand Assist : A Preliminary Study",
    "authors": [
      "Takahide Ito",
      "Jun-Ichiro Furukawa",
      "Qi An",
      "Jun Morimoto",
      "Yuichi Nakamura"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "With the recent advance in robot technologies and the aging of society, a variety of sit-to-stand (StS) assisting systems have been proposed. In this research, we preliminary measured the performance of the chair assistance that enables StS motions faster movements than conventional assistive chairs. The chair has two actuators for sliding forward and lifting up the seat, and has enough power and speed for enabling fast StS. We analyzed the users’ coordination of muscle activities in terms of the muscle synergy through the experiments. The results show that the users can perform natural sit-to-stand in which muscle synergy patterns similar to those without assists appear. It suggests that the chair device potentially enables natural and fast StS that healthy young people commonly perform.",
    "keywords": [
      "chair robot",
      "human assistance",
      "muscle synergy",
      "sit-to-stand"
    ],
    "doi": "10.1145/3582700.3583706",
    "url": "https://doi.org/10.1145/3582700.3583706",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "320–322",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583707",
    "title": "Gaze Depth Estimation for In-vehicle AR Displays",
    "authors": [
      "Ryusei Uramune",
      "Kazuki Sawamura",
      "Sei Ikeda",
      "Hiroki Ishizuka",
      "Osamu Oshiro"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In our previous study, we proposed a method for judging whether the user is gazing at a semi-transparent virtual object or real objects behind it in augmented reality environments. This paper shows that the accuracy of our method can be improved by selecting the optimal thresholds for the fixation detection. Fourteen participants experienced a virtual reality environment in which there were a transparent subway map and buildings behind it in the distance of 2&nbsp;m and 15&nbsp;m away from each participant, respectively. As a result, the accuracy of our method has achieved 88.3&nbsp;% and improved by 13.8 percentage points from the previous 74.5&nbsp;%.",
    "keywords": [
      "depth estimation",
      "eye tracking",
      "fixation detection",
      "valuation metric",
      "virtual reality"
    ],
    "doi": "10.1145/3582700.3583707",
    "url": "https://doi.org/10.1145/3582700.3583707",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "323–325",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583708",
    "title": "First Bite/Chew: distinguish typical allergic food by two IMUs",
    "authors": [
      "Juling Li",
      "Xiongqi Wang",
      "Junyu Chen",
      "Thad Starner",
      "George Chernyshov",
      "Jing Huang",
      "Yifei Huang",
      "Kai Kunze",
      "Qing Zhang"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Eating or overtaking allergic foods may cause fatal symptoms or even death for people with food allergies. Most current food intake tracking methods are camera-based, on-body sensor-based, microphone based, and self-reported. However, challenges that remain are allergic food detection, social acceptance, lightweight, easy to use, and inexpensive. Our approach leverages the first bite/chew and the corresponding hand movement as an indicator to distinguish typical types of the allergic food. Our initial feasibility study shows that our approach can distinguish six types of food at an accuracy of 89.7% over all four participants’ mixed data. Particularly, our method successfully detected and distinguished typical allergic foods such as burgers (wheat), instant noodles (wheat), peanuts, egg fried rice, and edamame, which can be expected to contribute to not only personal use but also medical usage.",
    "keywords": [
      "diet monitoring",
      "food intake",
      "smart eyewear"
    ],
    "doi": "10.1145/3582700.3583708",
    "url": "https://doi.org/10.1145/3582700.3583708",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "326–329",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583709",
    "title": "Towards Enhancing a Recorded Concert Experience in Virtual Reality by Visualizing the Physiological Data of the Audience",
    "authors": [
      "Xiaru Meng",
      "Yan He",
      "Kai Kunze"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "This work is a first attempt to visualize the social atmosphere of a audience in a VR experience using their recorded the physiological state, and then presents them to another group of audience members, aiming at a transformative perception from individual to collective experience. A virtual environment is built to share the audience’s aesthetic feelings and emotions, creating a novel way of non-verbal communication in performance scenarios that aim at enhancing audio-visual perception through physiological sensing and emotional experience sharing. This experiment was designed to investigate the effect of physiological data-based reproduction of musical performances in VR on affective states.",
    "keywords": [
      "classical music",
      "data visualization"
    ],
    "doi": "10.1145/3582700.3583709",
    "url": "https://doi.org/10.1145/3582700.3583709",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "330–333",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583711",
    "title": "Effect of Salesperson Avatar Automatically Mimicking Customer’s Nodding on the Enjoyment of Conversation in Virtual Environments",
    "authors": [
      "Takumi Wakabayashi",
      "Yukihiko Okada",
      "Keiichi Zempo"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "As the VR industry continues to expand, one area gaining attention is its application in the retail sector. In customer service, a crucial aspect of retail, the skills of a salesperson are vital. However, research on VE in this field is still limited. Imitation behavior is known to be an essential customer service skill. Therefore, this study focuses on the effect of nodding imitation by a salesperson’s avatar on customer service in VE. Consequently, an experiment was conducted in which an avatar of a salesperson that automatically mimics nodding was implemented and provided customer service in a virtual store that simulated a furniture store. The results revealed that when the avatar of the salesperson mimicked nodding, the item of enjoyable interaction aspect of rapport was higher, and a significant difference was observed. This results contribute to the development of customer service within VE and have potential implications for creating effective salesperson avatars.",
    "keywords": [
      "customer service",
      "imitation behavior",
      "nonverbal communication",
      "virtual interaction"
    ],
    "doi": "10.1145/3582700.3583711",
    "url": "https://doi.org/10.1145/3582700.3583711",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "334–337",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3583712",
    "title": "Melting into shadow: Toward less cognitively loaded communication in the dark",
    "authors": [
      "Mizuki Yabutani",
      "Hiroki Uchida",
      "Koichi Mizutani",
      "Naoto Wakatuki",
      "Keiichi Zempo"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "As digital devices become an integral part of daily life, people are increasingly experiencing fatigue from constant exposure to information. For example, videoconferencing has been found to cause fatigue, attributed to the cognitive load caused by constant exposure to the gaze of multiple persons and the effort to acquire limited non-verbal information from visual sources. However, current videoconferencing and VR technologies are being developed to increase the amount of visual information to enhance telepresence, resulting in a higher cognitive load.Therefore, in this study, we focused on low-light environments where the amount of visual information is low, and by using shadows instead of camera images as a communication interface, we developed a system that enables communication with people in remote locations with low cognitive load for long periods of time without disturbing their immersion in other content. We conducted an experiment in which participants were asked to read a manga for 5 minutes while engaging in a brief conversation about the manga by using this system every minute. The results showed that the system significantly reduced vertical eye movement compared to videoconferencing using a display (p &lt; 0.05) and that the system did not interfere with concentration on reading manga more than videoconferencing. These results suggest that this system is more effective than videoconferencing in terms of sharing the same time with person located remotely with minimal cognitive load, even while they are connected for a long time and doing in different activities.",
    "keywords": [
      "dark",
      "low cognitive load",
      "shadow"
    ],
    "doi": "10.1145/3582700.3583712",
    "url": "https://doi.org/10.1145/3582700.3583712",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "338–341",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583713",
    "title": "Brain-Computer Interface using Directional Auditory Perception",
    "authors": [
      "Yuto Koike",
      "Yuichi Hiroi",
      "Yuta Itoh",
      "Jun Rekimoto"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "We investigate the potential of brain-computer interface (BCI) using electroencephalogram (EEG) induced by listening (or recalling) auditory stimuli of different directions. In the initial attempt, we apply a time series classification model based on deep learning to the EEG to demonstrate whether each EEG can be classified by recognizing binary (left or right) auditory directions. The results showed high classification accuracy when trained and tested on the same users. Discussion is provided to further explore this topic.",
    "keywords": [
      "BCI",
      "BMI",
      "Brain-computer Interface",
      "Brain-machine Interface",
      "Directional Auditory Sensation",
      "EEG",
      "Electroencephalogram",
      "Electroencephalography"
    ],
    "doi": "10.1145/3582700.3583713",
    "url": "https://doi.org/10.1145/3582700.3583713",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "342–345",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583954",
    "title": "Instant Difficulty Adjustment: Predicting Success Rate of VR Kendama when Changing the Difficulty Level",
    "authors": [
      "Yusuke Goutsu",
      "Tetsunari Inamura"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "This paper presents a task difficulty adjustment method that allows the user to reach desired success rate instantly using VR technology. We propose a methodology based on a Gaussian process dynamical model (GPDM) to model the user’s skill from a small number of past performance observations, and predict future performance at a targeted difficulty level under consideration of model uncertainty. As a task to be performed within a VR environment, we focus on Kendama (a cup-and-ball sports game), in which the cup size is changeable to adjust the difficulty level. In the experiment, we evaluated the personalized skill model with participants who performed the VR Kendama. Our results indicate that the GPDM-based approach accurately reflects the users’ skills, and the predicted success rate when changing the difficulty level is close to the actual success rate even with a small number of trials. This instant difficulty adjustment can therefore help users to receive a pleasant user experience.",
    "keywords": [
      "Difficulty adjustment",
      "Gaussian process",
      "Stochastic approach",
      "User model",
      "Virtual reality"
    ],
    "doi": "10.1145/3582700.3583954",
    "url": "https://doi.org/10.1145/3582700.3583954",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "346–348",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583955",
    "title": "Hack the Room: Exploring the potential of an augmented reality game for teaching cyber security",
    "authors": [
      "Mikko Korkiakoski",
      "Anssi Antila",
      "Jouni Annamaa",
      "Saeid Sheikhi",
      "Paula Alavesa",
      "Panos Kostakos"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "There is a need for creating new educational paths for beginners as well as experienced students for cyber security. Recently, ethical hacking gamification platforms like Capture the Flag (CTF) have grown in popularity, providing newcomers with entertaining and engaging material that encourages the development of offensive and defensive cyber security skills. However, augmented reality (AR) applications for the development of cyber security skills remain mostly an untapped resource. The purpose of this work-in-progress study is to investigate whether CTF games in AR might improve learning in information security and increase security situational awareness (SA). In particular, we investigate how AR gamification influences training and overall experience in the context of ethical hacking tasks. To do this, we developed a Unity-based ethical hacking game in which participants complete CTF-style objectives. The game requires the player to execute basic Linux terminal commands, such as listing files in folders and reading data stored on virtual machines. Each gameplay session lasts up to twenty minutes and consists of three objectives. The game may be altered or made more challenging by modifying the virtual machines. In a pilot, our game was tested with six individuals separated into two groups: an expert group (N=3) and a novice group (N=3). The questionnaire given to the expert group examined their SA during the game, whereas the questionnaire administered to the novice group measured learning and remembering certain things they did in the game. In this paper we discuss our observations from the pilot.",
    "keywords": [
      "augmented reality",
      "cyber security",
      "educational games"
    ],
    "doi": "10.1145/3582700.3583955",
    "url": "https://doi.org/10.1145/3582700.3583955",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "349–353",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583694",
    "title": "HapticPole: Running Navigation Through Skin Drag and Shape Change",
    "authors": [
      "Frederik Wiehr",
      "Adrien Unger",
      "Antonio Krueger"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Trail running is a challenging type of running on unpaved ground, including rocks, pebbles, and dirt with elevation changes, that is becoming ever more popular. Trail runners encounter risks of falling, rely on more muscle groups to balance, and require awareness and attention to foot placement. Current navigational aids take their eyes away from the trail, forcing them to stop to read a map, or occupy the auditory channel, which could otherwise act as an important mode of hazard detection outside the field of view. We propose HapticPole, a prototype that is designed to transmit information through shape change and skindrag. The 3D printed prototype performs a set of tactile interaction patterns for navigational instructions as will be defined in an elicitation study.",
    "keywords": [
      "navigation for trail running",
      "shape change",
      "skin drag",
      "tactile feedback"
    ],
    "doi": "10.1145/3582700.3583694",
    "url": "https://doi.org/10.1145/3582700.3583694",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "354–356",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583698",
    "title": "Conducting and Mitigating Portable Thermal Imaging Attacks on User Authentication using AI-driven Methods",
    "authors": [
      "Shaun Alexander Macdonald",
      "Norah Mohsen T Alotaibi",
      "Md Shafiqul Islam",
      "Mohamed Khamis"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Thermal cameras have become portable enough to integrate into wearables, such as glasses, and can be used maliciously to infer passwords observing heat traces left on keyboards, keypads and screens. While prior work showed how AI-driven approaches can be used to further enhance the effectiveness of these attacks, we use similar approaches to detect vulnerable interfaces and obfuscate heat traces to defend against thermal attacks. At our Augmented Humans 2023 demo, attendees will have the chance to use a thermal camera to observe thermal traces on a keyboard, and observe how machine learning can both automatically identify keys pressed based and identify, then obfuscate, thermal images of a keyboard to prevent thermal attacks. This demo will provoke thought and discussion about the security risks presented by discrete, wearable thermal cameras and how these risks can be mitigated by both designers and users.",
    "keywords": [
      "Machine Learning",
      "Thermal Attacks",
      "Usable Security"
    ],
    "doi": "10.1145/3582700.3583698",
    "url": "https://doi.org/10.1145/3582700.3583698",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "357–359",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583700",
    "title": "Super-immersive Remote Working via Virtual Reality Controlled Robotics",
    "authors": [
      "Adwait Naik",
      "Sajjad Hussain",
      "Guodong Zhao",
      "Muhammad Imran",
      "Linda Steedman"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "In this paper, we briefly describe a novel VR-robot interface developed (on top of the eNetReality [2] authoring platform by eCom Scotland) to provide users with an enhanced learning experience by enabling them to operate the robot remotely through the VR device. We implemented and tested this interface to control the Franka Emika Panda robotic arm. The main objective of this work is to provide an immersive experience to the user by enabling them to interact with the digital twin of the Franka robot and visualize its actions in the VR device. To achieve this objective, we integrated a haptic feedback device to enable the user to receive feedback while the robotic arm performs the pick and place task. We have extensively tested this interface with a cohort of Electronics and Electrical engineering graduates from Glasgow College, UESTC, China. Our ultimate goal is to extend our interface to support multiple robots.",
    "keywords": [
      "digital twins",
      "immersive visualization",
      "positional tracking"
    ],
    "doi": "10.1145/3582700.3583700",
    "url": "https://doi.org/10.1145/3582700.3583700",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "360–362",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583702",
    "title": "Gaze-Augmented Drone Navigation",
    "authors": [
      "Kayla Pineda",
      "Bhanuka Mahanama",
      "Vikas Ashok",
      "Sampath Jayarathna"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "The use of unmanned aerial vehicles (UAVs) or drones, has significantly increased over the past few years. There is a growing demand in the drone industry, creating new workforce opportunities such as package delivery, search and rescue, real estate, transportation, agriculture, infrastructure inspection, and many others, signifying the importance of effective and efficient control techniques. We propose a scheme for controlling a drone through gaze extracted from eye-trackers, enabling an operator to navigate through a series of way points. Then we demonstrate and test the utility of our approach through a pilot study against traditional controls. Our results indicate gaze as a promising control technique for navigating drones revealing novel research avenues.",
    "keywords": [
      "drones",
      "gaze-based control",
      "navigation"
    ],
    "doi": "10.1145/3582700.3583702",
    "url": "https://doi.org/10.1145/3582700.3583702",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "363–366",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3583710",
    "title": "Phantom Undulations: Remote Physiological Sensing in Abstract Installation Works",
    "authors": [
      "Danny Hynds",
      "Dingding Zheng",
      "Yilin Zhang",
      "Hua Ma",
      "Kirill Ragozin",
      "George Chernyshov",
      "Jamie A. Ward",
      "Tatsuya Saito",
      "Kai Kunze",
      "Kouta Minamizawa"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Phantom Undulations is a mixed-media work in which an artist’s physiological data is being used remotely to manipulate the sounds and visuals of an abstract artifact in a gallery setting. This work relies heavily on the concept of showing the artist’s presence or liveness in an abstract and remote manner through changes in the harmony, rhythm, and timbre of a loosely structured soundscape as well as the physical appearance of the artifact. We propose a method of utilizing real time physiological sensing data through a custom built sensing wristband and accompanying software. This system reads the physiological data of the artist and sends it to the Internet, where it can be received by the artifact anywhere on Earth. In addition to the artist’s physiological data, we also offer a way for the audience to incorporate their own data into the work via several sensing wristbands which will accompany the artifact. Through this collaborative process, we wish to invite the audience to join the artist in manipulating the sonic and visual characteristics of this artifact and create a contrapuntally fluid and responsive musical experience.",
    "keywords": [
      "interactive art",
      "liveness",
      "physiological sensing",
      "remote music",
      "sound art"
    ],
    "doi": "10.1145/3582700.3583710",
    "url": "https://doi.org/10.1145/3582700.3583710",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "367–370",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582729",
    "title": "Exploring the Design Space of Assistive Augmentation",
    "authors": [
      "Suranga Chandima Nanayakkara",
      "Masahiko Inami",
      "Florian Mueller",
      "Jochen Huber",
      "Chitralekha Gupta",
      "Christophe Jouffrais",
      "Kai Kunze",
      "Rakesh Patibanda",
      "Samantha W T Chan",
      "Moritz Alexander Messerschmidt"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "Assistive Augmentation, the intersection of human-computer interaction, assistive technologies and human augmentation, was broadly discussed at the CHI’14 workshop and subsequently published as an edited volume on Springer Cognitive Science and Technology series. In this workshop, the aim is to propose a more structured way to design Assistive Augmentations. In addition, we aim to discuss the challenges and opportunities for Assistive Augmentations in light of current trends in research and technology. Participants of the workshop need to submit a short position paper or interactive system demonstration, which will be peer-reviewed. The selected position papers and demos will kick off a face-to-face discussion at the workshop. Participants will also be invited to extend the workshop discussion into a journal submission to a venue such as the Foundations and Trends in Human-Computer Interaction.",
    "keywords": [
      "Assistive Tech",
      "Augmented Body",
      "Augmented Human",
      "Human Computer Integration"
    ],
    "doi": "10.1145/3582700.3582729",
    "url": "https://doi.org/10.1145/3582700.3582729",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "371–373",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3582700.3582730",
    "title": "Cyborgs on the Road: Workshop on Augmenting Road Users to Quantify their Behaviour",
    "authors": [
      "Ammar Al-Taie",
      "Katharina Margareta Theresa Pöhlmann",
      "Thomas Goodge",
      "Andrii Matviienko",
      "Frank Pollick",
      "Stephen Brewster"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "There is an increasing number of studies evaluating road user (e.g. cyclists or drivers) behaviour in traffic. These are important for informing traffic safety, road infrastructure design and the impact of automated vehicles on traffic. Road user evaluations often involve collecting data such as perceived safety and motion sickness. However, collecting objective, quantified forms of these behaviours is challenging. Experimenters commonly measure these with questionnaires after the study. Finding solutions to collect these data in real-time without relying on participants’ subjective input could result in more rigorous study designs and a better understanding of user behaviour. This workshop aims to gain insights on how road users may be augmented with devices, such as heart rate monitors, in evaluation studies to quantify behaviour on-the-go. The workshop will result in study designs that augment road users to quantify their behaviour, which would inform future research with novel techniques for data collection.",
    "keywords": [
      "Augmentation",
      "Road Users",
      "Traffic",
      "User Study"
    ],
    "doi": "10.1145/3582700.3582730",
    "url": "https://doi.org/10.1145/3582700.3582730",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "374–378",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3582700.3582731",
    "title": "Intelligent Music Interfaces: When Interactive Assistance and Augmentation Meet Musical Instruments",
    "authors": [
      "Jordan Aiko Deja",
      "Bettina Eska",
      "Snehesh Shrestha",
      "Matthias Hoppe",
      "Jakob Karolus",
      "Thomas Kosch",
      "Andrii Matviienko",
      "Andreas Weiß",
      "Karola Marky"
    ],
    "year": 2023,
    "conference": "AHS",
    "conferenceYear": "AHs '23",
    "abstract": "The interactive augmentation of musical instruments to foster self-expressiveness and learning has a rich history. Over the past decades, the incorporation of interactive technologies into musical instruments emerged into a new research field requiring strong collaboration between different disciplines. The workshop \"Intelligent Music Interfaces\" covers a wide range of musical research subjects and directions, including (a) current challenges in musical learning, (b) prototyping for improvements, (c) new means of musical expression, and (d) evaluation of the solutions.",
    "keywords": [
      "Artistic Performance",
      "Augmented Instruments",
      "Music Interfaces",
      "Musical Instruments",
      "Self-Expression"
    ],
    "doi": "10.1145/3582700.3582731",
    "url": "https://doi.org/10.1145/3582700.3582731",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2023",
    "pages": "379–383",
    "publisher": "Association for Computing Machinery",
    "location": "Glasgow, United Kingdom",
    "articleno": "",
    "numpages": "5",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652925",
    "title": "WhisperMask: a noise suppressive mask-type microphone for whisper speech",
    "authors": [
      "Hirotaka Hiraki",
      "Shusuke Kanazawa",
      "Takahiro Miura",
      "Manabu Yoshida",
      "Masaaki Mochimaru",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Whispering is a common privacy-preserving technique in voice-based interactions, but its effectiveness is limited in noisy environments. In conventional hardware- and software-based noise reduction approaches, isolating whispered speech from ambient noise and other speech sounds remains a challenge. We thus propose WhisperMask, a mask-type microphone featuring a large diaphragm with low sensitivity, making the wearer’s voice significantly louder than the background noise. We evaluated WhisperMask using three key metrics: signal-to-noise ratio, quality of recorded voices, and speech recognition rate. Across all metrics, WhisperMask consistently outperformed traditional noise-suppressing microphones and software-based solutions. Notably, WhisperMask showed a 30% higher recognition accuracy for whispered speech recorded in an environment with 80 dB background noise compared with the pin microphone and earbuds. Furthermore, while a denoiser decreased the whispered speech recognition rate of these two microphones by approximately 20% at 30-60 dB noise, WhisperMask maintained a high performance even without denoising, surpassing the other microphones’ performances by a significant margin. WhisperMask’s design renders the wearer’s voice as the dominant input and effectively suppresses background noise without relying on signal processing. This device allows for reliable voice interactions, such as phone calls and voice commands, in a wide range of noisy real-world scenarios while preserving user privacy.",
    "keywords": [
      "microphone",
      "noise suppression",
      "wearable devices",
      "whispering"
    ],
    "doi": "10.1145/3652920.3652925",
    "url": "https://doi.org/10.1145/3652920.3652925",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "1–14",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "14",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652932",
    "title": "Synthetic Visual Sensations: Augmenting Human Spatial Awareness with a Wearable Retinal Electric Stimulation Device",
    "authors": [
      "Valdemar Munch Danry",
      "Laura Chicos",
      "Matheus Fonseca",
      "Ishraki Kazi",
      "Pattie Maes"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Alternating current stimulation of the retina (rACS) can non-invasively induce visual sensations called phosphenes (bright flashes) in the visual field. We explore the use of rACS to elicit visual sensations and explore the use cases of \"seeing\" objects behind the user. We designed a wearable rACS system and conducted a study to understand the visual sensations we could elicit and their efficacy when applied to augmenting a user’s spatial awareness. We found that our device reliably generated synthetic sensations and, when applied in an object avoidance task, significantly augmented users’ awareness of objects approaching them from behind compared to users with no stimulation feedback. Our results demonstrate how future research can use electrical stimulation in wearable systems for sensory enhancement.",
    "keywords": [
      "phosphenes",
      "sensory augmentation",
      "spatial awareness",
      "transcranial electrical stimulation"
    ],
    "doi": "10.1145/3652920.3652932",
    "url": "https://doi.org/10.1145/3652920.3652932",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "15–27",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "13",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652946",
    "title": "Looking From a Different Angle: Placing Head-Worn Displays Near the Nose",
    "authors": [
      "Yukun Song",
      "Parth Arora",
      "Srikanth T. Varadharajan",
      "Rajandeep Singh",
      "Malcolm Haynes",
      "Thad Starner"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Head-worn augmented reality displays such as Engo Eyewear avoid placing the virtual image in the user’s primary position of gaze (PPOG) to allow a clear view of the user’s primary task. Several studies suggest that horizontally offsetting the virtual image toward the ear provides good performance and comfort during different types of tasks. Less research focuses on offsetting the image toward the nose. Extending a previous study with displays positioned at 0°, +10°, +20°, and +30° (defining toward the ear as the positive direction), we run two studies each with four conditions and 12 participants (24 participants total) comparing user comfort at -30°, -20°, -10°, and 0° and -15°, 0°, +15°, and +25°. We follow the previous study’s procedures, using a 30-minute reading task and a video display terminal as an emulated right-eye monocular display with a smartphone-sized field of view (FOV). Comparing the results from all three studies suggests that reading on displays with pixels between -24.6° and +19.6° may be comfortable, with users tolerating negative offsets better than positive.",
    "keywords": [
      "Empirical study that tells us about people",
      "Usability Study",
      "Virtual/Augmented Reality",
      "Wearable Computers"
    ],
    "doi": "10.1145/3652920.3652946",
    "url": "https://doi.org/10.1145/3652920.3652946",
    "citations": 19,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "28–45",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "18",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652948",
    "title": "Future So Bright, Gotta Wear Shades: Lens Tint May Affect Social Perception of Head-Worn Displays",
    "authors": [
      "Sofia Vempala",
      "Joseph Mushyakov",
      "Srikanth Tindivanam Varadharajan",
      "Thad Starner"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Social perception of head worn displays (HWD) can be a barrier to usage. Some designers are attempting to create HWDs that could be mistaken for normal eyeglasses. However, hiding optical combiners in a eyeglass lens is difficult. One option is to tint the lens to match the tranmissivity of the combiner. The darker the shade, the more easily the optics can be hidden. However, how dark can the lenses be before they are perceived poorly by conversational partners, especially indoors? We present two studies with a total of 36 participants observing conversations with actors wearing 100%, 75%, 50%, and 25% transmissive lenses. The results suggest that even 25% opacity may be too much tint for the eyeglasses to be considered unremarkable.",
    "keywords": [
      "Head worn display",
      "optics",
      "social acceptability",
      "user-centered design"
    ],
    "doi": "10.1145/3652920.3652948",
    "url": "https://doi.org/10.1145/3652920.3652948",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "46–51",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652921",
    "title": "Everyday Life Challenges and Augmented Realities: Exploring Use Cases For, and User Perspectives on, an Augmented Everyday Life",
    "authors": [
      "Florian Mathis"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Contextually aware visual, auditory, and haptic interfaces can augment and empower people in their everyday life. However, little is known about the use cases and users’ perspectives on a pervasive augmented reality that augments places and humans. In this paper, we contribute a first step towards an augmented societal future by outlining promising example use cases for assistive mixed reality interfaces (i.e., AssistiveMR). By surveying 60 participants, we found that an augmented reality has the potential to find wide-spread application in a plethora of scenarios, including supporting people in recalling information, disconnecting from reality, and augmenting communication with others using visual augmentations. However, participants expressed concerns regarding the potential high costs associated with errors and raised questions about the social acceptability of augmentations of humans and real-world surroundings. Our exploration of promising use cases for assistive MR augmentations aims at serving as inspiration, motivating researchers to augment, support, and empower individuals in their daily activities.",
    "keywords": [
      "Assistive Mixed Reality",
      "Augmenting Humans",
      "Everyday AR"
    ],
    "doi": "10.1145/3652920.3652921",
    "url": "https://doi.org/10.1145/3652920.3652921",
    "citations": 5,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "52–62",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652941",
    "title": "GestureMark: Shortcut Input Technique using Smartwatch Touch Gestures for XR Glasses",
    "authors": [
      "Juyoung Lee",
      "Minju Baeck",
      "Hui-Shyong Yeo",
      "Thad Starner",
      "Woontack Woo"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "We propose GestureMark, a novel input technique for target selection on XR glasses using smartwatch touch gestures as input. As XR glasses get smaller and lighter, their usage increases rapidly, leading to a higher demand for efficient shortcuts for everyday life. We explored the uses of gesture input on smartwatch touchscreen, including simple swipe, swipe combinations, and bezel-to-bezel (B2B) gesture as an input modality. Through an experiment with 16 participants, we found that while swipe gestures were efficient for four-choice selections, B2B was superior for 16-choice inputs. Feedback mechanisms did not enhance performance but reduced perceived workload. Our findings highlight the potential of integrating smartwatches as secondary input devices for XR glasses.",
    "keywords": [
      "XR glasses",
      "bezel gestures",
      "marking menu",
      "smartwatch input"
    ],
    "doi": "10.1145/3652920.3652941",
    "url": "https://doi.org/10.1145/3652920.3652941",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "63–71",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652943",
    "title": "Holistic Patient Assessment System using Digital Twin for XR Medical Teleconsultation",
    "authors": [
      "Taeyeon Kim",
      "Hyun-Song Kwon",
      "Kyunghyun Cho",
      "Woontack Woo"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Assessing a patient’s social and environmental factors is crucial in medical treatment due to their impact on overall quality of life and treatment outcomes. However, current industrial solutions and research on augmented reality and virtual reality telehealth have primarily focused on training or physician collaboration, thereby overlooking the importance of assessing patients’ non-medical factors in the consulting environment. To address this issue, we propose a holistic patient assessment system in an extended reality (XR) teleconsultation environment, based on clinical evidence from the literature. By utilizing the digital twin (DT) of the patient’s environment and a physical examination assistance tool, healthcare professionals can effectively assess non-medical factors and enhance diagnosis accuracy. Through evaluations by current healthcare professionals, our system has been medically validated for its adequate integration of crucial factors in medicine and potential for real-world deployment. We believe that our system could envision a new line of research in future XR telehealth, particularly in the context of patient-centered and inclusive care.",
    "keywords": [
      "Digital twin",
      "augmented/virtual reality",
      "patient-centered care",
      "telehealth",
      "upstream factor"
    ],
    "doi": "10.1145/3652920.3652943",
    "url": "https://doi.org/10.1145/3652920.3652943",
    "citations": 4,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "72–78",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652945",
    "title": "Exploring the Kuroko Paradigm: The Effect of Enhancing Virtual Humans with Reality Actuators in Augmented Reality",
    "authors": [
      "Émilie Fabre",
      "Jun Rekimoto",
      "Yuta Itoh"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "This study explores the value of using virtual humans (VHs) to mask real entities within augmented reality (AR) interactive environments. We believe that one ultimate solution to bridge the gap between virtual and real worlds is to enable virtual entities to interact with the real world seamlessly and physically. However, unlike major approaches such as displaying haptics directly onto real users, letting virtual entities affect the real world physically is an underrepresented field in AR. To explore this approach, we employ reality actuators, here a robot arm, behind an AR VH as it may improve the sense of social presence and engagement in human-robot interaction systems. We developed an AR system where a VH is overlaid onto a robotic arm participating in a chess game. The preliminary results of our pilot study suggest the system’s significant potential to not only enhance the perceived social presence of VHs but also increase overall presence and user engagement, especially compared to situations where the mechanisms are visible without virtual concealment. We hope this work paves the way for more seamless reality experience between real and virtual worlds.",
    "keywords": [
      "agent",
      "ar",
      "augmented reality",
      "avatar",
      "chess",
      "kuroko",
      "robotics",
      "seamless reality"
    ],
    "doi": "10.1145/3652920.3652945",
    "url": "https://doi.org/10.1145/3652920.3652945",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "79–90",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652947",
    "title": "Real-time Slow-motion : A Framework for Slow-motion Without Deviating from Real-time",
    "authors": [
      "Goki Muramoto",
      "Hiroto Saito",
      "Sohei Wakisaka",
      "Masahiko Inami"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Real-time and slow-motion are incompatible. This is a fundamental incompatibility based on the relationship between recording and playback. We propose Real-time Slow-motion, a framework that allows for the coexistence of both. In a see-through system with a pair of a camera and head-mounted display (HMD), this framework provides temporal editing between camera and display within the short width of the subjective present (i.e. now): the camera inputs, divided into small durations, are alternately distributed on two timelines, then stretched twice on the timelines, and these layers are assigned to the left and right eye displays respectively. The hypothesis is that, under certain conditions, the brain integrates images from both eyes, resulting in a slow-motion experience that does not deviate from real-time (i.e., users experience real-time events with slow-motion mode). This paper describes the implementation and the psychological evaluation experiments and interviews about the realized experience. The framework of Real-time Slow-motion is then generalized, and its applications and limitations are discussed. This research suggests a new field of “Temporal Editing to Humans.”",
    "keywords": [
      "real-time",
      "slow-motion sensory augmentation",
      "time-perception"
    ],
    "doi": "10.1145/3652920.3652947",
    "url": "https://doi.org/10.1145/3652920.3652947",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "91–101",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653028",
    "title": "Identifying Hand-based Input Preference Based on Wearable EEG",
    "authors": [
      "Kaining Zhang",
      "Zehong Cao",
      "Xianglin Zheng",
      "Mark Billinghurst"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Understanding user input preference can improve the user experience, however automatically determining preference can be difficult. In this paper, we designed an EEG-based method for directly evaluating hand-based input preference for touch and mid-air gestures on a smartwatch. We conducted a two-phase experiment, recording EEG data from 18 participants as they performed gestures and captured their ratings (Phase 1) and preference choices (Phase 2) for each gesture. Our analysis uncovered distinct EEG patterns between preferred and non-preferred gestures, including significant differences in Power Spectral Density (PSD), Coherence (Coh), and Sample Entropy (SE) features. When participants engaged with their preferred input gestures, we identified decreased brain activity (PSD) in the central and occipital regions, reduced brain connectivity (Coh) in the delta and alpha bands, and increased brain complexity (SE) in multiple sizes. These insights offer the potential to develop rapid detection of user intent for interactive computing devices by analysing brain signals.",
    "keywords": [
      "EEG",
      "evaluation",
      "interaction input",
      "user input preference"
    ],
    "doi": "10.1145/3652920.3653028",
    "url": "https://doi.org/10.1145/3652920.3653028",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "102–118",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "17",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653059",
    "title": "EarAuthCam: Personal Identification and Authentication Method Using Ear Images Acquired with a Camera-Equipped Hearable Device",
    "authors": [
      "Yurina Mizuho",
      "Yohei Kawasaki",
      "Takashi Amesaka",
      "Yuta Sugiura"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Earphones are now used for longer hours than before with the advancement in wireless technology and miniaturization. In addition, the application of earphones has become more diverse, and opportunities to access highly confidential information through them have increased. We propose a method comprising a hearable device equipped with a small camera for user authentication from ear images. This method improves the security of the hearable device. Ear images are first captured with the camera. The ear regions in the images are then extracted using a mask region-based convolutional neural network. Finally, the user is identified using histograms of oriented gradient features and a support vector machine (SVM). Our method was able to identify 18 participants with an accuracy of 84.1%. Users are authenticated through unsupervised anomaly detection using an autoencoder with an error rate of 8.36%. This method facilitates hands- and eye-free operations without requiring any explicit authentication action by the user.",
    "keywords": [
      "Authentication",
      "Biometric",
      "Ear",
      "Earphones",
      "Hearables",
      "Identification",
      "Security",
      "Wearable Device"
    ],
    "doi": "10.1145/3652920.3653059",
    "url": "https://doi.org/10.1145/3652920.3653059",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "119–130",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652923",
    "title": "iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing",
    "authors": [
      "Mengxi Liu",
      "Hymalai Bello",
      "Bo Zhou",
      "Paul Lukowicz",
      "Jakob Karolus"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Hand-over-face gestures can provide important implicit interactions during conversations, such as frustration or excitement. However, in situations where interlocutors are not visible, such as phone calls or textual communication, the potential meaning contained in the hand-over-face gestures is lost. In this work, we present iFace, an unobtrusive, wearable impedance-sensing solution for recognizing different hand-over-face gestures. In contrast to most existing works, iFace does not require the placement of sensors on the user’s face or hands. Instead, we proposed a novel sensing configuration, the shoulders, which remains invisible to both the user and outside observers. The system can monitor the shoulder-to-shoulder impedance variation caused by gestures through electrodes attached to each shoulder. We evaluated iFace in a user study with eight participants, collecting six kinds of hand-over-face gestures with different meanings. Using a convolutional neural network and a user-dependent classification, iFace reaches 82.58 % macro F1 score. We discuss potential application scenarios of iFace as an implicit interaction interface.",
    "keywords": [
      "Hand-over-Face gesture recognition",
      "Impedance Sensing"
    ],
    "doi": "10.1145/3652920.3652923",
    "url": "https://doi.org/10.1145/3652920.3652923",
    "citations": 8,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "131–137",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "7",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652937",
    "title": "Auditory Interface for Empathetic Synchronization of Facial Expressions between People with Visual Impairment and the Interlocutors",
    "authors": [
      "Takayuki Komoda",
      "Hisham Elser Bilal Salih",
      "Tadashi Ebihara",
      "Naoto Wakatsuki",
      "Keiichi Zempo"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "This study proposes an auditory interface to support empathic communication between people with visual impairment (PVI) and their interlocutors. The interface employs riffs, sequences of repeated short notes, representing the facial expressions of PVI and the interlocutor, encouraging a synchronization of their expressions. This synchronization of facial expressions facilitates empathic communication between PVI and their interlocutors. We conducted a conversation experiment with participants using the proposed system and collected data from both quantitative and qualitative perspectives. The results suggest that the proposed system helps PVI relate empathically to their interlocutors. This study provides new insights and contributes to communication support for PVI.",
    "keywords": [
      "Assistive Technology",
      "Auditory Interface",
      "Empathic Communication",
      "Facial Expression",
      "People with Visual Impairment",
      "Sensory Substitution"
    ],
    "doi": "10.1145/3652920.3652937",
    "url": "https://doi.org/10.1145/3652920.3652937",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "138–147",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "10",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652928",
    "title": "Evaluations of Parallel Views for Sequential VR Search Tasks",
    "authors": [
      "Theophilus Teo",
      "Kuniharu Sakurada",
      "Maki Sugimoto",
      "Gun Lee",
      "Mark Billinghurst"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In collaborative virtual environments, sharing a mutual first-person view can lead to different problem-solving strategies among users. What if all views are controlled and seen by the same user? Could this impact how visual search tasks are performed? This paper explores the effects of a user receiving different numbers of parallel views while solving object search tasks in a virtual environment. We developed a prototype and conducted a pilot study, comparing two (2Heads), four (4Heads), and eight (8Heads) additional views. The results suggested that participants’ behaviors could be influenced by the number of parallel views used to solve tasks. Participants found the experiences with 2Heads and 8Heads unpleasant. In 2Heads, the ability to see additional but partial perspectives discouraged participants from using parallel views. Conversely, in 8Heads, the view’s full and overlapping nature forced participants to use parallel views regardless of their preferences. 4Heads received the least complaints as it provided users with the freedom and flexibility to choose their task-solving strategies. These results were translated into design implications for future development and research involving parallel views.",
    "keywords": [
      "Eye Gaze",
      "Multiple Perspectives",
      "Parallel Experience",
      "Virtual Reality"
    ],
    "doi": "10.1145/3652920.3652928",
    "url": "https://doi.org/10.1145/3652920.3652928",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "148–156",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 1
  },
  {
    "id": "10.1145/3652920.3652933",
    "title": "Social Simon Effect in Virtual Reality: Investigating the Impact of Co-actor Avatar's Visual Representation",
    "authors": [
      "Xiaotong Li",
      "Yuji Hatada",
      "Takuji Narumi"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "When engaging in joint actions in the real world, each individual achieves unconscious coordination with the other by automatically activating representations of the other’s behavior in the motor system. It has been investigated by examining the Social Simon Effect (SSE), which is a spatial stimulus–response interference induced by the presence and active engagement of a co-actor in a joint single response go/nogo task (the joint Simon task). On the other hand, collaborative co-actors are not always perceived in the same way between the real world and virtual reality (VR) as the visual representation of the avatar is thought to affect the perception of others’ presence and actions. In order to obtain design guidelines for a virtual environment (VE) that facilitates collaboration between users, this study looked into (1) whether SSE can occur during joint actions of avatars in a VE; and (2) how the visual representation of the co-actor’s avatar affects SSE; through the joint Simon task performed by two adjacent avatars. The results showed that SSE was induced when the co-actor’s avatar was displayed in full-body or was entirely transparent, but the SSE was weak when only the two hands were visible. These results suggested that participants perceived the full-body avatar as the socially engaged co-actor, representing its motions into their own motor planning. The same phenomenon occurred when the whole body of the co-actor was not visible, which could be attributed to the visibility of response actions and their consequential effects facilitated the occurrence of SSE. In contrast, the hand avatar, with its non-human-like appearance and seeming function independently of the entire body, inhibited the action co-representation process because it might be perceived as an unintentional artificial agent.",
    "keywords": [
      "avatar",
      "collaborative interaction",
      "perception",
      "social simon effect",
      "virtual reality"
    ],
    "doi": "10.1145/3652920.3652933",
    "url": "https://doi.org/10.1145/3652920.3652933",
    "citations": 3,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "157–167",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652935",
    "title": "Multiplexed VR: Individualized Multiplexing Virtual Environment to Facilitate Switches for Group Ideation Creativity",
    "authors": [
      "Masahiro Kamihira",
      "Juro Hosoi",
      "Yuki Ban",
      "Shin'ichi Warisawa"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In virtual reality (VR) meetings, the virtual environment (VE) can be used to support the creativity of idea generation. Based on the previous research, it is expected that a diverse range of ideas can be generated through the switching of the VEs. However, in VR meetings, since multiple individuals share a VE, the switching operations of VEs will interfere with each other; thus, it is considered that the easiness of switching VEs may be suppressed. Therefore, we propose Multiplexed VR, which simultaneously provides each participant with a distinct VE. The results of the brainstorming experiment, including 48 participants (16 groups; three participants in each group), suggested that Multiplexed VR facilitated the switching of VEs and improved the quality of generated ideas. This improvement was attributed to Multiplexed VR enabling a more free and independent approach to VE switching, thereby fostering free thinking. We also observed a unique behavior with Multiplexed VR, termed “zapping”: when participants were stuck on idea generation, they continuously switched VEs in a short time to seek hints, suggesting that Multiplexed VR changed the way people switch VEs. These findings provide new insights into VE designs for more effective idea generation in VR meetings.",
    "keywords": [
      "Creativity",
      "Ideation",
      "VR Meeting",
      "Virtual Brainstorming",
      "Virtual Environment"
    ],
    "doi": "10.1145/3652920.3652935",
    "url": "https://doi.org/10.1145/3652920.3652935",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "168–180",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "13",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652938",
    "title": "VR remote tourism system with natural gaze induction without causing user discomfort",
    "authors": [
      "Shogo Aoyagi",
      "Takayoshi Yamada",
      "Kelvin Cheng",
      "Soh Masuko",
      "Keiichi Zempo"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In VR remote tourism, where users can freely control where they look, gaze induction is necessary for presenting places and things the provider wants to show. In this study, we investigated the relationship between the visibility of gaze induction and the feeling of inhibition through two experiments. In the first experiment, the relationship between the strength of the guidance and the sense of inhibition was verified for gaze induction using Gaze Cue, which visualizes where the guide’s head is pointing, as a method of presenting eye gaze in a situation where a guide is attending locally. In the second experiment, we extended the method by placing an avatar in the absence of a local guide and verified the feasibility of gaze induction using the honeypot effect, in which gaze induction is performed unconsciously. The results showed that the following two methods of gaze induction, weak and natural gaze induction and emphasized gaze induction, were feasible. We obtained insight into the gaze induction method that is comfortable for the user and does not cause a feeling of inhibition.",
    "keywords": [
      "Gaze induction",
      "Remote tourism",
      "Virtual Reality"
    ],
    "doi": "10.1145/3652920.3652938",
    "url": "https://doi.org/10.1145/3652920.3652938",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "181–192",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652939",
    "title": "GUI Presentation Method based on Binocular Rivalry for Non-overlay Information Recognition in Visual Scenes",
    "authors": [
      "Kai Guo",
      "Yuki Shimomura",
      "Juro Hosoi",
      "Yuki Ban",
      "Shin'Ichi Warisawa"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In real-time applications like gaming and augmented reality systems, conveying additional information such as scores, warnings, and guides to users through a graphical user interface (GUI) is crucial, especially when the information is not directly accessible in the current context. However, the GUI overlays the visual scene, posing a challenge to users as they can not fully capture information within the visual scene. To address this challenge, we propose the binocular rivalry user interface (BUI), which leverages the binocular rivalry phenomenon occurring when different images are displayed to each eye, thereby extending human visual capabilities. The effectiveness of the proposed method was verified through experiments. The results suggest that, despite exhibiting lower UI readability than the widely used semi-translucent user interfaces, our proposed method induced a similar level of impression and willingness to use and achieved higher accuracy in recognizing changes in visual scene information.",
    "keywords": [
      "Binocular Rivalry",
      "GUI",
      "User Interface",
      "Virtual Reality"
    ],
    "doi": "10.1145/3652920.3652939",
    "url": "https://doi.org/10.1145/3652920.3652939",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "193–204",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652922",
    "title": "FastPerson: Enhancing Video-Based Learning through Video Summarization that Preserves Linguistic and Visual Contexts",
    "authors": [
      "Kazuki Kawamura",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency. To this end, video summarization has been actively researched to enable users to view only important scenes from a video. However, these studies focus on either the visual or audio information of a video and extract important segments in the video. Therefore, there is a risk of missing important information when both the teacher’s speech and visual information on the blackboard or slides are important, such as in a lecture video. To tackle this issue, we propose FastPerson, a video summarization approach that considers both the visual and auditory information in lecture videos. FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners. Further, it provides a feature that allows learners to switch between the summary and original videos for each chapter of the video, enabling them to adjust the pace of learning based on their interests and level of understanding. We conducted an evaluation with 40 participants to assess the effectiveness of our method and confirmed that it reduced viewing time by 53% at the same level of comprehension as that when using traditional video playback methods.",
    "keywords": [
      "Video summarization",
      "e-learning",
      "human–computer interaction",
      "large language model",
      "learning efficiency",
      "multimodal information processing",
      "speech synthesis",
      "user-centered design"
    ],
    "doi": "10.1145/3652920.3652922",
    "url": "https://doi.org/10.1145/3652920.3652922",
    "citations": 6,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "205–216",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "12",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652942",
    "title": "SkillsInterpreter: A Case Study of Automatic Annotation of Flowcharts to Support Browsing Instructional Videos in Modern Martial Arts using Large Language Models",
    "authors": [
      "Kotaro Oomori",
      "Yoshio Ishiguro",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The use of video for learning physical skills such as modern martial arts is becoming popular. Physical skills such as modern martial arts require decisions depending on the situation. An example of these decisions is selecting an appropriate off-balance technique based on the position of the opponent’s feet. However, the existing interface does not support video browsing based on the structure of the physical skills, including situations and the decisions that should be made at that time. We hypothesize browsing based on the structure can help the user’s skill comprehension. In this paper, we propose a structure-based video browsing method, SkillsInterpreter, which automatically generates a flowchart of the speech-contained skill instruction video by large language models (LLMs). The generated flowchart explores desired scenes, checks the current chapter, and reviews the skill structure while watching the video. Our study included interviews with experts and evaluations with learners in modern martial arts. Based on our two studies, it was suggested that SkillsInterpreter can support video-based skill learning in modern martial arts, especially in Brazilian Jiu-Jitsu, which needs situation-specific decision making.",
    "keywords": [
      "Flowchart",
      "Large Language Models",
      "Martial Arts",
      "Video Browsing Interface"
    ],
    "doi": "10.1145/3652920.3652942",
    "url": "https://doi.org/10.1145/3652920.3652942",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "217–225",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "9",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652944",
    "title": "Kavy: Fostering Language Speaking Skills and Self-Confidence Through Conversational AI",
    "authors": [
      "Sankha Cooray",
      "Chathuranga Hettiarachchi",
      "Vishaka Nanayakkara",
      "Denys Matthies",
      "Yasith Samaradivakara",
      "Suranga Nanayakkara"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Cognitive augmentation is the process of enhancing one’s abilities, including learning a new language. For this, we could utilize conversational chatbots. Conventional chatbots such as Siri, have predominantly been based on the question-and-answer model, where a communicator seeks a specific answer to accomplish a specific task. The conversational capabilities of chatbots offer great potential to promote English language learning, particularly in developing countries, such as Sri Lanka, where many young adults lack confidence in speaking English. This is due to limited exposure to conversational-style learning and a lack of opportunity to practice without social anxiety which is often rooted in the fear of making mistakes. In this paper, we developed a conversational chatbot, Kavy, as a companion to help them practice English. We investigated, in a study with 40 users, if Kavy could improve a communicator’s proficiency (e.g., verbal expression, conversation length, quality of speech) and self-confidence using both poetic and non-poetic conversational styles. We found that the users were highly motivated by the poetic version, with its use resulting in a significant increase in vocabulary. Nevertheless, a poetic chatbot may present challenges, with several users reporting that they find the poetic version confusing. We see this pioneering work as a first and promising approach that should be continued to be investigated in the future.",
    "keywords": [
      "Artificial Intelligence",
      "Cognitive Augmentation",
      "Conversational AI Agents",
      "Language Studies",
      "Poetry",
      "Self Confidence",
      "Social chatbots",
      "Voice Interfaces"
    ],
    "doi": "10.1145/3652920.3652944",
    "url": "https://doi.org/10.1145/3652920.3652944",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "226–236",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3652931",
    "title": "Serendipity Wall: A Discussion Support System Using Real-time Speech Recognition and Large Language Model",
    "authors": [
      "Shota Imamura",
      "Hirotaka Hiraki",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Group discussions are important for exploring new ideas. Discussion support systems will enhance human creative ability through better discussion experiences. One method to support discussions is presenting relevant keywords or images. However, the context of the conversation and information tended not to be taken into account. Therefore, we propose a system that develops group discussions by presenting related information in response to discussions. As a specific example, this study addressed academic discussions among HCI researchers. During brainstorming sessions, the system continuously transcribes the dialogue and generates embedding vectors of the discussions. These vectors are matched against those of existing research articles to identify relevant studies. Then, the system presented relevant studies on the screen with summaries by an LLM. In case studies, this system had the effect of broadening the topics of discussion and facilitating the acquisition of new knowledge. This study showed the possibility that AI can facilitate discussion by providing discussion support through information retrieval and summarizing.",
    "keywords": [
      "Discussion",
      "Embedding Vectors",
      "Facilitation",
      "Information retrieval",
      "Large Language Models"
    ],
    "doi": "10.1145/3652920.3652931",
    "url": "https://doi.org/10.1145/3652920.3652931",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "237–247",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653041",
    "title": "\"How do people control the s̈elf̈or motion? Investigation of the effects of pneumatic gel muscle intervention on motion self-control and ego depletion in emotional self-control\"",
    "authors": [
      "Chiaki Raima",
      "Yuichi Kurita"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Recent technology, particularly actuators like pneumatic gel muscle (PGM), has advanced to enhance health and physical capabilities. However, challenges can arise when individuals' behavior deviates from exercise goals because of their psychological background and actuators' intervention. This study explored motion self-control (SC) changes during exercise with a PGM actuator and ego depletion (ED) in emotional SC. Results of participant's motion data and subjective reports showed that by PGM intervention in the dumbbell curl task, (a) motion SC and subjective ED increased, (b) emotional SC strategies were changed while emotional expression was objectively suppressed in the emotional SC task. The findings provide valuable insights into understanding SC factors contributing to ED and enhancing exercise technology to utilize exercise actuators effectively.",
    "keywords": [
      "ego depletion",
      "psychological experiment",
      "self-control"
    ],
    "doi": "10.1145/3652920.3653041",
    "url": "https://doi.org/10.1145/3652920.3653041",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "248–251",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653042",
    "title": "Exploring relationship between EMG, confusion and smoothness of work progress in assembly tasks",
    "authors": [
      "Tzu-Yang Wang",
      "Suyeong Rhie",
      "Mai Otsuki",
      "Hideaki Kuzuoka",
      "Takaya Yuizono"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Analyzing an operator’s mental state is an important issue in manufacturing. In this paper, we focused on confusion and perceived smoothness of work progress. A 40-participant experiment was conducted in which participants performed two 50-minute assembly tasks and answered two self-report questions about perceived confusion and perceived smoothness of work progress after each step. The results showed that there was a moderate correlation between the two variables and the duration of the steps. In addition, our preliminary EMG analysis showed that there was a moderate correlation between EMG and perceived confusion.",
    "keywords": [
      "Assembly Task",
      "Confusion",
      "Electromyography",
      "Smoothness of work progress"
    ],
    "doi": "10.1145/3652920.3653042",
    "url": "https://doi.org/10.1145/3652920.3653042",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "252–254",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653043",
    "title": "Putting Our Minds Together: Iterative Exploration for Collaborative Mind Mapping",
    "authors": [
      "Ying Yang",
      "Tim Dwyer",
      "Zachari Swiecki",
      "Benjamin Lee",
      "Michael Wybrow",
      "Maxime Cordeil",
      "Teresa Wulandari",
      "Bruce H Thomas",
      "Mark Billinghurst"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "We delineate the development of a mind-mapping system designed concurrently for both VR and desktop platforms. Employing an iterative methodology with groups of users, we systematically examined and improved various facets of our system, including interactions, communication mechanisms and gamification elements, to streamline the mind-mapping process while augmenting situational awareness and promoting active engagement among collaborators. We also report our observational findings on these facets from this iterative design process.",
    "keywords": [
      "Collaborative Sensemaking",
      "Gamification",
      "Hand Gestures",
      "Virtual Reality"
    ],
    "doi": "10.1145/3652920.3653043",
    "url": "https://doi.org/10.1145/3652920.3653043",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "255–258",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653044",
    "title": "In-the-Wild Exploration of the Impact of the Lunar Cycle on Sleep in a University Cohort with Oura Rings",
    "authors": [
      "Shota Arai",
      "Andrew Vargo",
      "Benjamin Tag",
      "Koichi Kise"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Adequate sleep is an essential part of human life. The cumulative effects of sleep deprivation and sleep disorders are associated with a wide range of negative health effects and can cause issues with cognitive readiness. Using wearable physiological-sensing devices to monitor sleep and sleep patterns could lead to interventions where individuals and groups can improve their lives. In this study, we analyzed in-the-wild sleep data obtained from Oura Ring, a ring-shaped wearable device specializing in sleep tracking, for 74 university students over a minimum period of 5 months. We investigated the influence of the lunar cycle on sleep and found that the periodicity of the moon affects sleep. Since the lunar cycle has shown to to affect sleep duration and quality, it provides an interesting test-case for in-the-wild data collection form a large cohort. The results of this paper show the potential and limitations of physiological tracking for cohorts in an in-the-wild context.",
    "keywords": [
      "cohort analysis",
      "in-the-wild",
      "physiological sensing",
      "wearables"
    ],
    "doi": "10.1145/3652920.3653044",
    "url": "https://doi.org/10.1145/3652920.3653044",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "259–262",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653045",
    "title": "Personalizing Augmented Flashcards Towards Long-Term Vocabulary Learning",
    "authors": [
      "Yuichiro Iwashita",
      "Andrew Vargo",
      "Motoi Iwata",
      "Koichi Kise"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Flashcards are one of the most popular tools for learning vocabulary for second-language learners. While flashcard mediated learning is efficient, it may not induce motivation for continued use. Some researchers have proposed augmented flashcards that provide multimedia contexts to motivate people to study. However, the augmented flashcards also have a problem that it takes time to learn each target word. Understanding this tradeoff, we introduce a system that users can learn vocabulary with both standard and augmented flashcards. In addition, our system recommends the best learning strategy to users adaptively, and realizes the long-term vocabulary learning. In this paper, we describe the system and present the results of the preliminary data analysis towards the long-term vocabulary learning.",
    "keywords": [
      "Adaptive Learning",
      "Intelligence Augmentation",
      "Multimedia",
      "Vocabulary Learning"
    ],
    "doi": "10.1145/3652920.3653045",
    "url": "https://doi.org/10.1145/3652920.3653045",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "263–266",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653046",
    "title": "Lumbopelvic ratio based screening tool for Lumbar health assessment",
    "authors": [
      "Gunarajulu Renganathan",
      "Yuichi Kurita"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Low Back Pain (LBP) is a global health issue that affects individuals of all ages and professions. Work-related musculoskeletal disorder (WSMD) is a condition experienced by an average of 60-85% of adults within their lifetime. Although several studies have investigated non-invasive methods to control or reduce the workload, there remains an unmet clinical need. Hence, the main objective of the study was 1. to investigate the lumbar spine dynamic range of motions (flexion, extension, axial rotation, and lateral bending) at various speeds (fast, normal, and slow), and 2. to investigate the relationship between lumbosacral moments and lumbopelvic ratio (also known as hip-spine coordination ratio). The results obtained from this study could help in developing methodologies or as a screening tool to prevent WSMD at the early stages. Utilizing digital technologies such as smartphones or standalone digital mirrors, markerless health assessment can be performed and the health status of the lumbar spine can be visualized instantly.",
    "keywords": [
      "Human augmentation",
      "Lumbar health",
      "Lumbopelvic ratio",
      "Musculoskeletal Modelling",
      "OpenSim"
    ],
    "doi": "10.1145/3652920.3653046",
    "url": "https://doi.org/10.1145/3652920.3653046",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "267–270",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653047",
    "title": "Spatial feature optimization through a genetic algorithm in a sensory-association-based brain-machine interface",
    "authors": [
      "Hikaru Tsunekawa",
      "Yasuhisa Maruyama",
      "Laura Alejandra Martinez-Tejada",
      "Kazutoshi Hatakeyama",
      "Tomohiro Suda",
      "Chizu Wada",
      "Takumi Inomata",
      "Kimio Saito",
      "Yuji Kasukawa",
      "Naohisa Miyakoshi",
      "Natsue Yoshimura"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Brain-machine interfaces to classify basic responses, \"Yes\" or \"No,\" are particularly beneficial for individuals with amyotrophic lateral sclerosis (ALS) who have experienced loss of motor function. However, it is challenging to elicit these cognitive thoughts with high performance because of significant individual differences in brain activity. To overcome this challenge, a \"sensory association paradigm,\" which connects sensory stimulation and Yes/No recall, has been introduced. In this study, we explored the potential of employing a genetic algorithm (GA) to reduce analysis time, with the goal of applying this paradigm at the bedside of ALS patients. We utilized galvanic vestibular stimulation (GVS), which causes equilibrium distortion, and coupled it with Yes/No responses through classical conditioning. Electroencephalographic (EEG) signals were recorded when the distortion occurred due to Yes/No recall alone in the absence of GVS, and machine learning was used to classify Yes/No responses. The classification accuracy was compared between the signals of EEG electrodes selected based on the brain activity areas by the distortion of equilibrium (area-selected) and those of electrodes selected by GA (GA-selected), and the GA-selected condition showed significantly higher accuracy than the other conditions. The selected electrodes were relevant to GVS elicitation and cognitive thinking, and the time required for analysis was within 0.1 seconds, indicating that the time required for bedside analysis may be sufficient. Additional features beyond the basic phase locking value (PLVs) used in this study will be incorporated in our future study to enhance the accuracy.",
    "keywords": [
      "Amyotrophic lateral sclerosis",
      "Brain-machine interface",
      "Electroencephalography",
      "Genetic algorithm"
    ],
    "doi": "10.1145/3652920.3653047",
    "url": "https://doi.org/10.1145/3652920.3653047",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "271–274",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653048",
    "title": "Owl-Vision: Augmentation of Visual Field by Virtual Amplification of Head Rotation",
    "authors": [
      "Michiteru Kitazaki",
      "Ryu Onodera",
      "Junya Kataoka",
      "Yasuyuki Inoue",
      "Yukiko Iwasaki",
      "Gowrishankar Ganesh"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The human visual field is limited due to the eyes being located on the front of the face. Although head rotation can expand the visual field, it is also limited by the biomechanical constraints of the neck. Due to these constraints we need to turn our bodies when we want to see beyond 180 degrees in either direction. To address this issue, we have developed a sensory augmentation system that expands the visual field by intervening in the visuo-motor transformation between our vision and head rotation. The ’Owl-Vision’ system virtually amplifies the rotation angle relative to the actual head rotation, enabling 180-degree vision in either direction (360 degrees in total) controlled by a natural user head movement of 90 degrees in either direction. The system was implemented using a 360-degree camera mounted on a head-mounted display and in virtual environments. The usability of the Owl-Vision system was evaluated and found to be good and similar to our normal (not-amplified) vision system. This system provides a ’natural’ and effective method of sensory augmentation of the visual field.",
    "keywords": [
      "360 deg camera",
      "Sensory augmentation",
      "sensory-motor adaptation"
    ],
    "doi": "10.1145/3652920.3653048",
    "url": "https://doi.org/10.1145/3652920.3653048",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "275–277",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653049",
    "title": "Augmenting Sleep Behavior with a Wearable: Can Self-Reflection Help?",
    "authors": [
      "Hannah R. Nolasco",
      "Andrew Vargo",
      "Marc Moreeuw",
      "Toma Hara",
      "Koichi Kise"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Wearable devices can have potential applications to intelligence augmentation by reducing the cognitive effort of maintaining healthy habits, but they are yet to be successful at substantially modifying behavior. This paper investigates this failure through an examination of how self-reflection impacts habit transformation and discusses potential solutions for encouraging data actionability.",
    "keywords": [
      "Behavior Modification",
      "Intelligence Augmentation",
      "Self-Reflection",
      "Self-Tracking",
      "Wearable Device"
    ],
    "doi": "10.1145/3652920.3653049",
    "url": "https://doi.org/10.1145/3652920.3653049",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "278–281",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653050",
    "title": "Motor Enhancement through an Individually Optimized Imperceptible Vibration Stimulation",
    "authors": [
      "Takashi Suzuki"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The ability to control the grip force plays a crucial role in daily life. We introduce an approach to augment this ability far beyond the conventional technology using stochastic resonance (SR) phenomenon. The participants had to control their grip force in a task involving concentric (contraction), isometric (holding), and eccentric (stretching) components, while imperceptible white noise was applied. The grip force was tracked and visualized in real-time. The results reveal that the optimal vibration for the best grip force control differs based on the person and the muscle contraction mode. Sinusoidal vibration at 400 Hz resulted in a far greater extension of the finger grip force controllability than when white noise was used in the concentric mode. Furthermore, there were individual variations in the optimal frequency width, revealing a tendency to shift toward higher frequencies with age in the isometric and eccentric modes. Our study shows that the SR effect depends on the muscle contraction mode, opening new possibilities for human augmentation.",
    "keywords": [
      "receptor",
      "sensorimotor",
      "sinusoidal vibration",
      "stochastic resonance",
      "vibration",
      "white noise"
    ],
    "doi": "10.1145/3652920.3653050",
    "url": "https://doi.org/10.1145/3652920.3653050",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "282–285",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653051",
    "title": "Creating viewpoint-dependent display on Edible Cookies",
    "authors": [
      "Takumi Yamamoto",
      "Biyon Fernando",
      "Takashi Amesaka",
      "Anusha Withana",
      "Yuta Sugiura"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In this paper, we propose a fabrication approach to transform edible cookies into information displays. The proposed approach encodes the surface of cookie dough so that the displayed information can be changed with the user’s perspective. We use a computational design approach where small holes are pocked into cookie dough at specific angles to create shapes that are only visible from a given angle. This approach allows for selective switching of information depending on the viewer’s perspective. Our contributions include a software tool that converts images to poking patterns and a computer-aided poking machine to automatically transfer the patterns onto cookie dough. Furthermore, we demonstrate the results of visualizing information on cookies.",
    "keywords": [
      "food",
      "food fabrication",
      "human food interaction",
      "viewpoint-dependent appearance"
    ],
    "doi": "10.1145/3652920.3653051",
    "url": "https://doi.org/10.1145/3652920.3653051",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "286–289",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653052",
    "title": "QA-FastPerson: Extending Video Platform Search Capabilities by Creating Summary Videos in Response to User Queries",
    "authors": [
      "Kazuki Kawamura",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "In the rapidly evolving field of digital education, the need for efficient and targeted access to information within video content has become critical. This study presents a system designed to enhance the search capabilities of video platforms by generating summary videos that answer user queries. The system uses machine learning and natural language processing techniques to understand complex user queries, pinpoint the exact video segment that provides the answer, and answer user queries more efficiently by providing the user with a summary video around that segment. Preliminary evaluations have demonstrated the system’s potential to accurately identify relevant content and generate effective summaries.",
    "keywords": [
      "Video summarization",
      "e-learning",
      "human–computer interaction",
      "large language model",
      "learning efficiency",
      "user-centered design"
    ],
    "doi": "10.1145/3652920.3653052",
    "url": "https://doi.org/10.1145/3652920.3653052",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "290–293",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653053",
    "title": "Aged Eyes: Optically Simulating Presbyopia Using Tunable Lenses",
    "authors": [
      "Qing Zhang",
      "Yoshihito Kondoh",
      "Yuta Itoh",
      "Jun Rekimoto"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Presbyopia is currently an unavoidable visual condition and will be even more severe with the aging generations worldwide. However, our daily activities often need more understanding and awareness of presbyopia. Various visual simulators are intensively explored better to understand others, especially the visually impaired ones. This research proposes an optical solution that simulates two primary visual representations of presbyopia: increased near-point and blurred vision within the near-point range. This initial study reveals the effectiveness of our approach when modulating people under their forties’ near-points over various age modes.",
    "keywords": [
      "Optical See-through AR",
      "Presbyopia",
      "Programmable Vision",
      "Visual Experience Modulation",
      "Visual Simulation"
    ],
    "doi": "10.1145/3652920.3653053",
    "url": "https://doi.org/10.1145/3652920.3653053",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "294–297",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653040",
    "title": "Demonstrating VabricBeads: Variable Stiffness Fabric using Woven Beads Structure",
    "authors": [
      "Jefferson Pardomuan",
      "Shio Miyafuji",
      "Nobuhiro Takahashi",
      "Hideki Koike"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Stiffness properties are pivotal in defining fabric functionality and potential applications. This paper investigates the potential of variable stiffness, specifically in the context of beaded fabric. We explore innovative designs leveraging Pneumatic Artificial Muscles (PAMs) to dynamically adjust fabric stiffness. Additionally, we showcase the practical utility of our fabric design through applications in variable stiffness wrist braces.",
    "keywords": [
      "Fabrication",
      "Shape-changing Interface",
      "Soft Robotics",
      "Variable Stiffness",
      "Wearable Interface"
    ],
    "doi": "10.1145/3652920.3653040",
    "url": "https://doi.org/10.1145/3652920.3653040",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "298–300",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653054",
    "title": "Metacognition-EnGauge: Real-time Augmentation of Self-and-Group Engagement Levels Understanding by Gauge Interface in Online Meetings",
    "authors": [
      "Ko Watanabe",
      "Andreas Dengel",
      "Shoya Ishimaru"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Engagement is one of the core cognitive states in communication. Increased engagement improves the quality and immersion of the conversation. In this demonstration, we aim to present a metacognition augmentation application called Metacognition-EnGauge. This work aims to discover three research questions. Does self-metacognition augmentation of engagement levels support participant to be engaged in the meetings? Does group-metacognition augmentation of engagement levels support participant to be engaged in the meetings? Do participants prefer none, self, or group-metacognition augmentation intervention in the online meetings? To answer these questions, we conducted online meeting experiments with 18 participants. As a result, we found that self-metacognition augmentation outperformed well on increasing average engagement levels. According to the survey, participant preference shows group-metacognition as the best intervention.",
    "keywords": [
      "Affective Computing",
      "Metacognition Augmentation",
      "Social Cognitive State",
      "Social Interaction"
    ],
    "doi": "10.1145/3652920.3653054",
    "url": "https://doi.org/10.1145/3652920.3653054",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "301–303",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653055",
    "title": "RadioMe: Adaptive Radio with Music Intervention and Reminder System for People with Dementia in Their Own Home",
    "authors": [
      "Patrizia Di Campli San Vito",
      "Xiaochen Yang",
      "James Ross",
      "Gözel Shakeri",
      "Stephen Brewster",
      "Satvik Venkatesh",
      "Alex Street",
      "Jörg Fachner",
      "Paul Fernie",
      "Leonardo Muller-Rodriguez",
      "Ming Hung Hsu",
      "Helen Odell-Miller",
      "Hari Shaji",
      "Paulo Vitor Itaborai",
      "Nicolas Farina",
      "Sube Banerjee",
      "Alexis Kirke",
      "Eduardo Miranda"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The population of the world is continuously growing older, leading to more people with dementia who need support while living in their own home. Our RadioMe system was designed to adapt a live radio stream with reminders and music intervention for agitation mitigation for people with dementia living in their own home. In this demonstration we present our prototype, with features to record reminders and schedule them to be played during the live radio stream and a music intervention system when agitation is detected.",
    "keywords": [
      "agitation detection and mitigation",
      "people with dementia",
      "reminder system"
    ],
    "doi": "10.1145/3652920.3653055",
    "url": "https://doi.org/10.1145/3652920.3653055",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "304–306",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653056",
    "title": "E-Scooter Dynamics: Unveiling Rider Behaviours and Interactions with Road Users through Multi-Modal Data Analysis",
    "authors": [
      "Hiruni Kegalle",
      "Danula Hettiachchi",
      "Jeffrey Chan",
      "Flora Salim",
      "Mark Sanderson"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Electric scooters (e-scooters), characterised by their small size and lightweight design, have revolutionised urban commuting experiences. Their adaptability to multiple mobility infrastructures introduces advantages for users, enhancing the efficiency and flexibility of urban transit. However, this versatility also causes potential challenges, including increased interactions and conflicts with other road users. Previous research has primarily focused on historical trip data, leaving a gap in our understanding of real-time e-scooter user behaviours and interactions. To bridge this gap, we propose a novel multi-modal data collection and integrated data analysis methodology, aimed at capturing the dynamic behaviours of e-scooter riders and their interactions with other road users in real-world settings. We present the study setup and the analysis approach we used for an in the wild study with 15 participants, each traversing a pre-determined route equipped with off-the-shelf commercially available devices (e.g., cameras, bike computers) and eye-tracking glasses.",
    "keywords": [
      "E-scooter",
      "Eye-tracking",
      "Micro-mobility",
      "Road User Interaction",
      "Speed",
      "Video Analysis"
    ],
    "doi": "10.1145/3652920.3653056",
    "url": "https://doi.org/10.1145/3652920.3653056",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "307–310",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653057",
    "title": "PairPlayVR: Shared Hand Control for Virtual Games",
    "authors": [
      "Hongyu Zhou",
      "Pamuditha Somarathne",
      "Treshan Ayesh Peirispulle",
      "Chenyu Fan",
      "Zhanna Sarsenbayeva",
      "Anusha Withana"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Collaborative interactions are set to improve shared control of avatars in virtual reality settings. The literature indicates that shared control can enhance user experiences, but most previous demonstrations have a restricted movement system. In this demonstration, we will present PairPlayVR, a system that extends to a realistic shared control experience that provides unrestrained hand control in VR gameplay. Participants will have the opportunity to engage with the PairPlayVR system, experiencing a novel interaction paradigm where multiple users share control of a common virtual avatar for gaming. This demonstration aims to foster critical discussions on the implications of shared control mechanisms and their influence on user experience within interactive systems.",
    "keywords": [
      "Embodied Interaction",
      "Gaming Experience",
      "Shared control",
      "User Experience Design",
      "Virtual Reality"
    ],
    "doi": "10.1145/3652920.3653057",
    "url": "https://doi.org/10.1145/3652920.3653057",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "311–314",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653058",
    "title": "Tuning Infill Characteristics to Fabricate Customizable 3D Printed Pressure Sensors",
    "authors": [
      "Jiakun Yu",
      "Praneeth Perera",
      "Anusha Withana"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "We present a novel method for fabricating customizable pressure sensors by tuning the infill characteristics of flexible 3D prints, addressing the demand for precise sensing solutions across a range of applications that require variable softness. While infills are conventionally viewed as supportive structures in 3D printing, our work shifts the focus toward their potential to create compressive structures. Our demonstration highlights how different infill patterns and densities influence the mechanical behavior of 3D printed objects. Consequently, we demonstrate the versatility of this method through the fabrication of sensors for three exemplary applications, each with varying sensing needs.",
    "keywords": [
      "Customization",
      "Fabrication",
      "Pressure Sensor"
    ],
    "doi": "10.1145/3652920.3653058",
    "url": "https://doi.org/10.1145/3652920.3653058",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "315–317",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653037",
    "title": "Exploring Shared Bodily Control: Designing Augmented Human Systems for Intra- and Inter-Corporeality",
    "authors": [
      "Rakesh Patibanda",
      "Nathalie Overdevest",
      "Aryan Saini",
      "Zhuying Li",
      "Josh Andres",
      "Jarrod Knibbe",
      "Elise van den Hoven",
      "Florian 'Floyd' Mueller"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The human-computer interaction community has evolved from using body-sensing to body-actuating technologies, transforming the body’s role from a mere input to an input-output medium. With body-sensing, the separation between the human and the computer is clear, allowing for an easy understanding of who is in control. However, with body-actuating technologies, this separation diminishes. These technologies integrate more closely with our bodies, where both the user and the technology can share control over their bodily interactions. In this workshop, we will explore this notion of sharing control, specifically focusing on experiences where users interact with their own bodies (intra-corporeal experiences), and interact with others using technology (inter-corporeal experiences). Our discussions and group activities will focus on brainstorming and designing within human augmentation, examining how this shared control can lead to innovative applications.",
    "keywords": [
      "Bodily Interactions",
      "Body-Actuating Technologies",
      "Human Augmentation",
      "Input-Output Medium",
      "Inter-Corporeal Experiences",
      "Intra-Corporeal Experiences",
      "Physical Computing",
      "Sensory Augmentation",
      "Shared Control",
      "Wearable Robotics"
    ],
    "doi": "10.1145/3652920.3653037",
    "url": "https://doi.org/10.1145/3652920.3653037",
    "citations": 7,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "318–323",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "6",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653038",
    "title": "Developing Strategies for Co-designing Assistive Augmentation Technologies",
    "authors": [
      "Adele Tong",
      "Zhanna Sarsenbayeva",
      "Jorge Goncalves",
      "Hideki Koike",
      "Masahiko Inami",
      "Alistair McEwan",
      "Anusha Withana"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "Assistive augmentation promotes the independence of individuals with impairments by enhancing human capabilities that help overcome specific barriers or challenges. To understand these challenges and other requirements of the user, co-design – a method often used in the design of assistive technologies – could be adopted. Through engaging end-users in the design and development process, augmentation technologies could be designed to cater to the changing needs of users, including requirements for the modality of enhancement and technology integration. However, considerations specific to the co-design of assistive augmentations, such as the involvement of stakeholders beyond individuals with impairments, have yet to be explored. This workshop aims to gather insights from designers and researchers to synthesise new strategies for co-designing assistive augmentation technologies.",
    "keywords": [
      "Assistive augmentation",
      "assistive technologies",
      "augmented humans",
      "co-design"
    ],
    "doi": "10.1145/3652920.3653038",
    "url": "https://doi.org/10.1145/3652920.3653038",
    "citations": 1,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "324–326",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "3",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3653039",
    "title": "Intelligent Music Interfaces: When Interactive Assistance and Adaptive Augmentation Meet Musical Instruments",
    "authors": [
      "Thomas Kosch",
      "Andreas Weiß",
      "Jordan Aiko Deja",
      "Snehesh Shrestha",
      "Matthias Hoppe",
      "Andrii Matviienko",
      "Karola Marky"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "The interactive augmentation of musical instruments to foster self-expression and learning has a rich history. Over the past decades, incorporating interactive technologies into musical instruments has emerged as a research field requiring strong collaboration between disciplines. The workshop “Intelligent Music Interfaces” covers a wide range of musical research subjects and directions, including (a) current challenges in musical learning, (b) prototyping for improvements, (c) new means of musical expression, and (d) evaluation of the solutions.",
    "keywords": [
      "Augmented Instruments",
      "Music Interfaces",
      "Musical Instruments",
      "Self-Expression"
    ],
    "doi": "10.1145/3652920.3653039",
    "url": "https://doi.org/10.1145/3652920.3653039",
    "citations": 0,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "327–330",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "4",
    "influentialCitations": 0
  },
  {
    "id": "10.1145/3652920.3654916",
    "title": "Augmented Astronaut Survival: Updating the ‘How to Survive on the Moon’ scenario workshop in preparation for an Artemis Edition.",
    "authors": [
      "Sarah Jane Pell"
    ],
    "year": 2024,
    "conference": "AHS",
    "conferenceYear": "AHs '24",
    "abstract": "As humanity confronts the escalating complexities of space exploration, the development of innovative methodologies tailored to augment humans in the lunar environment is essential. This keynote posits a transformative framework for experimental approaches in space survival, particularly for the Artemis missions, through creative engagement and inventive brainstorming. It takes a participatory \"research-through-performance,\" scenario approach to probe the tapestry of scientific, engineering, and cultural challenges that future lunar missions will inevitably face. The early NASA Moon Survival Task, initiated before the Apollo landings, provided insights into team-think dynamics and decision-making under duress, ensuring its relevance and use for over 50 years now. However, the now-rudimentary survival kit contents are mismatched with the intricate realities of forthcoming lunar expedition technology, interactions, and protocols. Therefore, I present a newly reimagined scenario—\"How to Survive on the Moon: The Artemis Edition\"—a mission enriched with exponential technologies and speculated systems, including immersive virtual environments, bio-regenerative life-support, adaptive smart suits, personalised medicine, and 3D additive manufacturing. Today's challenge is again more than updating the toolkit; it primes the augmented human community for the nuanced demands of space and ignites a focus on emergency preparedness. Seeking new ways of collective brainstorming for the reconceptualisation of resource management and resourcefulness remains critical. Ultimately, asking important questions about what Augmented Astronauts need, seeks to support the next generation of lunar explorers—empowered today by global transdisciplinary collaboration, equipped with transformational technologies, and critically prepared to thrive in the Artemis era.",
    "keywords": [],
    "doi": "10.1145/3652920.3654916",
    "url": "https://doi.org/10.1145/3652920.3654916",
    "citations": 2,
    "booktitle": "Proceedings of the Augmented Humans International Conference 2024",
    "pages": "331–341",
    "publisher": "Association for Computing Machinery",
    "location": "Melbourne, VIC, Australia",
    "articleno": "",
    "numpages": "11",
    "influentialCitations": 0
  }
]